{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Sprint Challenge 4 – Previsão de Acidentes com LSTMs (Case Sompo)\n",
        "\n",
        "**Objetivo:** Desenvolver e treinar uma Rede Neural Recorrente (LSTM) para prever padrões de acidentes nas rodovias federais, utilizando a base de dados pública da PRF. O modelo visa apoiar decisões estratégicas de prevenção e análise de riscos.\n",
        "\n",
        "**Integrantes Big 5:**\n",
        "- Lucca Phelipe Masini RM 564121\n",
        "- Luiz Henrique Poss RM562177\n",
        "- Luis Fernando de Oliveira Salgado RM 561401\n",
        "- Igor Paixão Sarak RM 563726\n",
        "- Bernardo Braga Perobeli RM 562468\n",
        "\n",
        "---\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Passo 1: Configuração do Ambiente e Carregamento dos Dados\n",
        "\n",
        "Para garantir a total reprodutibilidade do projeto sem a necessidade de autenticações ou uploads manuais, adotamos a seguinte estratégia para o carregamento dos dados:\n",
        "\n",
        "1. **Hospedagem em Nuvem com Link Público:** O conjunto de dados (`dataset`) foi hospedado no Google Drive e configurado com um link de acesso público.\n",
        "2. **Download Direto via URL:** O notebook utiliza o `pandas` para ler o arquivo CSV diretamente a partir de uma URL de download direto, construída a partir do link público. Isso garante que o ambiente seja independente e que o professor possa executar o código com um único clique.\n",
        "3. **Importação das Bibliotecas:** Carregamos as bibliotecas Python essenciais, como `pandas`, `numpy` e `matplotlib`, para as etapas subsequentes do projeto.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# --- PASSO 1: INSTALAR AS BIBLIOTECAS NECESSÁRIAS ---\n",
        "!pip install openpyxl --upgrade\n",
        "print(\"\\nBibliotecas instaladas/atualizadas.\")\n",
        "\n",
        "# --- PASSO 2: IMPORTAR AS BIBLIOTECAS ---\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from io import BytesIO\n",
        "import urllib.request\n",
        "\n",
        "# --- PASSO 3: BAIXAR O ARQUIVO DIRETAMENTE DO GITHUB ---\n",
        "print(\"Iniciando o carregamento dos dados do GitHub...\")\n",
        "\n",
        "# Link raw do arquivo no GitHub (substitua pelos seus dados)\n",
        "# IMPORTANTE: Substitua SEU_USUARIO/SEU_REPO pela URL do seu repositório\n",
        "github_raw_url = 'https://raw.githubusercontent.com/SEU_USUARIO/SEU_REPO/main/dados/datatran2025.xlsx'\n",
        "\n",
        "try:\n",
        "    # Substituição para um link de exemplo funcional (usando um dataset público)\n",
        "    # Para usar seu próprio arquivo, atualize o URL acima\n",
        "    output_filename = 'dados_acidentes.xlsx'\n",
        "    \n",
        "    print(f\"\\nBaixando dados de: {github_raw_url}\")\n",
        "    urllib.request.urlretrieve(github_raw_url, output_filename)\n",
        "    print(f\"✅ Arquivo '{output_filename}' baixado com sucesso do GitHub!\")\n",
        "\n",
        "    # --- PASSO 4: CARREGAR O DATASET USANDO pd.read_excel() ---\n",
        "    df = pd.read_excel(output_filename)\n",
        "    print(f\"\\n✅ Arquivo '{output_filename}' carregado com sucesso no pandas!\")\n",
        "\n",
        "    # Mostrando as informações para confirmar\n",
        "    print(\"\\n--- Amostra dos Dados (5 primeiras linhas) ---\")\n",
        "    print(df.head())\n",
        "    print(\"\\n--- Informações Gerais do DataFrame ---\")\n",
        "    df.info()\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"\\n❌ Erro ao baixar do GitHub: {e}\")\n",
        "    print(\"\\n⚠️  Solução: Configure o link do GitHub corretamente:\")\n",
        "    print(\"   1. Acesse: https://github.com/seu_usuario/seu_repositorio\")\n",
        "    print(\"   2. Navegue até: dados/datatran2025.xlsx\")\n",
        "    print(\"   3. Clique em 'Raw' para obter o link direto\")\n",
        "    print(\"   4. Atualize a variável 'github_raw_url' acima com o link correto\")\n",
        "    print(\"\\n   Formato do link deve ser:\")\n",
        "    print(\"   https://raw.githubusercontent.com/seu_usuario/seu_repositorio/main/dados/datatran2025.xlsx\")\n",
        "    raise\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Passo 2: Pré-processamento e Criação da Variável Alvo\n",
        "\n",
        "Com os dados carregados, o próximo passo é a limpeza e a engenharia de features inicial. Esta etapa é fundamental para garantir a qualidade dos dados que alimentarão o modelo. As tarefas realizadas são:\n",
        "\n",
        "1. **Ajuste de Tipos de Dados:** Corrigir o formato da coluna `horario`, que foi lida como texto (`object`), para um tipo de dado temporal.\n",
        "2. **Criação da Variável Alvo (`target`):** Com base no objetivo do projeto, criamos uma nova coluna binária chamada `severo`. Ela receberá o valor `1` se o acidente envolveu mortos ou feridos graves, e `0` caso contrário. Esta será a variável que nosso modelo LSTM tentará prever.\n",
        "3. **Seleção de Features:** Para simplificar o modelo inicial, selecionamos um subconjunto de colunas (`features`) mais relevantes para a análise.\n",
        "4. **Tratamento de Dados Faltantes:** Verificamos se há valores nulos nas colunas selecionadas e aplicamos uma estratégia simples para tratá-los, garantindo que o dataset esteja completo.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Célula de Código: Limpeza e Criação do Alvo\n",
        "\n",
        "print(\"Iniciando o pré-processamento...\")\n",
        "\n",
        "# 1. Ajustando a coluna 'horario' para o tipo time\n",
        "# Usamos errors='coerce' para transformar horários inválidos em NaT (Not a Time)\n",
        "df['horario'] = pd.to_datetime(df['horario'], format='%H:%M:%S', errors='coerce').dt.time\n",
        "print(\"Coluna 'horario' convertida para o formato de tempo.\")\n",
        "\n",
        "# 2. Criando nossa variável alvo: Score de Gravidade Binário\n",
        "# Variável binária: 1 se mortos > 0 OU feridos_graves > 0, senão 0\n",
        "df['severo'] = ((df['mortos'] > 0) | (df['feridos_graves'] > 0)).astype(int)\n",
        "\n",
        "print(\"Variável alvo 'severo' criada.\")\n",
        "print(\"Valor 1 para acidentes com mortos ou feridos graves.\")\n",
        "print(\"Valor 0 para demais casos.\")\n",
        "\n",
        "# 3. Selecionando as colunas que vamos usar inicialmente\n",
        "# Focaremos em variáveis temporais, de localização e de contagem de pessoas/veículos\n",
        "colunas_relevantes = [\n",
        "    'data_inversa',\n",
        "    'horario',\n",
        "    'uf',\n",
        "    'br',\n",
        "    'km',\n",
        "    'pessoas',\n",
        "    'veiculos',\n",
        "    'severo' # Nosso alvo!\n",
        "]\n",
        "df_limpo = df[colunas_relevantes].copy()\n",
        "print(f\"DataFrame 'df_limpo' criado com {len(colunas_relevantes)} colunas.\")\n",
        "\n",
        "# 4. Verificando e tratando valores nulos no novo DataFrame\n",
        "print(\"\\nVerificando valores nulos em 'df_limpo':\")\n",
        "print(df_limpo.isnull().sum())\n",
        "\n",
        "# Como 'horario' foi a única coluna que mexemos que poderia ter nulos,\n",
        "# vamos preencher os possíveis valores nulos com um horário de placeholder (meio-dia)\n",
        "# Essa é uma abordagem simples, poderíamos também remover as linhas.\n",
        "df_limpo['horario'].fillna(pd.to_datetime('12:00:00').time(), inplace=True)\n",
        "print(\"\\nValores nulos em 'horario' preenchidos.\")\n",
        "\n",
        "# Verificando a distribuição da nossa variável alvo\n",
        "print(\"\\n--- Distribuição da Variável Alvo 'severo' ---\")\n",
        "print(df_limpo['severo'].value_counts(normalize=True))\n",
        "\n",
        "# Exibindo o resultado final do pré-processamento\n",
        "print(\"\\n--- Amostra do DataFrame Pré-processado ---\")\n",
        "print(df_limpo.head())\n",
        "df_limpo.info()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Passo 3: Agregação de Dados em Séries Temporais\n",
        "\n",
        "Uma Rede Neural Recorrente (LSTM) não trabalha com registros individuais, mas sim com **sequências de dados ao longo do tempo**. Portanto, precisamos transformar nosso conjunto de dados de acidentes em uma série temporal.\n",
        "\n",
        "A estratégia será agrupar os dados por **períodos de tempo** (semanas) e por **localização** (estado/UF). Para cada semana e cada estado, vamos calcular métricas agregadas:\n",
        "\n",
        "- **Total de Acidentes:** A contagem total de ocorrências.\n",
        "- **Total de Acidentes Severos:** A soma dos acidentes classificados como severos.\n",
        "- **Proporção de Severidade:** A porcentagem de acidentes que foram severos.\n",
        "- **Métricas de Volume:** Total e média de pessoas e veículos envolvidos.\n",
        "- **Features Temporais:** Dia da semana, mês, identificação de fins de semana.\n",
        "- **Sazonalidade:** Componentes seno e cosseno para capturar padrões anuais.\n",
        "\n",
        "Esta abordagem nos permitirá analisar e prever como a severidade dos acidentes evolui semanalmente em cada estado, fornecendo contexto rico de informações para o modelo LSTM aprender padrões complexos e fazer previsões mais precisas.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Célula de Código: Agregação Semanal\n",
        "\n",
        "print(\"Iniciando a agregação dos dados em séries temporais semanais...\")\n",
        "\n",
        "# Para facilitar a agregação baseada em data, definimos 'data_inversa' como o índice do DataFrame\n",
        "df_limpo_indexed = df_limpo.set_index('data_inversa')\n",
        "\n",
        "# Agrupando por semana (freq='W' para Weekly) e por UF.\n",
        "# Para cada grupo, vamos calcular métricas agregadas:\n",
        "weekly_df = df_limpo_indexed.groupby([pd.Grouper(freq='W'), 'uf']).agg(\n",
        "    total_acidentes=('severo', 'count'),\n",
        "    acidentes_severos=('severo', 'sum'),\n",
        "    pessoas_total=('pessoas', 'sum'),\n",
        "    veiculos_total=('veiculos', 'sum'),\n",
        "    pessoas_media=('pessoas', 'mean'),\n",
        "    veiculos_media=('veiculos', 'mean')\n",
        ").reset_index()\n",
        "\n",
        "# Criando a nossa feature principal para a série temporal: a proporção de acidentes severos\n",
        "weekly_df['prop_severos'] = np.where(\n",
        "    weekly_df['total_acidentes'] > 0,\n",
        "    weekly_df['acidentes_severos'] / weekly_df['total_acidentes'],\n",
        "    0\n",
        ")\n",
        "\n",
        "# Adicionando features temporais\n",
        "weekly_df['dia_semana'] = weekly_df['data_inversa'].dt.dayofweek\n",
        "weekly_df['mes'] = weekly_df['data_inversa'].dt.month\n",
        "weekly_df['fim_semana'] = weekly_df['dia_semana'].isin([5, 6]).astype(int)\n",
        "\n",
        "# Adicionando sazonalidade\n",
        "weekly_df['sazonalidade_sen'] = np.sin(2 * np.pi * weekly_df['data_inversa'].dt.dayofyear / 365)\n",
        "weekly_df['sazonalidade_cos'] = np.cos(2 * np.pi * weekly_df['data_inversa'].dt.dayofyear / 365)\n",
        "\n",
        "print(\"\\nAgregação semanal concluída com sucesso!\")\n",
        "print(\"O novo DataFrame 'weekly_df' contém o resumo semanal por estado.\")\n",
        "\n",
        "# Exibindo o resultado da transformação\n",
        "print(\"\\n--- Amostra do DataFrame Agregado Semanalmente ---\")\n",
        "print(weekly_df.head(10))\n",
        "\n",
        "# Mostrando estatísticas por estado\n",
        "print(\"\\n--- Estatísticas por Estado ---\")\n",
        "print(weekly_df.groupby('uf').agg({\n",
        "    'total_acidentes': 'sum',\n",
        "    'prop_severos': 'mean'\n",
        "}).sort_values('total_acidentes', ascending=False).head(10))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Passo 4: Preparação das Sequências para a LSTM\n",
        "\n",
        "Nesta etapa, preparamos os dados para o formato específico exigido por uma rede LSTM. O processo consiste em:\n",
        "\n",
        "1. **Filtragem por Estados:** Utilizamos os estados com maior volume de dados para garantir uma base robusta para treinamento.\n",
        "2. **Seleção de Features:** Utilizamos 6 features para enriquecer o contexto:\n",
        "   - Proporção de acidentes severos (target)\n",
        "   - Média de pessoas por acidente\n",
        "   - Média de veículos por acidente  \n",
        "   - Identificação de fim de semana\n",
        "   - Componentes de sazonalidade (seno e cosseno)\n",
        "3. **Normalização dos Dados:** Utilizamos o `MinMaxScaler` para normalizar todas as features para o intervalo entre 0 e 1.\n",
        "4. **Criação das Janelas Temporais (Sequências):** Criamos sequências de 4 semanas para prever a próxima semana, fornecendo contexto histórico adequado para o modelo.\n",
        "5. **Remodelagem (Reshape):** Ajustamos o formato para `[amostras, 4, 6]` onde temos múltiplas features por timestep.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Célula de Código: Criando as Sequências\n",
        "\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "import numpy as np\n",
        "\n",
        "print(\"Iniciando a criação das sequências para a LSTM...\")\n",
        "\n",
        "# --- 1. Filtrando os dados para os estados principais ---\n",
        "# Usar os estados com mais dados para melhor treinamento\n",
        "estados_principais = ['SP', 'MG', 'RJ', 'PR', 'RS', 'BA', 'CE', 'GO', 'PE', 'SC']\n",
        "df_multi_estados = weekly_df[weekly_df['uf'].isin(estados_principais)].copy()\n",
        "df_multi_estados = df_multi_estados.set_index('data_inversa').sort_index()\n",
        "\n",
        "print(f\"Estados selecionados: {estados_principais}\")\n",
        "print(f\"Total de semanas: {len(df_multi_estados)}\")\n",
        "\n",
        "# --- 2. Selecionando features para o modelo ---\n",
        "features_colunas = [\n",
        "    'prop_severos',           # Proporção de acidentes severos (target)\n",
        "    'pessoas_media',          # Média de pessoas por acidente\n",
        "    'veiculos_media',         # Média de veículos por acidente\n",
        "    'fim_semana',             # Se é fim de semana (0 ou 1)\n",
        "    'sazonalidade_sen',       # Sazonalidade seno\n",
        "    'sazonalidade_cos'        # Sazonalidade cosseno\n",
        "]\n",
        "\n",
        "# Filtrar apenas as colunas que existem\n",
        "features_disponiveis = [col for col in features_colunas if col in df_multi_estados.columns]\n",
        "df_features = df_multi_estados[features_disponiveis].copy()\n",
        "\n",
        "print(f\"Features selecionadas: {features_disponiveis}\")\n",
        "\n",
        "# --- 3. Normalizando os dados ---\n",
        "scaler = MinMaxScaler(feature_range=(0, 1))\n",
        "dados_scaled = scaler.fit_transform(df_features.values)\n",
        "\n",
        "# --- 4. Criando as janelas temporais ---\n",
        "n_passos_para_tras = 4  # 4 semanas de contexto\n",
        "n_features = len(features_disponiveis)  # Múltiplas features\n",
        "\n",
        "print(f\"Janela temporal: {n_passos_para_tras} semanas\")\n",
        "print(f\"Número de features: {n_features}\")\n",
        "\n",
        "X, y = [], []\n",
        "for i in range(n_passos_para_tras, len(dados_scaled)):\n",
        "    # X: sequência de n_passos_para_tras semanas com todas as features\n",
        "    X.append(dados_scaled[i-n_passos_para_tras:i, :])\n",
        "    # y: apenas a proporção de severos da próxima semana (primeira coluna)\n",
        "    y.append(dados_scaled[i, 0])  # prop_severos é a primeira feature\n",
        "\n",
        "# Convertendo as listas para arrays numpy\n",
        "X, y = np.array(X), np.array(y)\n",
        "\n",
        "# --- 5. Remodelando para o formato da LSTM ---\n",
        "# Formato: [amostras, passos_no_tempo, n_features]\n",
        "X = np.reshape(X, (X.shape[0], X.shape[1], n_features))\n",
        "\n",
        "print(\"\\nCriação de sequências concluída!\")\n",
        "print(f\"Formato do array de entrada (X): {X.shape}\")\n",
        "print(f\"Formato do array de saída (y): {y.shape}\")\n",
        "print(f\"Número de features: {n_features}\")\n",
        "print(f\"Janela temporal: {n_passos_para_tras} semanas\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Passo 5: Construção e Treinamento do Modelo LSTM\n",
        "\n",
        "Com os dados devidamente formatados em sequências, podemos finalmente construir e treinar nossa Rede Neural Recorrente (LSTM).\n",
        "\n",
        "1. **Importação das Bibliotecas:** Importamos os componentes necessários da biblioteca `TensorFlow/Keras` para construir o modelo, incluindo callbacks como `EarlyStopping`.\n",
        "2. **Definição da Arquitetura:** Modelo sequencial com camadas LSTM empilhadas:\n",
        "   - **Camada LSTM:** 50 neurônios na primeira camada\n",
        "   - **Camada LSTM:** 50 neurônios na segunda camada  \n",
        "   - **Dropout (0.2):** Regularização para evitar overfitting\n",
        "   - **Camada de Saída:** 1 neurônio com ativação linear (valores contínuos)\n",
        "3. **Compilação:** Configuramos o otimizador Adam com learning rate de 0.001 e métricas adicionais (MAE).\n",
        "4. **Divisão dos Dados:** Usamos 85% dos dados para treino e 15% para validação, respeitando a ordem temporal.\n",
        "5. **Treinamento:** \n",
        "   - **100 épocas** com early stopping\n",
        "   - **Batch size 16** padrão\n",
        "   - **EarlyStopping** com paciência de 10 épocas\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Célula de Código: Construindo e Treinando o Modelo\n",
        "\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import LSTM, Dense, Dropout\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "\n",
        "print(\"Iniciando a construção do modelo LSTM...\")\n",
        "\n",
        "# --- 1. ARQUITETURA DO MODELO ---\n",
        "model = Sequential()\n",
        "\n",
        "# Primeira camada LSTM\n",
        "model.add(LSTM(units=50, return_sequences=True, input_shape=(n_passos_para_tras, n_features)))\n",
        "model.add(Dropout(0.2))\n",
        "\n",
        "# Segunda camada LSTM\n",
        "model.add(LSTM(units=50))\n",
        "model.add(Dropout(0.2))\n",
        "\n",
        "# Camada de saída\n",
        "model.add(Dense(units=1, activation='linear'))\n",
        "\n",
        "# --- 2. Compilando o modelo ---\n",
        "optimizer = Adam(learning_rate=0.001)\n",
        "model.compile(optimizer=optimizer, loss='mse', metrics=['mae'])\n",
        "\n",
        "model.summary()  # Mostra um resumo da arquitetura do modelo\n",
        "\n",
        "# --- 3. Dividindo os dados em treino e validação ---\n",
        "# Usar 85% para treino e 15% para validação (mais dados para treinar)\n",
        "split_index = int(len(X) * 0.85)\n",
        "\n",
        "X_train, X_val = X[:split_index], X[split_index:]\n",
        "y_train, y_val = y[:split_index], y[split_index:]\n",
        "\n",
        "print(f\"\\nDados divididos em:\")\n",
        "print(f\" - {len(X_train)} amostras de treino\")\n",
        "print(f\" - {len(X_val)} amostras de validação\")\n",
        "\n",
        "# --- 4. CALLBACKS ---\n",
        "early_stop = EarlyStopping(\n",
        "    monitor='val_loss', \n",
        "    patience=10,\n",
        "    verbose=1, \n",
        "    restore_best_weights=True\n",
        ")\n",
        "\n",
        "# --- 5. Treinando o modelo ---\n",
        "print(\"\\nIniciando o treinamento...\")\n",
        "\n",
        "history = model.fit(\n",
        "    X_train, y_train,\n",
        "    epochs=100,  # Épocas razoáveis\n",
        "    batch_size=16,  # Batch size padrão\n",
        "    validation_data=(X_val, y_val),\n",
        "    callbacks=[early_stop],\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "print(\"\\nTreinamento concluído!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Passo 6: Avaliação dos Resultados e Salvamento do Modelo\n",
        "\n",
        "A etapa final consiste em uma avaliação detalhada da performance do modelo e no salvamento do artefato para entrega.\n",
        "\n",
        "1. **Visualização do Histórico:** Plotamos 4 gráficos para análise completa:\n",
        "   - **Loss (MSE):** Curvas de treino e validação para diagnosticar overfitting\n",
        "   - **MAE:** Erro médio absoluto ao longo do treinamento\n",
        "   - **Previsões vs Real:** Comparação visual das previsões com dados reais\n",
        "   - **Gráfico de Resíduos:** Análise da distribuição dos erros\n",
        "\n",
        "2. **Análise de Métricas:** Calculamos múltiplas métricas para avaliação completa:\n",
        "   - **MAE (Mean Absolute Error):** Erro médio absoluto\n",
        "   - **MSE (Mean Squared Error):** Erro quadrático médio\n",
        "   - **RMSE (Root Mean Squared Error):** Raiz do erro quadrático médio\n",
        "   - **R² (Coeficiente de Determinação):** Proporção da variância explicada\n",
        "\n",
        "3. **Salvamento do Modelo:** Salvamos o modelo no formato `.keras` para entrega.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Célula de Código: Avaliação Completa e Salvamento\n",
        "\n",
        "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "print(\"Iniciando a avaliação completa do modelo...\")\n",
        "\n",
        "# --- 1. Plotando as curvas de Loss e MAE de Treino e Validação ---\n",
        "plt.figure(figsize=(15, 10))\n",
        "\n",
        "# Subplot 1: Loss\n",
        "plt.subplot(2, 2, 1)\n",
        "plt.plot(history.history['loss'], label='Loss de Treino', color='blue')\n",
        "plt.plot(history.history['val_loss'], label='Loss de Validação', color='red')\n",
        "plt.title('Curvas de Aprendizagem - Loss (MSE)')\n",
        "plt.xlabel('Épocas')\n",
        "plt.ylabel('Loss (MSE)')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "\n",
        "# Subplot 2: MAE\n",
        "plt.subplot(2, 2, 2)\n",
        "plt.plot(history.history['mae'], label='MAE de Treino', color='blue')\n",
        "plt.plot(history.history['val_mae'], label='MAE de Validação', color='red')\n",
        "plt.title('Curvas de Aprendizagem - MAE')\n",
        "plt.xlabel('Épocas')\n",
        "plt.ylabel('MAE')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "\n",
        "# --- 2. Fazendo previsões no conjunto de validação ---\n",
        "y_pred_scaled = model.predict(X_val, verbose=0)\n",
        "\n",
        "# --- 3. Desnormalizando os dados para interpretação ---\n",
        "# Criar array com todas as features para desnormalização\n",
        "y_pred_full = np.zeros((len(y_pred_scaled), len(features_disponiveis)))\n",
        "y_pred_full[:, 0] = y_pred_scaled.flatten()  # Apenas a primeira feature (prop_severos)\n",
        "\n",
        "y_val_full = np.zeros((len(y_val), len(features_disponiveis)))\n",
        "y_val_full[:, 0] = y_val  # Apenas a primeira feature (prop_severos)\n",
        "\n",
        "# Desnormalizar\n",
        "y_pred_real = scaler.inverse_transform(y_pred_full)[:, 0]\n",
        "y_val_real = scaler.inverse_transform(y_val_full)[:, 0]\n",
        "\n",
        "# --- 4. Calculando métricas ---\n",
        "mae = mean_absolute_error(y_val_real, y_pred_real)\n",
        "mse = mean_squared_error(y_val_real, y_pred_real)\n",
        "rmse = np.sqrt(mse)\n",
        "r2 = r2_score(y_val_real, y_pred_real)\n",
        "\n",
        "print(f\"\\n--- Métricas de Avaliação ---\")\n",
        "print(f\"Erro Médio Absoluto (MAE): {mae:.4f}\")\n",
        "print(f\"Erro Quadrático Médio (MSE): {mse:.4f}\")\n",
        "print(f\"Raiz do Erro Quadrático Médio (RMSE): {rmse:.4f}\")\n",
        "print(f\"Coeficiente de Determinação (R²): {r2:.4f}\")\n",
        "print(f\"Erro percentual médio: {mae*100:.2f} pontos percentuais\")\n",
        "\n",
        "# --- 5. Plotando o gráfico de Previsão vs. Real ---\n",
        "plt.subplot(2, 2, 3)\n",
        "plt.plot(y_val_real, label='Valores Reais', marker='o', linewidth=2, markersize=6)\n",
        "plt.plot(y_pred_real, label='Previsões do Modelo', marker='x', linestyle='--', linewidth=2, markersize=6)\n",
        "plt.title('Comparação: Valores Reais vs. Previsões')\n",
        "plt.xlabel('Semanas (no conjunto de validação)')\n",
        "plt.ylabel('Proporção de Acidentes Severos')\n",
        "plt.legend()\n",
        "plt.grid(True, alpha=0.3)\n",
        "\n",
        "# --- 6. Gráfico de Resíduos ---\n",
        "plt.subplot(2, 2, 4)\n",
        "residuos = y_val_real - y_pred_real\n",
        "plt.scatter(y_pred_real, residuos, alpha=0.7)\n",
        "plt.axhline(y=0, color='red', linestyle='--')\n",
        "plt.title('Gráfico de Resíduos')\n",
        "plt.xlabel('Previsões')\n",
        "plt.ylabel('Resíduos (Real - Predito)')\n",
        "plt.grid(True, alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# --- 7. Análise de Performance ---\n",
        "print(f\"\\n--- Análise de Performance ---\")\n",
        "print(f\"Modelo treinado com {len(X_train)} amostras de treino\")\n",
        "print(f\"Modelo validado com {len(X_val)} amostras de validação\")\n",
        "print(f\"Features utilizadas: {len(features_disponiveis)}\")\n",
        "print(f\"Janela temporal: {n_passos_para_tras} semanas\")\n",
        "print(f\"Estados incluídos: {estados_principais}\")\n",
        "\n",
        "# --- 8. Salvando o modelo ---\n",
        "model_filename = 'modelo_lstm_acidentes_sp.keras'\n",
        "model.save(model_filename)\n",
        "\n",
        "print(f\"\\nModelo salvo com sucesso no arquivo: '{model_filename}'\")\n",
        "print(\"✅ Avaliação completa finalizada!\")\n",
        "print(\"📊 Gráficos exibidos acima\")\n",
        "print(\"🎯 Modelo pronto para uso\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Passo 7: Conclusão e Próximos Passos\n",
        "\n",
        "### Análise dos Resultados\n",
        "\n",
        "O treinamento do modelo LSTM para a previsão da proporção de acidentes severos demonstrou resultados promissores. O modelo foi desenvolvido com uma arquitetura robusta que inclui múltiplas camadas LSTM e técnicas de regularização adequadas.\n",
        "\n",
        "**Características do Modelo Desenvolvido:**\n",
        "\n",
        "- **Base de Dados Robusta:** Utilização de dados de múltiplos estados brasileiros (SP, MG, RJ, PR, RS, BA, CE, GO, PE, SC) para garantir diversidade geográfica e maior volume de dados para treinamento.\n",
        "\n",
        "- **Features Enriquecidas:** Incorporação de 6 features que incluem componentes temporais (dia da semana, mês, fim de semana), métricas de volume (pessoas e veículos envolvidos) e componentes de sazonalidade (seno e cosseno) para capturar padrões anuais.\n",
        "\n",
        "- **Contexto Temporal Adequado:** Janela temporal de 4 semanas fornece contexto histórico suficiente para o modelo aprender padrões temporais complexos na evolução da severidade dos acidentes.\n",
        "\n",
        "- **Arquitetura:** Modelo com 2 camadas LSTM (50 neurônios cada) e técnicas de regularização (Dropout) para prevenir overfitting.\n",
        "\n",
        "**Capacidades do Modelo:**\n",
        "\n",
        "O modelo apresenta capacidade de generalização, sendo treinado com dados diversos de diferentes regiões do país. A avaliação através de múltiplas métricas (MAE, MSE, RMSE, R²) e visualizações detalhadas permite uma análise abrangente da performance do modelo.\n",
        "\n",
        "Este modelo pode ser utilizado para apoiar decisões estratégicas de prevenção e análise de riscos nas rodovias federais, fornecendo previsões sobre a evolução da severidade dos acidentes com base em padrões históricos e características temporais.\n",
        "\n",
        "### Próximos Passos\n",
        "\n",
        "1. **Expandir o Dataset:** Incorporar mais estados e períodos históricos para aumentar a robustez do modelo.\n",
        "2. **Features Adicionais:** Adicionar features como condições climáticas, dados de tráfego e eventos especiais.\n",
        "3. **Otimização de Hiperparâmetros:** Realizar grid search para encontrar os melhores parâmetros do modelo.\n",
        "4. **Deploy:** Implementar o modelo em produção para uso em tempo real.\n",
        "5. **Monitoramento:** Estabelecer sistema de monitoramento contínuo da performance do modelo.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
