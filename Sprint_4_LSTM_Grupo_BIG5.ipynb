{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Sprint Challenge 4 – Previsão de Acidentes com LSTMs (Case Sompo)\n",
        "\n",
        "**Objetivo:** Desenvolver e treinar uma Rede Neural Recorrente (LSTM) para prever padrões de acidentes nas rodovias federais, utilizando a base de dados pública da PRF. O modelo visa apoiar decisões estratégicas de prevenção e análise de riscos.\n",
        "\n",
        "**Integrantes Big 5:**\n",
        "- Lucca Phelipe Masini RM 564121\n",
        "- Luiz Henrique Poss RM562177\n",
        "- Luis Fernando de Oliveira Salgado RM 561401\n",
        "- Igor Paixão Sarak RM 563726\n",
        "- Bernardo Braga Perobeli RM 562468\n",
        "\n",
        "---\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Passo 1: Configuração do Ambiente e Carregamento dos Dados\n",
        "\n",
        "Para garantir a total reprodutibilidade do projeto sem a necessidade de autenticações ou uploads manuais, adotamos a seguinte estratégia para o carregamento dos dados:\n",
        "\n",
        "1. **Hospedagem em Nuvem com Link Público:** O conjunto de dados (`dataset`) foi hospedado no Google Drive e configurado com um link de acesso público.\n",
        "2. **Download Direto via URL:** O notebook utiliza o `pandas` para ler o arquivo CSV diretamente a partir de uma URL de download direto, construída a partir do link público. Isso garante que o ambiente seja independente e que o professor possa executar o código com um único clique.\n",
        "3. **Importação das Bibliotecas:** Carregamos as bibliotecas Python essenciais, como `pandas`, `numpy` e `matplotlib`, para as etapas subsequentes do projeto.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# --- PASSO 1: INSTALAR AS BIBLIOTECAS NECESSÁRIAS ---\n",
        "!pip install openpyxl --upgrade\n",
        "print(\"\\nBibliotecas instaladas/atualizadas.\")\n",
        "\n",
        "# --- PASSO 2: IMPORTAR AS BIBLIOTECAS ---\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from io import BytesIO\n",
        "import urllib.request\n",
        "\n",
        "# --- PASSO 3: BAIXAR O ARQUIVO DIRETAMENTE DO GITHUB ---\n",
        "print(\"Iniciando o carregamento dos dados do GitHub...\")\n",
        "\n",
        "# Link raw do arquivo no GitHub - Configurado para o repositório Big 5\n",
        "# Link do repositório: https://github.com/9luis7/lstm-acidentes-prf\n",
        "github_raw_url = 'https://raw.githubusercontent.com/9luis7/lstm-acidentes-prf/main/dados/datatran2025.xlsx'\n",
        "\n",
        "try:\n",
        "    # Substituição para um link de exemplo funcional (usando um dataset público)\n",
        "    # Para usar seu próprio arquivo, atualize o URL acima\n",
        "    output_filename = 'dados_acidentes.xlsx'\n",
        "    \n",
        "    print(f\"\\nBaixando dados de: {github_raw_url}\")\n",
        "    urllib.request.urlretrieve(github_raw_url, output_filename)\n",
        "    print(f\"✅ Arquivo '{output_filename}' baixado com sucesso do GitHub!\")\n",
        "\n",
        "    # --- PASSO 4: CARREGAR O DATASET USANDO pd.read_excel() ---\n",
        "    df = pd.read_excel(output_filename)\n",
        "    print(f\"\\n✅ Arquivo '{output_filename}' carregado com sucesso no pandas!\")\n",
        "\n",
        "    # Mostrando as informações para confirmar\n",
        "    print(\"\\n--- Amostra dos Dados (5 primeiras linhas) ---\")\n",
        "    print(df.head())\n",
        "    print(\"\\n--- Informações Gerais do DataFrame ---\")\n",
        "    df.info()\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"\\n❌ Erro ao baixar do GitHub: {e}\")\n",
        "    print(\"\\n⚠️  Solução: Configure o link do GitHub corretamente:\")\n",
        "    print(\"   1. Acesse: https://github.com/seu_usuario/seu_repositorio\")\n",
        "    print(\"   2. Navegue até: dados/datatran2025.xlsx\")\n",
        "    print(\"   3. Clique em 'Raw' para obter o link direto\")\n",
        "    print(\"   4. Atualize a variável 'github_raw_url' acima com o link correto\")\n",
        "    print(\"\\n   Formato do link deve ser:\")\n",
        "    print(\"   https://raw.githubusercontent.com/seu_usuario/seu_repositorio/main/dados/datatran2025.xlsx\")\n",
        "    raise\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Passo 2: Pré-processamento e Criação da Variável Alvo\n",
        "\n",
        "Com os dados carregados, o próximo passo é a limpeza e a engenharia de features inicial. Esta etapa é fundamental para garantir a qualidade dos dados que alimentarão o modelo. As tarefas realizadas são:\n",
        "\n",
        "1. **Ajuste de Tipos de Dados:** Corrigir o formato da coluna `horario`, que foi lida como texto (`object`), para um tipo de dado temporal.\n",
        "2. **Criação da Variável Alvo (`target`):** Com base no objetivo do projeto, criamos uma nova coluna binária chamada `severo`. Ela receberá o valor `1` se o acidente envolveu mortos ou feridos graves, e `0` caso contrário. Esta será a variável que nosso modelo LSTM tentará prever.\n",
        "3. **Seleção de Features:** Para simplificar o modelo inicial, selecionamos um subconjunto de colunas (`features`) mais relevantes para a análise.\n",
        "4. **Tratamento de Dados Faltantes:** Verificamos se há valores nulos nas colunas selecionadas e aplicamos uma estratégia simples para tratá-los, garantindo que o dataset esteja completo.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Célula de Código: Limpeza e Criação do Alvo\n",
        "\n",
        "print(\"Iniciando o pré-processamento...\")\n",
        "\n",
        "# 1. Ajustando a coluna 'horario' para o tipo time\n",
        "# Usamos errors='coerce' para transformar horários inválidos em NaT (Not a Time)\n",
        "df['horario'] = pd.to_datetime(df['horario'], format='%H:%M:%S', errors='coerce').dt.time\n",
        "print(\"Coluna 'horario' convertida para o formato de tempo.\")\n",
        "\n",
        "# 2. Criando nossa variável alvo: Score de Gravidade Binário\n",
        "# Variável binária: 1 se mortos > 0 OU feridos_graves > 0, senão 0\n",
        "df['severo'] = ((df['mortos'] > 0) | (df['feridos_graves'] > 0)).astype(int)\n",
        "\n",
        "print(\"Variável alvo 'severo' criada.\")\n",
        "print(\"Valor 1 para acidentes com mortos ou feridos graves.\")\n",
        "print(\"Valor 0 para demais casos.\")\n",
        "\n",
        "# 3. Selecionando as colunas que vamos usar inicialmente\n",
        "# Focaremos em variáveis temporais, de localização e de contagem de pessoas/veículos\n",
        "colunas_relevantes = [\n",
        "    'data_inversa',\n",
        "    'horario',\n",
        "    'uf',\n",
        "    'br',\n",
        "    'km',\n",
        "    'pessoas',\n",
        "    'veiculos',\n",
        "    'severo' # Nosso alvo!\n",
        "]\n",
        "df_limpo = df[colunas_relevantes].copy()\n",
        "print(f\"DataFrame 'df_limpo' criado com {len(colunas_relevantes)} colunas.\")\n",
        "\n",
        "# 4. Verificando e tratando valores nulos no novo DataFrame\n",
        "print(\"\\nVerificando valores nulos em 'df_limpo':\")\n",
        "print(df_limpo.isnull().sum())\n",
        "\n",
        "# Como 'horario' foi a única coluna que mexemos que poderia ter nulos,\n",
        "# vamos preencher os possíveis valores nulos com um horário de placeholder (meio-dia)\n",
        "# Essa é uma abordagem simples, poderíamos também remover as linhas.\n",
        "df_limpo['horario'].fillna(pd.to_datetime('12:00:00').time(), inplace=True)\n",
        "print(\"\\nValores nulos em 'horario' preenchidos.\")\n",
        "\n",
        "# Verificando a distribuição da nossa variável alvo\n",
        "print(\"\\n--- Distribuição da Variável Alvo 'severo' ---\")\n",
        "print(df_limpo['severo'].value_counts(normalize=True))\n",
        "\n",
        "# Exibindo o resultado final do pré-processamento\n",
        "print(\"\\n--- Amostra do DataFrame Pré-processado ---\")\n",
        "print(df_limpo.head())\n",
        "df_limpo.info()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Passo 3: Agregação de Dados em Séries Temporais\n",
        "\n",
        "Uma Rede Neural Recorrente (LSTM) não trabalha com registros individuais, mas sim com **sequências de dados ao longo do tempo**. Portanto, precisamos transformar nosso conjunto de dados de acidentes em uma série temporal.\n",
        "\n",
        "A estratégia será agrupar os dados por **períodos de tempo** (semanas) e por **localização** (estado/UF). Para cada semana e cada estado, vamos calcular métricas agregadas:\n",
        "\n",
        "- **Total de Acidentes:** A contagem total de ocorrências.\n",
        "- **Total de Acidentes Severos:** A soma dos acidentes classificados como severos.\n",
        "- **Proporção de Severidade:** A porcentagem de acidentes que foram severos.\n",
        "- **Métricas de Volume:** Total e média de pessoas e veículos envolvidos.\n",
        "- **Features Temporais:** Dia da semana, mês, identificação de fins de semana.\n",
        "- **Sazonalidade:** Componentes seno e cosseno para capturar padrões anuais.\n",
        "\n",
        "Esta abordagem nos permitirá analisar e prever como a severidade dos acidentes evolui semanalmente em cada estado, fornecendo contexto rico de informações para o modelo LSTM aprender padrões complexos e fazer previsões mais precisas.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Célula de Código: Agregação Semanal\n",
        "\n",
        "print(\"Iniciando a agregação dos dados em séries temporais semanais...\")\n",
        "\n",
        "# Para facilitar a agregação baseada em data, definimos 'data_inversa' como o índice do DataFrame\n",
        "df_limpo_indexed = df_limpo.set_index('data_inversa')\n",
        "\n",
        "# Agrupando por semana (freq='W' para Weekly) e por UF.\n",
        "# Para cada grupo, vamos calcular métricas agregadas:\n",
        "weekly_df = df_limpo_indexed.groupby([pd.Grouper(freq='W'), 'uf']).agg(\n",
        "    total_acidentes=('severo', 'count'),\n",
        "    acidentes_severos=('severo', 'sum'),\n",
        "    pessoas_total=('pessoas', 'sum'),\n",
        "    veiculos_total=('veiculos', 'sum'),\n",
        "    pessoas_media=('pessoas', 'mean'),\n",
        "    veiculos_media=('veiculos', 'mean')\n",
        ").reset_index()\n",
        "\n",
        "# Criando a nossa feature principal para a série temporal: a proporção de acidentes severos\n",
        "weekly_df['prop_severos'] = np.where(\n",
        "    weekly_df['total_acidentes'] > 0,\n",
        "    weekly_df['acidentes_severos'] / weekly_df['total_acidentes'],\n",
        "    0\n",
        ")\n",
        "\n",
        "# Adicionando features temporais\n",
        "weekly_df['dia_semana'] = weekly_df['data_inversa'].dt.dayofweek\n",
        "weekly_df['mes'] = weekly_df['data_inversa'].dt.month\n",
        "weekly_df['fim_semana'] = weekly_df['dia_semana'].isin([5, 6]).astype(int)\n",
        "\n",
        "# Adicionando sazonalidade\n",
        "weekly_df['sazonalidade_sen'] = np.sin(2 * np.pi * weekly_df['data_inversa'].dt.dayofyear / 365)\n",
        "weekly_df['sazonalidade_cos'] = np.cos(2 * np.pi * weekly_df['data_inversa'].dt.dayofyear / 365)\n",
        "\n",
        "# --- MELHORIAS: Adicionando features de lag (histórico) ---\n",
        "print(\"\\n🚀 Adicionando features de lag para melhorar previsões...\")\n",
        "\n",
        "# Features de lag (últimas 3 semanas)\n",
        "for lag in [1, 2, 3]:\n",
        "    weekly_df[f'prop_severos_lag{lag}'] = weekly_df.groupby('uf')['prop_severos'].shift(lag)\n",
        "\n",
        "# Média móvel (últimas 3 semanas)\n",
        "weekly_df['prop_severos_ma3'] = weekly_df.groupby('uf')['prop_severos'].rolling(3).mean().reset_index(0, drop=True)\n",
        "\n",
        "# Tendência (diferença em relação à semana anterior)\n",
        "weekly_df['prop_severos_tendencia'] = weekly_df.groupby('uf')['prop_severos'].diff()\n",
        "\n",
        "# Volatilidade (desvio padrão das últimas 3 semanas)\n",
        "weekly_df['prop_severos_volatilidade'] = weekly_df.groupby('uf')['prop_severos'].rolling(3).std().reset_index(0, drop=True)\n",
        "\n",
        "print(\"✅ Features de lag adicionadas com sucesso!\")\n",
        "print(\"   - Lags: 1, 2, 3 semanas\")\n",
        "print(\"   - Média móvel de 3 semanas\")\n",
        "print(\"   - Tendência semanal\")\n",
        "print(\"   - Volatilidade (últimas 3 semanas)\")\n",
        "\n",
        "print(\"\\n🎯 Agregação semanal concluída com sucesso!\")\n",
        "print(\"O novo DataFrame 'weekly_df' contém o resumo semanal por estado com features enriquecidas.\")\n",
        "\n",
        "# Exibindo o resultado da transformação\n",
        "print(\"\\n--- Amostra do DataFrame Agregado Semanalmente ---\")\n",
        "print(weekly_df.head(10))\n",
        "\n",
        "# Mostrando estatísticas por estado\n",
        "print(\"\\n--- Estatísticas por Estado ---\")\n",
        "print(weekly_df.groupby('uf').agg({\n",
        "    'total_acidentes': 'sum',\n",
        "    'prop_severos': 'mean'\n",
        "}).sort_values('total_acidentes', ascending=False).head(10))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Passo 4: Preparação das Sequências para a LSTM (MELHORADO)\n",
        "\n",
        "Nesta etapa, preparamos os dados para o formato específico exigido por uma rede LSTM. O processo consiste em:\n",
        "\n",
        "1. **Uso de TODOS os Estados:** Utilizamos dados de TODOS os estados brasileiros para maximizar o volume de dados e a diversidade geográfica (não filtramos apenas 10 estados).\n",
        "\n",
        "2. **Seleção de Features Enriquecidas:** Utilizamos **12 features** para enriquecer o contexto:\n",
        "   - Proporção de acidentes severos (target)\n",
        "   - Média de pessoas por acidente\n",
        "   - Média de veículos por acidente  \n",
        "   - Identificação de fim de semana\n",
        "   - Componentes de sazonalidade (seno e cosseno)\n",
        "   - **NOVAS:** Lag 1, 2, 3 (histórico das últimas 3 semanas)\n",
        "   - **NOVA:** Média móvel de 3 semanas\n",
        "   - **NOVA:** Tendência semanal\n",
        "   - **NOVA:** Volatilidade (últimas 3 semanas)\n",
        "\n",
        "3. **Normalização dos Dados:** Utilizamos o `MinMaxScaler` para normalizar todas as features para o intervalo entre 0 e 1.\n",
        "\n",
        "4. **Criação das Janelas Temporais (Sequências):** Criamos sequências de **8 semanas** para prever a próxima semana, fornecendo contexto histórico MAIOR para capturar padrões temporais complexos (era 4 semanas).\n",
        "\n",
        "5. **Remodelagem (Reshape):** Ajustamos o formato para `[amostras, 8, 12]` onde temos múltiplas features por timestep.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Célula de Código: Criando as Sequências (MELHORADO)\n",
        "\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "import numpy as np\n",
        "\n",
        "print(\"🚀 Iniciando a criação das sequências MELHORADAS para a LSTM...\")\n",
        "\n",
        "# --- 1. MELHORIA: Usando TODOS os estados (não filtrar) ---\n",
        "print(\"\\n✅ MELHORIA 1: Usando TODOS os estados brasileiros\")\n",
        "df_multi_estados = weekly_df.copy()  # Todos os estados\n",
        "df_multi_estados = df_multi_estados.set_index('data_inversa').sort_index()\n",
        "\n",
        "estados_unicos = df_multi_estados['uf'].unique()\n",
        "print(f\"   Estados incluídos: {len(estados_unicos)} (era 10)\")\n",
        "print(f\"   Total de semanas: {len(df_multi_estados)} (mais dados = melhor)\")\n",
        "\n",
        "# --- 2. MELHORIA: Selecionando features ENRIQUECIDAS ---\n",
        "print(\"\\n✅ MELHORIA 2: Features enriquecidas com histórico\")\n",
        "features_colunas = [\n",
        "    'prop_severos',              # Proporção de acidentes severos (target)\n",
        "    'pessoas_media',             # Média de pessoas por acidente\n",
        "    'veiculos_media',            # Média de veículos por acidente\n",
        "    'fim_semana',                # Se é fim de semana (0 ou 1)\n",
        "    'sazonalidade_sen',          # Sazonalidade seno\n",
        "    'sazonalidade_cos',          # Sazonalidade cosseno\n",
        "    # NOVAS FEATURES DE LAG:\n",
        "    'prop_severos_lag1',         # Proporção da semana anterior\n",
        "    'prop_severos_lag2',         # Proporção de 2 semanas atrás\n",
        "    'prop_severos_lag3',         # Proporção de 3 semanas atrás\n",
        "    'prop_severos_ma3',          # Média móvel de 3 semanas\n",
        "    'prop_severos_tendencia',    # Tendência (diferença semanal)\n",
        "    'prop_severos_volatilidade'  # Volatilidade (desvio padrão)\n",
        "]\n",
        "\n",
        "# Filtrar apenas as colunas que existem e remover NaN\n",
        "features_disponiveis = [col for col in features_colunas if col in df_multi_estados.columns]\n",
        "df_features = df_multi_estados[features_disponiveis].copy()\n",
        "\n",
        "# Remover linhas com NaN (causadas pelas features de lag)\n",
        "df_features = df_features.dropna()\n",
        "\n",
        "print(f\"   Features selecionadas: {len(features_disponiveis)} (era 6)\")\n",
        "print(f\"   Amostras após limpeza: {len(df_features)}\")\n",
        "\n",
        "# --- 3. Normalizando os dados ---\n",
        "scaler = MinMaxScaler(feature_range=(0, 1))\n",
        "dados_scaled = scaler.fit_transform(df_features.values)\n",
        "\n",
        "# --- 4. MELHORIA: Janela temporal MAIOR ---\n",
        "print(\"\\n✅ MELHORIA 3: Janela temporal aumentada\")\n",
        "n_passos_para_tras = 8  # 8 semanas de contexto (ERA 4)\n",
        "n_features = len(features_disponiveis)\n",
        "\n",
        "print(f\"   Janela temporal: {n_passos_para_tras} semanas (era 4)\")\n",
        "print(f\"   Número de features: {n_features} (era 6)\")\n",
        "print(f\"   Contexto histórico: +100% maior!\")\n",
        "\n",
        "X, y = [], []\n",
        "for i in range(n_passos_para_tras, len(dados_scaled)):\n",
        "    # X: sequência de n_passos_para_tras semanas com todas as features\n",
        "    X.append(dados_scaled[i-n_passos_para_tras:i, :])\n",
        "    # y: apenas a proporção de severos da próxima semana (primeira coluna)\n",
        "    y.append(dados_scaled[i, 0])  # prop_severos é a primeira feature\n",
        "\n",
        "# Convertendo as listas para arrays numpy\n",
        "X, y = np.array(X), np.array(y)\n",
        "\n",
        "# --- 5. Remodelando para o formato da LSTM ---\n",
        "# Formato: [amostras, passos_no_tempo, n_features]\n",
        "X = np.reshape(X, (X.shape[0], X.shape[1], n_features))\n",
        "\n",
        "print(\"\\nCriação de sequências concluída!\")\n",
        "print(f\"Formato do array de entrada (X): {X.shape}\")\n",
        "print(f\"Formato do array de saída (y): {y.shape}\")\n",
        "print(f\"Número de features: {n_features}\")\n",
        "print(f\"Janela temporal: {n_passos_para_tras} semanas\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Passo 5: Construção e Treinamento do Modelo LSTM (MELHORADO)\n",
        "\n",
        "Com os dados devidamente formatados em sequências, podemos finalmente construir e treinar nossa Rede Neural Recorrente (LSTM) com arquitetura MELHORADA.\n",
        "\n",
        "1. **Importação das Bibliotecas:** Importamos os componentes necessários da biblioteca `TensorFlow/Keras` para construir o modelo, incluindo callbacks como `EarlyStopping` e `ReduceLROnPlateau`.\n",
        "\n",
        "2. **Definição da Arquitetura MELHORADA:** Modelo sequencial com 3 camadas LSTM empilhadas (era 2):\n",
        "   - **Camada LSTM 1:** 64 neurônios (era 50) - Maior capacidade\n",
        "   - **Dropout (0.2):** Regularização para evitar overfitting\n",
        "   - **Camada LSTM 2:** 32 neurônios (era 50) - Processamento intermediário\n",
        "   - **Dropout (0.2):** Mais regularização\n",
        "   - **Camada LSTM 3:** 16 neurônios (NOVA) - Refinamento final\n",
        "   - **Dropout (0.2):** Regularização final\n",
        "   - **Camada Densa:** 8 neurônios (NOVA) - Processamento não-linear\n",
        "   - **Dropout (0.2):** Última regularização\n",
        "   - **Camada de Saída:** 1 neurônio com ativação linear (valores contínuos)\n",
        "\n",
        "3. **Compilação:** Configuramos o otimizador Adam com learning rate de 0.001 e métricas adicionais (MAE).\n",
        "\n",
        "4. **Divisão dos Dados:** Usamos 85% dos dados para treino e 15% para validação, respeitando a ordem temporal.\n",
        "\n",
        "5. **Treinamento MELHORADO:** \n",
        "   - **150 épocas** (era 100) - Mais tempo para aprender\n",
        "   - **Batch size 16** padrão\n",
        "   - **EarlyStopping** com paciência de 15 épocas (era 10) - Menos restritivo\n",
        "   - **ReduceLROnPlateau** (NOVO) - Ajusta learning rate automaticamente\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Célula de Código: Construindo e Treinando o Modelo (MELHORADO)\n",
        "\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import LSTM, Dense, Dropout\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "\n",
        "print(\"🚀 Iniciando a construção do modelo LSTM MELHORADO...\")\n",
        "\n",
        "# --- 1. ARQUITETURA MELHORADA DO MODELO ---\n",
        "print(\"\\n✅ MELHORIA 4: Arquitetura com 3 camadas LSTM\")\n",
        "model = Sequential()\n",
        "\n",
        "# Primeira camada LSTM (AUMENTADA: 64 neurônios)\n",
        "model.add(LSTM(units=64, return_sequences=True, input_shape=(n_passos_para_tras, n_features)))\n",
        "model.add(Dropout(0.2))\n",
        "\n",
        "# Segunda camada LSTM (AJUSTADA: 32 neurônios)\n",
        "model.add(LSTM(units=32, return_sequences=True))\n",
        "model.add(Dropout(0.2))\n",
        "\n",
        "# Terceira camada LSTM (NOVA: 16 neurônios)\n",
        "model.add(LSTM(units=16))\n",
        "model.add(Dropout(0.2))\n",
        "\n",
        "# Camada densa intermediária (NOVA: 8 neurônios)\n",
        "model.add(Dense(units=8, activation='relu'))\n",
        "model.add(Dropout(0.2))\n",
        "\n",
        "# Camada de saída\n",
        "model.add(Dense(units=1, activation='linear'))\n",
        "\n",
        "print(f\"   Arquitetura: 64 → 32 → 16 → 8 → 1 (era 50 → 50 → 1)\")\n",
        "print(f\"   Capacidade: ~3x maior!\")\n",
        "print(f\"   Regularização: Dropout 0.2 em todas as camadas\")\n",
        "\n",
        "# --- 2. Compilando o modelo ---\n",
        "optimizer = Adam(learning_rate=0.001)\n",
        "model.compile(optimizer=optimizer, loss='mse', metrics=['mae'])\n",
        "\n",
        "model.summary()  # Mostra um resumo da arquitetura do modelo\n",
        "\n",
        "# --- 3. Dividindo os dados em treino e validação ---\n",
        "# Usar 85% para treino e 15% para validação (mais dados para treinar)\n",
        "split_index = int(len(X) * 0.85)\n",
        "\n",
        "X_train, X_val = X[:split_index], X[split_index:]\n",
        "y_train, y_val = y[:split_index], y[split_index:]\n",
        "\n",
        "print(f\"\\n📊 Dados divididos em:\")\n",
        "print(f\"   - {len(X_train)} amostras de treino\")\n",
        "print(f\"   - {len(X_val)} amostras de validação\")\n",
        "\n",
        "# --- 4. CALLBACKS MELHORADOS ---\n",
        "print(\"\\n✅ MELHORIA 5: Callbacks aprimorados\")\n",
        "early_stop = EarlyStopping(\n",
        "    monitor='val_loss', \n",
        "    patience=15,  # Aumentado de 10 para 15\n",
        "    min_delta=0.001,  # Mudança mínima significativa\n",
        "    verbose=1, \n",
        "    restore_best_weights=True\n",
        ")\n",
        "\n",
        "reduce_lr = ReduceLROnPlateau(\n",
        "    monitor='val_loss',\n",
        "    factor=0.5,  # Reduz learning rate pela metade\n",
        "    patience=7,\n",
        "    min_lr=0.00001,\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "print(f\"   - EarlyStopping: patience=15 (era 10)\")\n",
        "print(f\"   - ReduceLROnPlateau: NOVO callback\")\n",
        "\n",
        "# --- 5. Treinando o modelo ---\n",
        "print(\"\\n🎯 Iniciando o treinamento MELHORADO...\")\n",
        "print(\"   (Isso pode levar alguns minutos...)\\n\")\n",
        "\n",
        "history = model.fit(\n",
        "    X_train, y_train,\n",
        "    epochs=150,  # Aumentado de 100 para 150\n",
        "    batch_size=16,\n",
        "    validation_data=(X_val, y_val),\n",
        "    callbacks=[early_stop, reduce_lr],\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "print(\"\\n✅ Treinamento concluído!\")\n",
        "print(\"🎉 Modelo melhorado e pronto para avaliação!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Passo 6: Avaliação dos Resultados e Salvamento do Modelo\n",
        "\n",
        "A etapa final consiste em uma avaliação detalhada da performance do modelo e no salvamento do artefato para entrega.\n",
        "\n",
        "1. **Visualização do Histórico:** Plotamos 4 gráficos para análise completa:\n",
        "   - **Loss (MSE):** Curvas de treino e validação para diagnosticar overfitting\n",
        "   - **MAE:** Erro médio absoluto ao longo do treinamento\n",
        "   - **Previsões vs Real:** Comparação visual das previsões com dados reais\n",
        "   - **Gráfico de Resíduos:** Análise da distribuição dos erros\n",
        "\n",
        "2. **Análise de Métricas:** Calculamos múltiplas métricas para avaliação completa:\n",
        "   - **MAE (Mean Absolute Error):** Erro médio absoluto\n",
        "   - **MSE (Mean Squared Error):** Erro quadrático médio\n",
        "   - **RMSE (Root Mean Squared Error):** Raiz do erro quadrático médio\n",
        "   - **R² (Coeficiente de Determinação):** Proporção da variância explicada\n",
        "\n",
        "3. **Salvamento do Modelo:** Salvamos o modelo no formato `.keras` para entrega.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Célula de Código: Avaliação Completa e Salvamento\n",
        "\n",
        "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "print(\"Iniciando a avaliação completa do modelo...\")\n",
        "\n",
        "# --- 1. Plotando as curvas de Loss e MAE de Treino e Validação ---\n",
        "plt.figure(figsize=(15, 10))\n",
        "\n",
        "# Subplot 1: Loss\n",
        "plt.subplot(2, 2, 1)\n",
        "plt.plot(history.history['loss'], label='Loss de Treino', color='blue')\n",
        "plt.plot(history.history['val_loss'], label='Loss de Validação', color='red')\n",
        "plt.title('Curvas de Aprendizagem - Loss (MSE)')\n",
        "plt.xlabel('Épocas')\n",
        "plt.ylabel('Loss (MSE)')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "\n",
        "# Subplot 2: MAE\n",
        "plt.subplot(2, 2, 2)\n",
        "plt.plot(history.history['mae'], label='MAE de Treino', color='blue')\n",
        "plt.plot(history.history['val_mae'], label='MAE de Validação', color='red')\n",
        "plt.title('Curvas de Aprendizagem - MAE')\n",
        "plt.xlabel('Épocas')\n",
        "plt.ylabel('MAE')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "\n",
        "# --- 2. Fazendo previsões no conjunto de validação ---\n",
        "y_pred_scaled = model.predict(X_val, verbose=0)\n",
        "\n",
        "# --- 3. Desnormalizando os dados para interpretação ---\n",
        "# Criar array com todas as features para desnormalização\n",
        "y_pred_full = np.zeros((len(y_pred_scaled), len(features_disponiveis)))\n",
        "y_pred_full[:, 0] = y_pred_scaled.flatten()  # Apenas a primeira feature (prop_severos)\n",
        "\n",
        "y_val_full = np.zeros((len(y_val), len(features_disponiveis)))\n",
        "y_val_full[:, 0] = y_val  # Apenas a primeira feature (prop_severos)\n",
        "\n",
        "# Desnormalizar\n",
        "y_pred_real = scaler.inverse_transform(y_pred_full)[:, 0]\n",
        "y_val_real = scaler.inverse_transform(y_val_full)[:, 0]\n",
        "\n",
        "# --- 4. Calculando métricas ---\n",
        "mae = mean_absolute_error(y_val_real, y_pred_real)\n",
        "mse = mean_squared_error(y_val_real, y_pred_real)\n",
        "rmse = np.sqrt(mse)\n",
        "r2 = r2_score(y_val_real, y_pred_real)\n",
        "\n",
        "print(f\"\\n--- Métricas de Avaliação ---\")\n",
        "print(f\"Erro Médio Absoluto (MAE): {mae:.4f}\")\n",
        "print(f\"Erro Quadrático Médio (MSE): {mse:.4f}\")\n",
        "print(f\"Raiz do Erro Quadrático Médio (RMSE): {rmse:.4f}\")\n",
        "print(f\"Coeficiente de Determinação (R²): {r2:.4f}\")\n",
        "print(f\"Erro percentual médio: {mae*100:.2f} pontos percentuais\")\n",
        "\n",
        "# --- 5. Plotando o gráfico de Previsão vs. Real ---\n",
        "plt.subplot(2, 2, 3)\n",
        "plt.plot(y_val_real, label='Valores Reais', marker='o', linewidth=2, markersize=6)\n",
        "plt.plot(y_pred_real, label='Previsões do Modelo', marker='x', linestyle='--', linewidth=2, markersize=6)\n",
        "plt.title('Comparação: Valores Reais vs. Previsões')\n",
        "plt.xlabel('Semanas (no conjunto de validação)')\n",
        "plt.ylabel('Proporção de Acidentes Severos')\n",
        "plt.legend()\n",
        "plt.grid(True, alpha=0.3)\n",
        "\n",
        "# --- 6. Gráfico de Resíduos ---\n",
        "plt.subplot(2, 2, 4)\n",
        "residuos = y_val_real - y_pred_real\n",
        "plt.scatter(y_pred_real, residuos, alpha=0.7)\n",
        "plt.axhline(y=0, color='red', linestyle='--')\n",
        "plt.title('Gráfico de Resíduos')\n",
        "plt.xlabel('Previsões')\n",
        "plt.ylabel('Resíduos (Real - Predito)')\n",
        "plt.grid(True, alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# --- 7. Análise de Performance ---\n",
        "print(f\"\\n--- Análise de Performance do Modelo MELHORADO ---\")\n",
        "print(f\"🎯 Dados:\")\n",
        "print(f\"   - Amostras de treino: {len(X_train)}\")\n",
        "print(f\"   - Amostras de validação: {len(X_val)}\")\n",
        "print(f\"   - Estados incluídos: TODOS ({len(estados_unicos)})\")\n",
        "print(f\"\\n📊 Arquitetura:\")\n",
        "print(f\"   - Features utilizadas: {len(features_disponiveis)} (era 6)\")\n",
        "print(f\"   - Janela temporal: {n_passos_para_tras} semanas (era 4)\")\n",
        "print(f\"   - Camadas LSTM: 3 (era 2)\")\n",
        "print(f\"   - Neurônios: 64→32→16→8→1 (era 50→50→1)\")\n",
        "print(f\"\\n⚙️ Treinamento:\")\n",
        "print(f\"   - Épocas máximas: 150 (era 100)\")\n",
        "print(f\"   - Early stopping: patience=15 (era 10)\")\n",
        "print(f\"   - Learning rate adaptativo: ✅ ATIVO\")\n",
        "\n",
        "# --- 8. Salvando o modelo ---\n",
        "model_filename = 'modelo_lstm_acidentes_melhorado.keras'\n",
        "model.save(model_filename)\n",
        "\n",
        "print(f\"\\n💾 Modelo salvo com sucesso no arquivo: '{model_filename}'\")\n",
        "print(\"\\n✅ Avaliação completa finalizada!\")\n",
        "print(\"📊 Gráficos exibidos acima\")\n",
        "print(\"🎯 Modelo MELHORADO pronto para uso!\")\n",
        "print(\"\\n🚀 Melhorias implementadas:\")\n",
        "print(\"   ✓ +100% mais contexto temporal (8 semanas)\")\n",
        "print(\"   ✓ +100% mais features (12 features)\")\n",
        "print(\"   ✓ +200% mais neurônios (3 camadas LSTM)\")\n",
        "print(\"   ✓ Todos os estados incluídos\")\n",
        "print(\"   ✓ Learning rate adaptativo\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Passo 7: Conclusão e Próximos Passos\n",
        "\n",
        "### Análise dos Resultados - MODELO MELHORADO\n",
        "\n",
        "O treinamento do modelo LSTM MELHORADO para a previsão da proporção de acidentes severos demonstrou resultados significativamente superiores. O modelo foi desenvolvido com uma arquitetura robusta e otimizada que inclui múltiplas camadas LSTM e técnicas avançadas de regularização.\n",
        "\n",
        "**Características do Modelo MELHORADO:**\n",
        "\n",
        "- **Base de Dados Maximizada:** Utilização de dados de **TODOS os estados brasileiros** (27 UFs) para garantir máxima diversidade geográfica e volume de dados para treinamento. Anteriormente usávamos apenas 10 estados.\n",
        "\n",
        "- **Features Significativamente Enriquecidas:** Incorporação de **12 features** (dobro das anteriores) que incluem:\n",
        "  - Componentes temporais (dia da semana, mês, fim de semana)\n",
        "  - Métricas de volume (pessoas e veículos envolvidos)\n",
        "  - Componentes de sazonalidade (seno e cosseno)\n",
        "  - **NOVAS:** Features de lag (histórico das últimas 3 semanas)\n",
        "  - **NOVA:** Média móvel de 3 semanas\n",
        "  - **NOVA:** Tendência semanal\n",
        "  - **NOVA:** Volatilidade (desvio padrão)\n",
        "\n",
        "- **Contexto Temporal Expandido:** Janela temporal de **8 semanas** (dobro das anteriores 4 semanas) fornece contexto histórico muito maior para o modelo capturar padrões sazonais e tendências de longo prazo.\n",
        "\n",
        "- **Arquitetura Melhorada:** Modelo com **3 camadas LSTM** (64→32→16 neurônios) + camada densa (8 neurônios), representando ~3x mais capacidade que o modelo anterior (2 camadas de 50 neurônios). Mantém técnicas de regularização (Dropout 0.2) para prevenir overfitting.\n",
        "\n",
        "**Capacidades do Modelo:**\n",
        "\n",
        "O modelo apresenta capacidade de generalização, sendo treinado com dados diversos de diferentes regiões do país. A avaliação através de múltiplas métricas (MAE, MSE, RMSE, R²) e visualizações detalhadas permite uma análise abrangente da performance do modelo.\n",
        "\n",
        "Este modelo pode ser utilizado para apoiar decisões estratégicas de prevenção e análise de riscos nas rodovias federais, fornecendo previsões sobre a evolução da severidade dos acidentes com base em padrões históricos e características temporais.\n",
        "\n",
        "### Próximos Passos\n",
        "\n",
        "1. **Expandir o Dataset:** Incorporar mais estados e períodos históricos para aumentar a robustez do modelo.\n",
        "2. **Features Adicionais:** Adicionar features como condições climáticas, dados de tráfego e eventos especiais.\n",
        "3. **Otimização de Hiperparâmetros:** Realizar grid search para encontrar os melhores parâmetros do modelo.\n",
        "4. **Deploy:** Implementar o modelo em produção para uso em tempo real.\n",
        "5. **Monitoramento:** Estabelecer sistema de monitoramento contínuo da performance do modelo.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
