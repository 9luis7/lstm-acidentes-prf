{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üöó Sprint Challenge 4 ‚Äì Previs√£o de Acidentes com LSTMs\n",
    "## Case Sompo: Antecipando Padr√µes de Risco em Rodovias Brasileiras\n",
    "\n",
    "---\n",
    "\n",
    "**Equipe Big 5**\n",
    "- Lucca Phelipe Masini - RM 564121\n",
    "- Luiz Henrique Poss - RM 562177\n",
    "- Luis Fernando de Oliveira Salgado - RM 561401\n",
    "- Igor Paix√£o Sarak - RM 563726\n",
    "- Bernardo Braga Perobeli - RM 562468\n",
    "\n",
    "---\n",
    "\n",
    "## üéØ Target Escolhido: Classifica√ß√£o de 4 N√≠veis de Risco\n",
    "\n",
    "- **Classe 0 - BAIXO**: < 20% de acidentes severos\n",
    "- **Classe 1 - M√âDIO-BAIXO**: 20-30% de acidentes severos\n",
    "- **Classe 2 - M√âDIO-ALTO**: 30-40% de acidentes severos\n",
    "- **Classe 3 - ALTO**: ‚â• 40% de acidentes severos\n",
    "\n",
    "### Justificativa\n",
    "\n",
    "Inicialmente tentamos **regress√£o** (prever propor√ß√£o exata), mas obtivemos R¬≤ negativo - as features dispon√≠veis n√£o capturam fatores cr√≠ticos como clima e eventos. Reformulamos como **classifica√ß√£o** - mais robusta e √∫til na pr√°tica."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üìö Passo 1: Instala√ß√£o e Importa√ß√£o de Bibliotecas\n",
    "\n",
    "Primeiro, vamos instalar e importar todas as bibliotecas necess√°rias para o projeto.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install openpyxl --quiet\n",
    "print(\"‚úÖ Bibliotecas instaladas!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üì• Passo 2: Carregamento dos Dados\n",
    "\n",
    "Carregamos os dados diretamente do GitHub para garantir reprodutibilidade total.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import urllib.request\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "print(\"üìö Bibliotecas importadas!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üéØ Passo 3: Pr√©-processamento e Cria√ß√£o da Vari√°vel Target\n",
    "\n",
    "Criamos a vari√°vel bin√°ria `severo` que identifica acidentes com mortos ou feridos graves.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"üì• Baixando dataset da PRF do GitHub...\")\n",
    "\n",
    "github_raw_url = 'https://raw.githubusercontent.com/9luis7/lstm-acidentes-prf/main/dados/datatran2025.xlsx'\n",
    "output_filename = 'dados_acidentes.xlsx'\n",
    "\n",
    "try:\n",
    "    urllib.request.urlretrieve(github_raw_url, output_filename)\n",
    "    df = pd.read_excel(output_filename)\n",
    "    print(f\"‚úÖ Dataset carregado: {len(df):,} acidentes\")\n",
    "    print(\"\\nüìä Per√≠odo:\", df['data_inversa'].min(), \"at√©\", df['data_inversa'].max())\n",
    "    print(\"üìä Estados:\", df['uf'].nunique(), \"UFs\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Erro: {e}\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üîÑ Passo 4: Agrega√ß√£o Semanal\n",
    "\n",
    "Transformamos acidentes individuais em s√©ries temporais semanais por estado.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"üéØ Criando vari√°vel target 'severo'...\")\n",
    "\n",
    "df['horario'] = pd.to_datetime(df['horario'], format='%H:%M:%S', errors='coerce').dt.time\n",
    "df['severo'] = ((df['mortos'] > 0) | (df['feridos_graves'] > 0)).astype(int)\n",
    "\n",
    "colunas_relevantes = ['data_inversa', 'horario', 'uf', 'br', 'km', 'pessoas', 'veiculos', 'severo']\n",
    "df_limpo = df[colunas_relevantes].copy()\n",
    "df_limpo['horario'].fillna(pd.to_datetime('12:00:00').time(), inplace=True)\n",
    "\n",
    "print(\"‚úÖ Vari√°vel 'severo' criada!\")\n",
    "print(\"\\nüìä Distribui√ß√£o:\")\n",
    "print(df_limpo['severo'].value_counts(normalize=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üé® Passo 5: Feature Engineering\n",
    "\n",
    "Criamos 12 features enriquecidas: temporais, sazonalidade e hist√≥rico (lags).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"üîÑ Agregando dados em s√©ries temporais semanais...\")\n",
    "\n",
    "df_indexed = df_limpo.set_index('data_inversa')\n",
    "\n",
    "weekly_df = df_indexed.groupby([pd.Grouper(freq='W'), 'uf']).agg(\n",
    "    total_acidentes=('severo', 'count'),\n",
    "    acidentes_severos=('severo', 'sum'),\n",
    "    pessoas_total=('pessoas', 'sum'),\n",
    "    veiculos_total=('veiculos', 'sum'),\n",
    "    pessoas_media=('pessoas', 'mean'),\n",
    "    veiculos_media=('veiculos', 'mean')\n",
    ").reset_index()\n",
    "\n",
    "weekly_df['prop_severos'] = np.where(\n",
    "    weekly_df['total_acidentes'] > 0,\n",
    "    weekly_df['acidentes_severos'] / weekly_df['total_acidentes'],\n",
    "    0\n",
    ")\n",
    "\n",
    "print(f\"‚úÖ Dados agregados: {len(weekly_df):,} semanas √ó estados\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üî¢ Passo 6: Cria√ß√£o de Sequ√™ncias Temporais\n",
    "\n",
    "Criamos janelas de 8 semanas para prever a semana seguinte. Normalizamos os dados com MinMaxScaler.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"üé® Criando features temporais e de hist√≥rico...\")\n",
    "\n",
    "# Temporais\n",
    "weekly_df['dia_semana'] = weekly_df['data_inversa'].dt.dayofweek\n",
    "weekly_df['mes'] = weekly_df['data_inversa'].dt.month\n",
    "weekly_df['fim_semana'] = weekly_df['dia_semana'].isin([5, 6]).astype(int)\n",
    "\n",
    "# Sazonalidade\n",
    "weekly_df['sazonalidade_sen'] = np.sin(2 * np.pi * weekly_df['data_inversa'].dt.dayofyear / 365)\n",
    "weekly_df['sazonalidade_cos'] = np.cos(2 * np.pi * weekly_df['data_inversa'].dt.dayofyear / 365)\n",
    "\n",
    "# Lags\n",
    "for lag in [1, 2, 3]:\n",
    "    weekly_df[f'prop_severos_lag{lag}'] = weekly_df.groupby('uf')['prop_severos'].shift(lag)\n",
    "\n",
    "# Estat√≠sticas\n",
    "weekly_df['prop_severos_ma3'] = weekly_df.groupby('uf')['prop_severos'].rolling(3).mean().reset_index(0, drop=True)\n",
    "weekly_df['prop_severos_tendencia'] = weekly_df.groupby('uf')['prop_severos'].diff()\n",
    "weekly_df['prop_severos_volatilidade'] = weekly_df.groupby('uf')['prop_severos'].rolling(3).std().reset_index(0, drop=True)\n",
    "\n",
    "print(\"‚úÖ Features criadas!\")\n",
    "print(f\"   Total: {len(weekly_df.columns)} colunas\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üéØ Passo 7: Transforma√ß√£o para Classifica√ß√£o\n",
    "\n",
    "Como regress√£o falhou (R¬≤ negativo), transformamos o problema em classifica√ß√£o de 4 n√≠veis de risco.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üèóÔ∏è Passo 8: Constru√ß√£o do Modelo LSTM (SIMPLIFICADO)\n",
    "\n",
    "**Arquitetura Simplificada** para evitar overfitting:\n",
    "- Uma camada LSTM (32 neur√¥nios)\n",
    "- Dropout aumentado (0.3)\n",
    "- Learning rate reduzido (0.0005)\n",
    "- Early stopping mais restritivo (patience=10)\n",
    "\n",
    "**Class Weights** para corrigir desbalanceamento entre classes.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "print(\"üî¢ Preparando sequ√™ncias para LSTM...\")\n",
    "\n",
    "features_colunas = [\n",
    "    'prop_severos', 'pessoas_media', 'veiculos_media', 'fim_semana',\n",
    "    'sazonalidade_sen', 'sazonalidade_cos',\n",
    "    'prop_severos_lag1', 'prop_severos_lag2', 'prop_severos_lag3',\n",
    "    'prop_severos_ma3', 'prop_severos_tendencia', 'prop_severos_volatilidade'\n",
    "]\n",
    "\n",
    "df_features = weekly_df.set_index('data_inversa').sort_index()\n",
    "df_features = df_features[features_colunas].copy()\n",
    "df_features = df_features.dropna()\n",
    "\n",
    "scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "dados_scaled = scaler.fit_transform(df_features.values)\n",
    "\n",
    "n_passos_para_tras = 8\n",
    "n_features = len(features_colunas)\n",
    "\n",
    "X, y = [], []\n",
    "for i in range(n_passos_para_tras, len(dados_scaled)):\n",
    "    X.append(dados_scaled[i-n_passos_para_tras:i, :])\n",
    "    y.append(dados_scaled[i, 0])\n",
    "\n",
    "X, y = np.array(X), np.array(y)\n",
    "\n",
    "print(f\"‚úÖ Sequ√™ncias criadas!\")\n",
    "print(f\"   Shape X: {X.shape}\")\n",
    "print(f\"   Shape y: {y.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "print(\"üéØ Transformando para CLASSIFICA√á√ÉO...\")\n",
    "\n",
    "def criar_classes_risco(y_data):\n",
    "    classes = np.zeros_like(y_data, dtype=int)\n",
    "    classes[y_data < 0.20] = 0  # BAIXO\n",
    "    classes[(y_data >= 0.20) & (y_data < 0.30)] = 1  # M√âDIO-BAIXO\n",
    "    classes[(y_data >= 0.30) & (y_data < 0.40)] = 2  # M√âDIO-ALTO\n",
    "    classes[y_data >= 0.40] = 3  # ALTO\n",
    "    return classes\n",
    "\n",
    "# Dividir dados\n",
    "split_index = int(len(X) * 0.85)\n",
    "X_train, X_val = X[:split_index], X[split_index:]\n",
    "y_train, y_val = y[:split_index], y[split_index:]\n",
    "\n",
    "# Criar classes\n",
    "y_train_classes = criar_classes_risco(y_train)\n",
    "y_val_classes = criar_classes_risco(y_val)\n",
    "\n",
    "# One-hot encoding\n",
    "y_train_categorical = to_categorical(y_train_classes, num_classes=4)\n",
    "y_val_categorical = to_categorical(y_val_classes, num_classes=4)\n",
    "\n",
    "nomes_classes = ['BAIXO (<0.20)', 'M√âDIO-BAIXO (0.20-0.30)', \n",
    "                 'M√âDIO-ALTO (0.30-0.40)', 'ALTO (‚â•0.40)']\n",
    "\n",
    "print(\"\\nüìä Distribui√ß√£o no TREINO:\")\n",
    "for i in range(4):\n",
    "    count = (y_train_classes == i).sum()\n",
    "    percent = count / len(y_train_classes) * 100\n",
    "    print(f\"   Classe {i}: {count:4d} ({percent:5.1f}%)\")\n",
    "\n",
    "print(\"\\nüìä Distribui√ß√£o na VALIDA√á√ÉO:\")\n",
    "for i in range(4):\n",
    "    count = (y_val_classes == i).sum()\n",
    "    percent = count / len(y_val_classes) * 100\n",
    "    print(f\"   Classe {i}: {count:4d} ({percent:5.1f}%)\")\n",
    "\n",
    "print(\"\\n‚úÖ Dados preparados!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Dropout\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "\n",
    "print(\"üèóÔ∏è  Construindo modelo SIMPLIFICADO (para evitar overfitting)...\")\n",
    "\n",
    "# MODELO SIMPLIFICADO: Apenas 1 LSTM\n",
    "model = Sequential([\n",
    "    LSTM(units=32, return_sequences=False, input_shape=(n_passos_para_tras, n_features)),\n",
    "    Dropout(0.3),  # AUMENTADO de 0.2 para 0.3\n",
    "    Dense(units=16, activation='relu'),\n",
    "    Dropout(0.3),  # AUMENTADO\n",
    "    Dense(units=4, activation='softmax')\n",
    "])\n",
    "\n",
    "# LEARNING RATE REDUZIDO\n",
    "optimizer = Adam(learning_rate=0.0005)  # ERA 0.001, AGORA 0.0005\n",
    "model.compile(\n",
    "    optimizer=optimizer,\n",
    "    loss='categorical_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "print(\"‚úÖ Modelo simplificado constru√≠do!\")\n",
    "print(\"   Arquitetura: LSTM 32 ‚Üí Dense 16 ‚Üí Softmax 4\")\n",
    "print(\"   Dropout: 0.3 (aumentado)\")\n",
    "print(\"   Learning rate: 0.0005 (reduzido)\")\n",
    "model.summary()\n",
    "\n",
    "# CLASS WEIGHTS para corrigir desbalanceamento\n",
    "class_weights = compute_class_weight('balanced', classes=np.unique(y_train_classes), y=y_train_classes)\n",
    "class_weight_dict = {i: class_weights[i] for i in range(4)}\n",
    "\n",
    "print(\"\\n‚öñÔ∏è  Class Weights (para corrigir desbalanceamento):\")\n",
    "for i in range(4):\n",
    "    print(f\"   Classe {i} ({nomes_classes[i]}): {class_weight_dict[i]:.3f}\")\n",
    "\n",
    "# EARLY STOPPING MAIS RESTRITIVO\n",
    "early_stop = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True, verbose=1)  # ERA 20\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=5, min_lr=0.00001, verbose=1)  # ERA 10\n",
    "\n",
    "print(\"\\n‚úÖ Callbacks configurados!\")\n",
    "print(\"   Early Stopping: patience=10 (mais restritivo)\")\n",
    "print(\"   Reduce LR: patience=5 (mais restritivo)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üöÄ Passo 9: Treinamento do Modelo\n",
    "\n",
    "Treinamos o modelo com class weights para corrigir desbalanceamento. Monitoramos val_loss para evitar overfitting.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"üöÄ TREINANDO MODELO SIMPLIFICADO\")\n",
    "print(\"=\"*70)\n",
    "print(\"\\n‚öñÔ∏è  Usando CLASS WEIGHTS para corrigir desbalanceamento!\")\n",
    "print(\"‚è±Ô∏è  Aguarde 10-20 minutos...\\n\")\n",
    "\n",
    "history = model.fit(\n",
    "    X_train, y_train_categorical,\n",
    "    epochs=50,  # REDUZIDO de 100 para 50\n",
    "    batch_size=16,\n",
    "    validation_data=(X_val, y_val_categorical),\n",
    "    class_weight=class_weight_dict,\n",
    "    callbacks=[early_stop, reduce_lr],\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"‚úÖ TREINAMENTO CONCLU√çDO!\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üìä Passo 10: Avalia√ß√£o e M√©tricas\n",
    "\n",
    "Avaliamos o modelo com accuracy, precision, recall e F1-score. Tamb√©m geramos matriz de confus√£o.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üìà Passo 11: Visualiza√ß√µes\n",
    "\n",
    "Geramos 6 gr√°ficos: curvas de aprendizagem, matriz de confus√£o, compara√ß√£o temporal, distribui√ß√£o de probabilidades e acur√°cia por classe.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, classification_report, accuracy_score\n",
    "\n",
    "print(\"üìä Avaliando modelo...\")\n",
    "\n",
    "y_pred_proba = model.predict(X_val, verbose=0)\n",
    "y_pred_classes = np.argmax(y_pred_proba, axis=1)\n",
    "\n",
    "accuracy = accuracy_score(y_val_classes, y_pred_classes)\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"üéØ RESULTADOS FINAIS\")\n",
    "print(\"=\"*70)\n",
    "print(f\"\\nüèÜ ACUR√ÅCIA: {accuracy:.2%}\\n\")\n",
    "\n",
    "baseline_random = 0.25\n",
    "baseline_majority = np.bincount(y_val_classes).max() / len(y_val_classes)\n",
    "\n",
    "print(\"üìä Compara√ß√£o com Baselines:\")\n",
    "print(f\"   Random Guess: {baseline_random:.1%}\")\n",
    "print(f\"   Classe mais comum: {baseline_majority:.1%}\")\n",
    "print(f\"   Nosso modelo: {accuracy:.1%} ‚úÖ\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"üìä RELAT√ìRIO POR CLASSE\")\n",
    "print(\"=\"*70 + \"\\n\")\n",
    "print(classification_report(y_val_classes, y_pred_classes, target_names=nomes_classes, digits=3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üíæ Passo 12: Salvamento do Modelo\n",
    "\n",
    "Salvamos o modelo treinado no formato `.keras` para uso futuro.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(16, 12))\n",
    "\n",
    "# 1. Loss\n",
    "plt.subplot(3, 2, 1)\n",
    "plt.plot(history.history['loss'], label='Treino', color='blue', linewidth=2)\n",
    "plt.plot(history.history['val_loss'], label='Valida√ß√£o', color='red', linewidth=2)\n",
    "plt.title('Curvas de Aprendizagem - Loss', fontsize=14, fontweight='bold')\n",
    "plt.xlabel('√âpocas')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# 2. Accuracy\n",
    "plt.subplot(3, 2, 2)\n",
    "plt.plot(history.history['accuracy'], label='Treino', color='blue', linewidth=2)\n",
    "plt.plot(history.history['val_accuracy'], label='Valida√ß√£o', color='red', linewidth=2)\n",
    "plt.title('Curvas de Aprendizagem - Acur√°cia', fontsize=14, fontweight='bold')\n",
    "plt.xlabel('√âpocas')\n",
    "plt.ylabel('Acur√°cia')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# 3. Matriz de Confus√£o\n",
    "plt.subplot(3, 2, 3)\n",
    "cm = confusion_matrix(y_val_classes, y_pred_classes)\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n",
    "plt.title('Matriz de Confus√£o', fontsize=14, fontweight='bold')\n",
    "plt.ylabel('Real')\n",
    "plt.xlabel('Previsto')\n",
    "plt.xticks([0.5, 1.5, 2.5, 3.5], ['Baixo', 'M√©dio-Baixo', 'M√©dio-Alto', 'Alto'], rotation=0)\n",
    "plt.yticks([0.5, 1.5, 2.5, 3.5], ['Baixo', 'M√©dio-Baixo', 'M√©dio-Alto', 'Alto'], rotation=0)\n",
    "\n",
    "# 4. Compara√ß√£o Temporal\n",
    "plt.subplot(3, 2, 4)\n",
    "plt.plot(y_val_classes, label='Real', marker='o', linewidth=2, markersize=5, alpha=0.7)\n",
    "plt.plot(y_pred_classes, label='Previsto', marker='x', linestyle='--', linewidth=2, markersize=5, alpha=0.7)\n",
    "plt.title('Compara√ß√£o Temporal', fontsize=14, fontweight='bold')\n",
    "plt.xlabel('Amostras')\n",
    "plt.ylabel('Classe')\n",
    "plt.yticks([0, 1, 2, 3], ['Baixo', 'M√©dio-Baixo', 'M√©dio-Alto', 'Alto'])\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# 5. Distribui√ß√£o de Probabilidades\n",
    "plt.subplot(3, 2, 5)\n",
    "for i in range(4):\n",
    "    plt.hist(y_pred_proba[:, i], bins=20, alpha=0.6, label=f'Classe {i}', edgecolor='black')\n",
    "plt.title('Distribui√ß√£o de Probabilidades', fontsize=14, fontweight='bold')\n",
    "plt.xlabel('Probabilidade')\n",
    "plt.ylabel('Frequ√™ncia')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# 6. Acur√°cia por Classe\n",
    "plt.subplot(3, 2, 6)\n",
    "acertos_por_classe = []\n",
    "for i in range(4):\n",
    "    mask = (y_val_classes == i)\n",
    "    if mask.sum() > 0:\n",
    "        acertos = (y_pred_classes[mask] == i).sum() / mask.sum() * 100\n",
    "        acertos_por_classe.append(acertos)\n",
    "    else:\n",
    "        acertos_por_classe.append(0)\n",
    "\n",
    "colors = ['green' if acc > 50 else 'orange' if acc > 30 else 'red' for acc in acertos_por_classe]\n",
    "bars = plt.bar(['Baixo', 'M√©dio-Baixo', 'M√©dio-Alto', 'Alto'], acertos_por_classe, color=colors, edgecolor='black')\n",
    "plt.title('Acur√°cia por Classe', fontsize=14, fontweight='bold')\n",
    "plt.ylabel('Acur√°cia (%)\")\n",
    "plt.ylim(0, 100)\n",
    "plt.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "for bar, acc in zip(bars, acertos_por_classe):\n",
    "    height = bar.get_height()\n",
    "    plt.text(bar.get_x() + bar.get_width()/2., height, f'{acc:.1f}%', ha='center', va='bottom', fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n‚úÖ Visualiza√ß√µes geradas!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_filename = 'modelo_lstm_classificacao_risco.keras'\n",
    "model.save(model_filename)\n",
    "print(f\"üíæ Modelo salvo: '{model_filename}'\")\n",
    "print(\"\\n‚úÖ Projeto conclu√≠do!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
