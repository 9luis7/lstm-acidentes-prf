{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ðŸš— Sprint Challenge 4 â€“ PrevisÃ£o de Acidentes com LSTMs\n",
    "## Case Sompo: Antecipando PadrÃµes de Risco em Rodovias Brasileiras\n",
    "\n",
    "---\n",
    "\n",
    "**Equipe Big 5**\n",
    "- Lucca Phelipe Masini - RM 564121\n",
    "- Luiz Henrique Poss - RM 562177\n",
    "- Luis Fernando de Oliveira Salgado - RM 561401\n",
    "- Igor PaixÃ£o Sarak - RM 563726\n",
    "- Bernardo Braga Perobeli - RM 562468\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸŽ¯ Target Escolhido: ClassificaÃ§Ã£o de 4 NÃ­veis de Risco\n",
    "\n",
    "- **Classe 0 - BAIXO**: < 20% de acidentes severos\n",
    "- **Classe 1 - MÃ‰DIO-BAIXO**: 20-30% de acidentes severos\n",
    "- **Classe 2 - MÃ‰DIO-ALTO**: 30-40% de acidentes severos\n",
    "- **Classe 3 - ALTO**: â‰¥ 40% de acidentes severos\n",
    "\n",
    "### Justificativa\n",
    "\n",
    "Inicialmente tentamos **regressÃ£o** (prever proporÃ§Ã£o exata), mas obtivemos RÂ² negativo - as features disponÃ­veis nÃ£o capturam fatores crÃ­ticos como clima e eventos. Reformulamos como **classificaÃ§Ã£o** - mais robusta e Ãºtil na prÃ¡tica."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install openpyxl --quiet\n",
    "print(\"âœ… Bibliotecas instaladas!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import urllib.request\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "print(\"ðŸ“š Bibliotecas importadas!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"ðŸ“¥ Baixando dataset da PRF do GitHub...\")\n",
    "\n",
    "github_raw_url = 'https://raw.githubusercontent.com/9luis7/lstm-acidentes-prf/main/dados/datatran2025.xlsx'\n",
    "output_filename = 'dados_acidentes.xlsx'\n",
    "\n",
    "try:\n",
    "    urllib.request.urlretrieve(github_raw_url, output_filename)\n",
    "    df = pd.read_excel(output_filename)\n",
    "    print(f\"âœ… Dataset carregado: {len(df):,} acidentes\")\n",
    "    print(\"\\nðŸ“Š PerÃ­odo:\", df['data_inversa'].min(), \"atÃ©\", df['data_inversa'].max())\n",
    "    print(\"ðŸ“Š Estados:\", df['uf'].nunique(), \"UFs\")\n",
    "except Exception as e:\n",
    "    print(f\"âŒ Erro: {e}\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"ðŸŽ¯ Criando variÃ¡vel target 'severo'...\")\n",
    "\n",
    "df['horario'] = pd.to_datetime(df['horario'], format='%H:%M:%S', errors='coerce').dt.time\n",
    "df['severo'] = ((df['mortos'] > 0) | (df['feridos_graves'] > 0)).astype(int)\n",
    "\n",
    "colunas_relevantes = ['data_inversa', 'horario', 'uf', 'br', 'km', 'pessoas', 'veiculos', 'severo']\n",
    "df_limpo = df[colunas_relevantes].copy()\n",
    "df_limpo['horario'].fillna(pd.to_datetime('12:00:00').time(), inplace=True)\n",
    "\n",
    "print(\"âœ… VariÃ¡vel 'severo' criada!\")\n",
    "print(\"\\nðŸ“Š DistribuiÃ§Ã£o:\")\n",
    "print(df_limpo['severo'].value_counts(normalize=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"ðŸ”„ Agregando dados em sÃ©ries temporais semanais...\")\n",
    "\n",
    "df_indexed = df_limpo.set_index('data_inversa')\n",
    "\n",
    "weekly_df = df_indexed.groupby([pd.Grouper(freq='W'), 'uf']).agg(\n",
    "    total_acidentes=('severo', 'count'),\n",
    "    acidentes_severos=('severo', 'sum'),\n",
    "    pessoas_total=('pessoas', 'sum'),\n",
    "    veiculos_total=('veiculos', 'sum'),\n",
    "    pessoas_media=('pessoas', 'mean'),\n",
    "    veiculos_media=('veiculos', 'mean')\n",
    ").reset_index()\n",
    "\n",
    "weekly_df['prop_severos'] = np.where(\n",
    "    weekly_df['total_acidentes'] > 0,\n",
    "    weekly_df['acidentes_severos'] / weekly_df['total_acidentes'],\n",
    "    0\n",
    ")\n",
    "\n",
    "print(f\"âœ… Dados agregados: {len(weekly_df):,} semanas Ã— estados\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"ðŸŽ¨ Criando features temporais e de histÃ³rico...\")\n",
    "\n",
    "# Temporais\n",
    "weekly_df['dia_semana'] = weekly_df['data_inversa'].dt.dayofweek\n",
    "weekly_df['mes'] = weekly_df['data_inversa'].dt.month\n",
    "weekly_df['fim_semana'] = weekly_df['dia_semana'].isin([5, 6]).astype(int)\n",
    "\n",
    "# Sazonalidade\n",
    "weekly_df['sazonalidade_sen'] = np.sin(2 * np.pi * weekly_df['data_inversa'].dt.dayofyear / 365)\n",
    "weekly_df['sazonalidade_cos'] = np.cos(2 * np.pi * weekly_df['data_inversa'].dt.dayofyear / 365)\n",
    "\n",
    "# Lags\n",
    "for lag in [1, 2, 3]:\n",
    "    weekly_df[f'prop_severos_lag{lag}'] = weekly_df.groupby('uf')['prop_severos'].shift(lag)\n",
    "\n",
    "# EstatÃ­sticas\n",
    "weekly_df['prop_severos_ma3'] = weekly_df.groupby('uf')['prop_severos'].rolling(3).mean().reset_index(0, drop=True)\n",
    "weekly_df['prop_severos_tendencia'] = weekly_df.groupby('uf')['prop_severos'].diff()\n",
    "weekly_df['prop_severos_volatilidade'] = weekly_df.groupby('uf')['prop_severos'].rolling(3).std().reset_index(0, drop=True)\n",
    "\n",
    "print(\"âœ… Features criadas!\")\n",
    "print(f\"   Total: {len(weekly_df.columns)} colunas\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "print(\"ðŸ”¢ Preparando sequÃªncias para LSTM...\")\n",
    "\n",
    "features_colunas = [\n",
    "    'prop_severos', 'pessoas_media', 'veiculos_media', 'fim_semana',\n",
    "    'sazonalidade_sen', 'sazonalidade_cos',\n",
    "    'prop_severos_lag1', 'prop_severos_lag2', 'prop_severos_lag3',\n",
    "    'prop_severos_ma3', 'prop_severos_tendencia', 'prop_severos_volatilidade'\n",
    "]\n",
    "\n",
    "df_features = weekly_df.set_index('data_inversa').sort_index()\n",
    "df_features = df_features[features_colunas].copy()\n",
    "df_features = df_features.dropna()\n",
    "\n",
    "scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "dados_scaled = scaler.fit_transform(df_features.values)\n",
    "\n",
    "n_passos_para_tras = 8\n",
    "n_features = len(features_colunas)\n",
    "\n",
    "X, y = [], []\n",
    "for i in range(n_passos_para_tras, len(dados_scaled)):\n",
    "    X.append(dados_scaled[i-n_passos_para_tras:i, :])\n",
    "    y.append(dados_scaled[i, 0])\n",
    "\n",
    "X, y = np.array(X), np.array(y)\n",
    "\n",
    "print(f\"âœ… SequÃªncias criadas!\")\n",
    "print(f\"   Shape X: {X.shape}\")\n",
    "print(f\"   Shape y: {y.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "print(\"ðŸŽ¯ Transformando para CLASSIFICAÃ‡ÃƒO...\")\n",
    "\n",
    "def criar_classes_risco(y_data):\n",
    "    classes = np.zeros_like(y_data, dtype=int)\n",
    "    classes[y_data < 0.20] = 0  # BAIXO\n",
    "    classes[(y_data >= 0.20) & (y_data < 0.30)] = 1  # MÃ‰DIO-BAIXO\n",
    "    classes[(y_data >= 0.30) & (y_data < 0.40)] = 2  # MÃ‰DIO-ALTO\n",
    "    classes[y_data >= 0.40] = 3  # ALTO\n",
    "    return classes\n",
    "\n",
    "# Dividir dados\n",
    "split_index = int(len(X) * 0.85)\n",
    "X_train, X_val = X[:split_index], X[split_index:]\n",
    "y_train, y_val = y[:split_index], y[split_index:]\n",
    "\n",
    "# Criar classes\n",
    "y_train_classes = criar_classes_risco(y_train)\n",
    "y_val_classes = criar_classes_risco(y_val)\n",
    "\n",
    "# One-hot encoding\n",
    "y_train_categorical = to_categorical(y_train_classes, num_classes=4)\n",
    "y_val_categorical = to_categorical(y_val_classes, num_classes=4)\n",
    "\n",
    "nomes_classes = ['BAIXO (<0.20)', 'MÃ‰DIO-BAIXO (0.20-0.30)', \n",
    "                 'MÃ‰DIO-ALTO (0.30-0.40)', 'ALTO (â‰¥0.40)']\n",
    "\n",
    "print(\"\\nðŸ“Š DistribuiÃ§Ã£o no TREINO:\")\n",
    "for i in range(4):\n",
    "    count = (y_train_classes == i).sum()\n",
    "    percent = count / len(y_train_classes) * 100\n",
    "    print(f\"   Classe {i}: {count:4d} ({percent:5.1f}%)\")\n",
    "\n",
    "print(\"\\nðŸ“Š DistribuiÃ§Ã£o na VALIDAÃ‡ÃƒO:\")\n",
    "for i in range(4):\n",
    "    count = (y_val_classes == i).sum()\n",
    "    percent = count / len(y_val_classes) * 100\n",
    "    print(f\"   Classe {i}: {count:4d} ({percent:5.1f}%)\")\n",
    "\n",
    "print(\"\\nâœ… Dados preparados!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Dropout\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "\n",
    "print(\"ðŸ—ï¸  Construindo modelo de CLASSIFICAÃ‡ÃƒO...\")\n",
    "\n",
    "model = Sequential([\n",
    "    LSTM(units=64, return_sequences=True, input_shape=(n_passos_para_tras, n_features)),\n",
    "    Dropout(0.2),\n",
    "    LSTM(units=32, return_sequences=False),\n",
    "    Dropout(0.2),\n",
    "    Dense(units=32, activation='relu'),\n",
    "    Dropout(0.2),\n",
    "    Dense(units=4, activation='softmax')  # 4 CLASSES!\n",
    "])\n",
    "\n",
    "optimizer = Adam(learning_rate=0.001)\n",
    "model.compile(\n",
    "    optimizer=optimizer,\n",
    "    loss='categorical_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "print(\"âœ… Modelo construÃ­do!\")\n",
    "model.summary()\n",
    "\n",
    "# CLASS WEIGHTS para corrigir desbalanceamento\n",
    "class_weights = compute_class_weight('balanced', classes=np.unique(y_train_classes), y=y_train_classes)\n",
    "class_weight_dict = {i: class_weights[i] for i in range(4)}\n",
    "\n",
    "print(\"\\nâš–ï¸  Class Weights (para corrigir desbalanceamento):\")\n",
    "for i in range(4):\n",
    "    print(f\"   Classe {i} ({nomes_classes[i]}): {class_weight_dict[i]:.3f}\")\n",
    "\n",
    "early_stop = EarlyStopping(monitor='val_loss', patience=20, restore_best_weights=True, verbose=1)\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=10, min_lr=0.00001, verbose=1)\n",
    "\n",
    "print(\"\\nâœ… Callbacks configurados!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"ðŸš€ TREINANDO MODELO DE CLASSIFICAÃ‡ÃƒO\")\n",
    "print(\"=\"*70)\n",
    "print(\"\\nâš–ï¸  Usando CLASS WEIGHTS para corrigir desbalanceamento!\")\n",
    "print(\"â±ï¸  Aguarde 15-30 minutos...\\n\")\n",
    "\n",
    "history = model.fit(\n",
    "    X_train, y_train_categorical,\n",
    "    epochs=100,\n",
    "    batch_size=16,\n",
    "    validation_data=(X_val, y_val_categorical),\n",
    "    class_weight=class_weight_dict,  # â† CORRIGE DESBALANCEAMENTO!\n",
    "    callbacks=[early_stop, reduce_lr],\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"âœ… TREINAMENTO CONCLUÃDO!\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, classification_report, accuracy_score\n",
    "\n",
    "print(\"ðŸ“Š Avaliando modelo...\")\n",
    "\n",
    "y_pred_proba = model.predict(X_val, verbose=0)\n",
    "y_pred_classes = np.argmax(y_pred_proba, axis=1)\n",
    "\n",
    "accuracy = accuracy_score(y_val_classes, y_pred_classes)\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"ðŸŽ¯ RESULTADOS FINAIS\")\n",
    "print(\"=\"*70)\n",
    "print(f\"\\nðŸ† ACURÃCIA: {accuracy:.2%}\\n\")\n",
    "\n",
    "baseline_random = 0.25\n",
    "baseline_majority = np.bincount(y_val_classes).max() / len(y_val_classes)\n",
    "\n",
    "print(\"ðŸ“Š ComparaÃ§Ã£o com Baselines:\")\n",
    "print(f\"   Random Guess: {baseline_random:.1%}\")\n",
    "print(f\"   Classe mais comum: {baseline_majority:.1%}\")\n",
    "print(f\"   Nosso modelo: {accuracy:.1%} âœ…\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"ðŸ“Š RELATÃ“RIO POR CLASSE\")\n",
    "print(\"=\"*70 + \"\\n\")\n",
    "print(classification_report(y_val_classes, y_pred_classes, target_names=nomes_classes, digits=3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(16, 12))\n",
    "\n",
    "# 1. Loss\n",
    "plt.subplot(3, 2, 1)\n",
    "plt.plot(history.history['loss'], label='Treino', color='blue', linewidth=2)\n",
    "plt.plot(history.history['val_loss'], label='ValidaÃ§Ã£o', color='red', linewidth=2)\n",
    "plt.title('Curvas de Aprendizagem - Loss', fontsize=14, fontweight='bold')\n",
    "plt.xlabel('Ã‰pocas')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# 2. Accuracy\n",
    "plt.subplot(3, 2, 2)\n",
    "plt.plot(history.history['accuracy'], label='Treino', color='blue', linewidth=2)\n",
    "plt.plot(history.history['val_accuracy'], label='ValidaÃ§Ã£o', color='red', linewidth=2)\n",
    "plt.title('Curvas de Aprendizagem - AcurÃ¡cia', fontsize=14, fontweight='bold')\n",
    "plt.xlabel('Ã‰pocas')\n",
    "plt.ylabel('AcurÃ¡cia')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# 3. Matriz de ConfusÃ£o\n",
    "plt.subplot(3, 2, 3)\n",
    "cm = confusion_matrix(y_val_classes, y_pred_classes)\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n",
    "plt.title('Matriz de ConfusÃ£o', fontsize=14, fontweight='bold')\n",
    "plt.ylabel('Real')\n",
    "plt.xlabel('Previsto')\n",
    "plt.xticks([0.5, 1.5, 2.5, 3.5], ['Baixo', 'MÃ©dio-Baixo', 'MÃ©dio-Alto', 'Alto'], rotation=0)\n",
    "plt.yticks([0.5, 1.5, 2.5, 3.5], ['Baixo', 'MÃ©dio-Baixo', 'MÃ©dio-Alto', 'Alto'], rotation=0)\n",
    "\n",
    "# 4. ComparaÃ§Ã£o Temporal\n",
    "plt.subplot(3, 2, 4)\n",
    "plt.plot(y_val_classes, label='Real', marker='o', linewidth=2, markersize=5, alpha=0.7)\n",
    "plt.plot(y_pred_classes, label='Previsto', marker='x', linestyle='--', linewidth=2, markersize=5, alpha=0.7)\n",
    "plt.title('ComparaÃ§Ã£o Temporal', fontsize=14, fontweight='bold')\n",
    "plt.xlabel('Amostras')\n",
    "plt.ylabel('Classe')\n",
    "plt.yticks([0, 1, 2, 3], ['Baixo', 'MÃ©dio-Baixo', 'MÃ©dio-Alto', 'Alto'])\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# 5. DistribuiÃ§Ã£o de Probabilidades\n",
    "plt.subplot(3, 2, 5)\n",
    "for i in range(4):\n",
    "    plt.hist(y_pred_proba[:, i], bins=20, alpha=0.6, label=f'Classe {i}', edgecolor='black')\n",
    "plt.title('DistribuiÃ§Ã£o de Probabilidades', fontsize=14, fontweight='bold')\n",
    "plt.xlabel('Probabilidade')\n",
    "plt.ylabel('FrequÃªncia')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# 6. AcurÃ¡cia por Classe\n",
    "plt.subplot(3, 2, 6)\n",
    "acertos_por_classe = []\n",
    "for i in range(4):\n",
    "    mask = (y_val_classes == i)\n",
    "    if mask.sum() > 0:\n",
    "        acertos = (y_pred_classes[mask] == i).sum() / mask.sum() * 100\n",
    "        acertos_por_classe.append(acertos)\n",
    "    else:\n",
    "        acertos_por_classe.append(0)\n",
    "\n",
    "colors = ['green' if acc > 50 else 'orange' if acc > 30 else 'red' for acc in acertos_por_classe]\n",
    "bars = plt.bar(['Baixo', 'MÃ©dio-Baixo', 'MÃ©dio-Alto', 'Alto'], acertos_por_classe, color=colors, edgecolor='black')\n",
    "plt.title('AcurÃ¡cia por Classe', fontsize=14, fontweight='bold')\n",
    "plt.ylabel('AcurÃ¡cia (%)\")\n",
    "plt.ylim(0, 100)\n",
    "plt.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "for bar, acc in zip(bars, acertos_por_classe):\n",
    "    height = bar.get_height()\n",
    "    plt.text(bar.get_x() + bar.get_width()/2., height, f'{acc:.1f}%', ha='center', va='bottom', fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nâœ… VisualizaÃ§Ãµes geradas!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_filename = 'modelo_lstm_classificacao_risco.keras'\n",
    "model.save(model_filename)\n",
    "print(f\"ðŸ’¾ Modelo salvo: '{model_filename}'\")\n",
    "print(\"\\nâœ… Projeto concluÃ­do!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
