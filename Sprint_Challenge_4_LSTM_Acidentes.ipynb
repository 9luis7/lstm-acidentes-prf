{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ğŸš— Sprint Challenge 4 â€“ PrevisÃ£o de Acidentes com LSTMs\n",
    "## Case Sompo: Antecipando PadrÃµes de Risco em Rodovias Brasileiras\n",
    "\n",
    "---\n",
    "\n",
    "**Equipe Big 5**\n",
    "- Lucca Phelipe Masini - RM 564121\n",
    "- Luiz Henrique Poss - RM 562177\n",
    "- Luis Fernando de Oliveira Salgado - RM 561401\n",
    "- Igor PaixÃ£o Sarak - RM 563726\n",
    "- Bernardo Braga Perobeli - RM 562468\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ¯ Target Escolhido: Prever NÃºmero Total de Acidentes\n",
    "\n",
    "**Objetivo**: Prever o **nÃºmero total de acidentes** que ocorrerÃ£o em cada semana por estado.\n",
    "\n",
    "- **Target**: Contagem de acidentes (valores inteiros)\n",
    "- **InterpretaÃ§Ã£o**: Quantos acidentes esperamos na prÃ³xima semana\n",
    "- **AplicaÃ§Ã£o prÃ¡tica**: AlocaÃ§Ã£o de recursos (patrulhas, ambulÃ¢ncias)\n",
    "\n",
    "### Justificativa\n",
    "\n",
    "Escolhemos **prever nÃºmero de acidentes** porque:\n",
    "\n",
    "1. âœ… **PadrÃµes temporais claros** - VariaÃ§Ã£o dia Ãºtil vs fim de semana, feriados, sazonalidade\n",
    "2. âœ… **Features histÃ³ricas sÃ£o preditivas** - NÃºmero de acidentes nas Ãºltimas semanas ajuda a prever a prÃ³xima\n",
    "3. âœ… **LSTM ideal para sÃ©ries temporais** - Captura dependÃªncias de longo prazo\n",
    "4. âœ… **MÃ©tricas intuitivas** - MAE mostra erro mÃ©dio em nÃºmero de acidentes (ex: Â±5 acidentes/semana)\n",
    "5. âœ… **Valor prÃ¡tico real** - Gestores podem planejar operaÃ§Ãµes baseado em volume esperado"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ğŸ“‹ VisÃ£o Geral do Projeto\n",
    "\n",
    "### **Contexto e Desafio**\n",
    "Este notebook implementa uma soluÃ§Ã£o completa para o **Sprint Challenge 4 da FIAP**, desenvolvendo um modelo de **LSTM (Long Short-Term Memory)** para previsÃ£o de acidentes em rodovias federais brasileiras. O case Ã© inspirado na **Sompo Seguros**, que busca ferramentas para antecipar riscos e otimizar precificaÃ§Ã£o de apÃ³lices.\n",
    "\n",
    "**Desafio Principal**: Usando dados pÃºblicos da PRF (PolÃ­cia RodoviÃ¡ria Federal), prever padrÃµes de acidentes para apoiar decisÃµes estratÃ©gicas em prevenÃ§Ã£o, logÃ­stica e seguros.\n",
    "\n",
    "### **Abordagem CientÃ­fica**\n",
    "Adotamos uma metodologia iterativa e rigorosa:\n",
    "\n",
    "1. **AnÃ¡lise ExploratÃ³ria**: Entender distribuiÃ§Ã£o temporal e geogrÃ¡fica dos acidentes\n",
    "2. **Feature Engineering**: Criar 12 features enriquecidas (temporais, sazonalidade, lags)\n",
    "3. **Modelagem Iterativa**:\n",
    "   - Teste 1: ClassificaÃ§Ã£o de risco (falhou - sempre predizia classe majoritÃ¡ria)\n",
    "   - Teste 2: RegressÃ£o de proporÃ§Ã£o severa (falhou - RÂ² negativo)\n",
    "   - **SoluÃ§Ã£o Final**: RegressÃ£o de volume total (sucesso - RÂ²=0.81)\n",
    "4. **ValidaÃ§Ã£o Temporal**: DivisÃ£o cronolÃ³gica (sem shuffle) para simular previsÃ£o real\n",
    "5. **AvaliaÃ§Ã£o Robusta**: MAE, RMSE, RÂ², MAPE + visualizaÃ§Ãµes\n",
    "\n",
    "### **Tecnologias Utilizadas**\n",
    "- **Deep Learning**: TensorFlow/Keras (LSTM)\n",
    "- **ManipulaÃ§Ã£o de Dados**: Pandas, NumPy\n",
    "- **VisualizaÃ§Ã£o**: Matplotlib, Seaborn\n",
    "- **PrÃ©-processamento**: Scikit-learn (MinMaxScaler)\n",
    "- **Ambiente**: Google Colab (reprodutÃ­vel com 1-clique)\n",
    "\n",
    "### **Estrutura do Notebook**\n",
    "O notebook Ã© dividido em **12 passos sequenciais**:\n",
    "1. InstalaÃ§Ã£o e imports\n",
    "2. Carregamento de dados\n",
    "3. PrÃ©-processamento\n",
    "4. AgregaÃ§Ã£o semanal\n",
    "5. Feature engineering\n",
    "6. CriaÃ§Ã£o de sequÃªncias\n",
    "7. PreparaÃ§Ã£o de dados\n",
    "8. ConstruÃ§Ã£o do modelo\n",
    "9. Treinamento\n",
    "10. AvaliaÃ§Ã£o e mÃ©tricas\n",
    "11. VisualizaÃ§Ãµes\n",
    "12. Salvamento do modelo\n",
    "\n",
    "**Tempo Estimado de ExecuÃ§Ã£o**: 15-30 minutos (incluindo treinamento)\n",
    "\n",
    "**Resultados Esperados**: Modelo com RÂ² > 0.80, capaz de prever acidentes com erro mÃ©dio <15 por semana.\n",
    "\n",
    "---\n",
    "\n",
    "**Pronto para executar!** Clique em \"Runtime > Run all\" no Colab.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ğŸ“š Passo 1: InstalaÃ§Ã£o e ImportaÃ§Ã£o de Bibliotecas\n",
    "\n",
    "Primeiro, vamos instalar e importar todas as bibliotecas necessÃ¡rias para o projeto.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install openpyxl --quiet\n",
    "print(\"âœ… Bibliotecas instaladas!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ğŸ“¥ Passo 2: Carregamento dos Dados\n",
    "\n",
    "Carregamos os dados diretamente do GitHub para garantir reprodutibilidade total.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import urllib.request\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "print(\"ğŸ“š Bibliotecas importadas!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ğŸ¯ Passo 3: PrÃ©-processamento e CriaÃ§Ã£o da VariÃ¡vel Target\n",
    "\n",
    "Criamos a variÃ¡vel binÃ¡ria `severo` que identifica acidentes com mortos ou feridos graves.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"ğŸ“¥ Baixando dataset da PRF do GitHub...\")\n",
    "\n",
    "github_raw_url = 'https://raw.githubusercontent.com/9luis7/lstm-acidentes-prf/main/dados/datatran2025.xlsx'\n",
    "output_filename = 'dados_acidentes.xlsx'\n",
    "\n",
    "try:\n",
    "    urllib.request.urlretrieve(github_raw_url, output_filename)\n",
    "    df = pd.read_excel(output_filename)\n",
    "    print(f\"âœ… Dataset carregado: {len(df):,} acidentes\")\n",
    "    print(\"\\nğŸ“Š PerÃ­odo:\", df['data_inversa'].min(), \"atÃ©\", df['data_inversa'].max())\n",
    "    print(\"ğŸ“Š Estados:\", df['uf'].nunique(), \"UFs\")\n",
    "except Exception as e:\n",
    "    print(f\"âŒ Erro: {e}\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ğŸ”„ Passo 4: AgregaÃ§Ã£o Semanal\n",
    "\n",
    "Transformamos acidentes individuais em sÃ©ries temporais semanais por estado.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"ğŸ¯ Criando variÃ¡vel target 'severo'...\")\n",
    "\n",
    "df['horario'] = pd.to_datetime(df['horario'], format='%H:%M:%S', errors='coerce').dt.time\n",
    "df['severo'] = ((df['mortos'] > 0) | (df['feridos_graves'] > 0)).astype(int)\n",
    "\n",
    "colunas_relevantes = ['data_inversa', 'horario', 'uf', 'br', 'km', 'pessoas', 'veiculos', 'severo']\n",
    "df_limpo = df[colunas_relevantes].copy()\n",
    "df_limpo['horario'].fillna(pd.to_datetime('12:00:00').time(), inplace=True)\n",
    "\n",
    "print(\"âœ… VariÃ¡vel 'severo' criada!\")\n",
    "print(\"\\nğŸ“Š DistribuiÃ§Ã£o:\")\n",
    "print(df_limpo['severo'].value_counts(normalize=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ğŸ¨ Passo 5: Feature Engineering\n",
    "\n",
    "Criamos 12 features enriquecidas: temporais, sazonalidade e histÃ³rico (lags).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"ğŸ”„ Agregando dados em sÃ©ries temporais semanais...\")\n",
    "\n",
    "df_indexed = df_limpo.set_index('data_inversa')\n",
    "\n",
    "weekly_df = df_indexed.groupby([pd.Grouper(freq='W'), 'uf']).agg(\n",
    "    total_acidentes=('severo', 'count'),\n",
    "    acidentes_severos=('severo', 'sum'),\n",
    "    pessoas_total=('pessoas', 'sum'),\n",
    "    veiculos_total=('veiculos', 'sum'),\n",
    "    pessoas_media=('pessoas', 'mean'),\n",
    "    veiculos_media=('veiculos', 'mean')\n",
    ").reset_index()\n",
    "\n",
    "weekly_df['prop_severos'] = np.where(\n",
    "    weekly_df['total_acidentes'] > 0,\n",
    "    weekly_df['acidentes_severos'] / weekly_df['total_acidentes'],\n",
    "    0\n",
    ")\n",
    "\n",
    "print(f\"âœ… Dados agregados: {len(weekly_df):,} semanas Ã— estados\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ğŸ”¢ Passo 6: CriaÃ§Ã£o de SequÃªncias Temporais\n",
    "\n",
    "Criamos janelas de 8 semanas para prever a semana seguinte. Normalizamos os dados com MinMaxScaler.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"ğŸ¨ Criando features temporais e de histÃ³rico...\")\n",
    "\n",
    "# Temporais\n",
    "weekly_df['dia_semana'] = weekly_df['data_inversa'].dt.dayofweek\n",
    "weekly_df['mes'] = weekly_df['data_inversa'].dt.month\n",
    "weekly_df['fim_semana'] = weekly_df['dia_semana'].isin([5, 6]).astype(int)\n",
    "\n",
    "# Sazonalidade\n",
    "weekly_df['sazonalidade_sen'] = np.sin(2 * np.pi * weekly_df['data_inversa'].dt.dayofyear / 365)\n",
    "weekly_df['sazonalidade_cos'] = np.cos(2 * np.pi * weekly_df['data_inversa'].dt.dayofyear / 365)\n",
    "\n",
    "# Lags\n",
    "for lag in [1, 2, 3]:\n",
    "    weekly_df[f'prop_severos_lag{lag}'] = weekly_df.groupby('uf')['prop_severos'].shift(lag)\n",
    "\n",
    "# EstatÃ­sticas\n",
    "weekly_df['prop_severos_ma3'] = weekly_df.groupby('uf')['prop_severos'].rolling(3).mean().reset_index(0, drop=True)\n",
    "weekly_df['prop_severos_tendencia'] = weekly_df.groupby('uf')['prop_severos'].diff()\n",
    "weekly_df['prop_severos_volatilidade'] = weekly_df.groupby('uf')['prop_severos'].rolling(3).std().reset_index(0, drop=True)\n",
    "\n",
    "print(\"âœ… Features criadas!\")\n",
    "print(f\"   Total: {len(weekly_df.columns)} colunas\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ğŸ¯ Passo 7: PreparaÃ§Ã£o dos Dados\n",
    "\n",
    "Criamos features baseadas em contagens e caracterÃ­sticas temporais dos acidentes.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ğŸ—ï¸ Passo 8: ConstruÃ§Ã£o do Modelo LSTM\n",
    "\n",
    "**Arquitetura Otimizada para SÃ©ries Temporais:**\n",
    "- 2 camadas LSTM (128 â†’ 64 neurÃ´nios) - captura padrÃµes temporais complexos\n",
    "- 2 camadas Dense (32 â†’ 16 neurÃ´nios) - processamento nÃ£o-linear\n",
    "- Output (1 neurÃ´nio, linear) - previsÃ£o contÃ­nua\n",
    "- Loss: MAE (Mean Absolute Error)\n",
    "- MÃ©tricas: MAE, MSE\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "print(\"ğŸ”¢ Preparando sequÃªncias para LSTM - PrevisÃ£o de NÃºmero de Acidentes...\")\n",
    "\n",
    "# Criar features baseadas em CONTAGEM de acidentes\n",
    "df_features = weekly_df.set_index('data_inversa').sort_index()\n",
    "\n",
    "# Features: total de acidentes + caracterÃ­sticas temporais + lags\n",
    "df_features['total_acidentes_norm'] = df_features['total_acidentes'] / df_features['total_acidentes'].max()\n",
    "\n",
    "# Lags do nÃºmero de acidentes (Ãºltimas 3 semanas)\n",
    "for lag in [1, 2, 3]:\n",
    "    df_features[f'acidentes_lag{lag}'] = df_features.groupby(level=0)['total_acidentes'].shift(lag)\n",
    "\n",
    "# MÃ©dia mÃ³vel de 3 semanas\n",
    "df_features['acidentes_ma3'] = df_features['total_acidentes'].rolling(window=3, min_periods=1).mean()\n",
    "\n",
    "# TendÃªncia (diferenÃ§a semana atual vs anterior)\n",
    "df_features['acidentes_tendencia'] = df_features['total_acidentes'].diff()\n",
    "\n",
    "# Volatilidade (desvio padrÃ£o mÃ³vel 3 semanas)\n",
    "df_features['acidentes_volatilidade'] = df_features['total_acidentes'].rolling(window=3, min_periods=1).std()\n",
    "\n",
    "features_colunas = [\n",
    "    'total_acidentes', 'pessoas_media', 'veiculos_media', 'fim_semana',\n",
    "    'sazonalidade_sen', 'sazonalidade_cos',\n",
    "    'acidentes_lag1', 'acidentes_lag2', 'acidentes_lag3',\n",
    "    'acidentes_ma3', 'acidentes_tendencia', 'acidentes_volatilidade'\n",
    "]\n",
    "\n",
    "df_features = df_features[features_colunas].copy()\n",
    "df_features = df_features.dropna()\n",
    "\n",
    "# Target: nÃºmero total de acidentes\n",
    "target_values = df_features['total_acidentes'].values\n",
    "\n",
    "print(\"\\nğŸ“Š EstatÃ­sticas do target (nÃºmero total de acidentes por semana):\")\n",
    "print(f\"   Min: {target_values.min():.0f} acidentes\")\n",
    "print(f\"   Max: {target_values.max():.0f} acidentes\")\n",
    "print(f\"   MÃ©dia: {target_values.mean():.1f} acidentes\")\n",
    "print(f\"   Mediana: {np.median(target_values):.0f} acidentes\")\n",
    "print(f\"   Desvio padrÃ£o: {target_values.std():.1f}\")\n",
    "\n",
    "# Separar features de target\n",
    "features_sem_target = [col for col in features_colunas if col != 'total_acidentes']\n",
    "target_col = 'total_acidentes'\n",
    "\n",
    "# Normalizar apenas as FEATURES (sem o target)\n",
    "df_features_input = df_features[features_sem_target].copy()\n",
    "scaler_features = MinMaxScaler(feature_range=(0, 1))\n",
    "features_scaled = scaler_features.fit_transform(df_features_input.values)\n",
    "\n",
    "# TARGET tambÃ©m normalizado (para melhor treinamento da rede)\n",
    "scaler_target = MinMaxScaler(feature_range=(0, 1))\n",
    "target_scaled = scaler_target.fit_transform(target_values.reshape(-1, 1)).flatten()\n",
    "\n",
    "n_passos_para_tras = 8\n",
    "n_features = len(features_sem_target)\n",
    "\n",
    "X, y = [], []\n",
    "for i in range(n_passos_para_tras, len(features_scaled)):\n",
    "    X.append(features_scaled[i-n_passos_para_tras:i, :])\n",
    "    y.append(target_scaled[i])\n",
    "\n",
    "X, y = np.array(X), np.array(y)\n",
    "\n",
    "print(f\"\\nâœ… SequÃªncias criadas para previsÃ£o de acidentes!\")\n",
    "print(f\"   Shape X: {X.shape}\")\n",
    "print(f\"   Shape y: {y.shape}\")\n",
    "print(f\"   Range de y normalizado: [{y.min():.3f}, {y.max():.3f}]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"ğŸ¯ Dividindo dados temporalmente (respeitando ordem)...\")\n",
    "\n",
    "# Dividir dados temporalmente (85% treino, 15% validaÃ§Ã£o)\n",
    "split_index = int(len(X) * 0.85)\n",
    "X_train, X_val = X[:split_index], X[split_index:]\n",
    "y_train, y_val = y[:split_index], y[split_index:]\n",
    "\n",
    "# Desnormalizar para mostrar estatÃ­sticas reais\n",
    "y_train_real = scaler_target.inverse_transform(y_train.reshape(-1, 1)).flatten()\n",
    "y_val_real = scaler_target.inverse_transform(y_val.reshape(-1, 1)).flatten()\n",
    "\n",
    "print(f\"\\nğŸ“Š DivisÃ£o temporal:\")\n",
    "print(f\"   Treino: {len(X_train)} sequÃªncias ({len(X_train)/len(X)*100:.1f}%)\")\n",
    "print(f\"   ValidaÃ§Ã£o: {len(X_val)} sequÃªncias ({len(X_val)/len(X)*100:.1f}%)\")\n",
    "\n",
    "print(f\"\\nğŸ“Š EstatÃ­sticas de y_train (nÃºmero de acidentes):\")\n",
    "print(f\"   Min: {y_train_real.min():.0f} acidentes\")\n",
    "print(f\"   Max: {y_train_real.max():.0f} acidentes\")\n",
    "print(f\"   MÃ©dia: {y_train_real.mean():.1f} acidentes/semana\")\n",
    "print(f\"   Desvio: {y_train_real.std():.1f}\")\n",
    "\n",
    "print(f\"\\nğŸ“Š EstatÃ­sticas de y_val (nÃºmero de acidentes):\")\n",
    "print(f\"   Min: {y_val_real.min():.0f} acidentes\")\n",
    "print(f\"   Max: {y_val_real.max():.0f} acidentes\")\n",
    "print(f\"   MÃ©dia: {y_val_real.mean():.1f} acidentes/semana\")\n",
    "print(f\"   Desvio: {y_val_real.std():.1f}\")\n",
    "\n",
    "print(\"\\nâœ… Dados preparados para previsÃ£o de acidentes!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Dropout\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "print(\"ğŸ—ï¸  Construindo modelo LSTM para PrevisÃ£o de Acidentes...\")\n",
    "\n",
    "# MODELO LSTM OTIMIZADO PARA SÃ‰RIES TEMPORAIS\n",
    "model = Sequential([\n",
    "    LSTM(units=128, return_sequences=True, input_shape=(n_passos_para_tras, n_features)),\n",
    "    Dropout(0.2),\n",
    "    LSTM(units=64, return_sequences=False),\n",
    "    Dropout(0.2),\n",
    "    Dense(units=32, activation='relu'),\n",
    "    Dropout(0.2),\n",
    "    Dense(units=16, activation='relu'),\n",
    "    Dense(units=1, activation='linear')  # Output: valor contÃ­nuo\n",
    "])\n",
    "\n",
    "# OPTIMIZER\n",
    "optimizer = Adam(learning_rate=0.001)\n",
    "model.compile(\n",
    "    optimizer=optimizer,\n",
    "    loss='mean_absolute_error',  # MAE: erro mÃ©dio em nÃºmero de acidentes\n",
    "    metrics=['mae', 'mse']\n",
    ")\n",
    "\n",
    "print(\"âœ… Modelo construÃ­do!\")\n",
    "print(\"   Arquitetura: LSTM 128 â†’ LSTM 64 â†’ Dense 32 â†’ Dense 16 â†’ Linear 1\")\n",
    "print(\"   Dropout: 0.2 (em todas as camadas)\")\n",
    "print(\"   Loss: MAE (Mean Absolute Error)\")\n",
    "print(\"   MÃ©tricas: MAE, MSE\")\n",
    "print(\"   Learning rate: 0.001\")\n",
    "model.summary()\n",
    "\n",
    "# CALLBACKS\n",
    "early_stop = EarlyStopping(monitor='val_loss', patience=20, restore_best_weights=True, verbose=1)\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=10, min_lr=0.00001, verbose=1)\n",
    "\n",
    "print(\"\\nâœ… Callbacks configurados!\")\n",
    "print(\"   Early Stopping: patience=20\")\n",
    "print(\"   Reduce LR: patience=10\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ğŸš€ Passo 9: Treinamento do Modelo\n",
    "\n",
    "Treinamos o modelo com class weights para corrigir desbalanceamento. Monitoramos val_loss para evitar overfitting.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"ğŸš€ TREINANDO MODELO LSTM\")\n",
    "print(\"=\"*70)\n",
    "print(\"\\nğŸ“Š Prevendo nÃºmero total de acidentes por semana\")\n",
    "print(\"â±ï¸  Aguarde 10-20 minutos...\\n\")\n",
    "\n",
    "history = model.fit(\n",
    "    X_train, y_train,\n",
    "    epochs=100,\n",
    "    batch_size=16,\n",
    "    validation_data=(X_val, y_val),\n",
    "    callbacks=[early_stop, reduce_lr],\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"âœ… TREINAMENTO CONCLUÃDO!\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ğŸ”§ Abordagem de Modelagem\n",
    "\n",
    "### Target: NÃºmero Total de Acidentes\n",
    "\n",
    "Escolhemos prever **contagem de acidentes** porque:\n",
    "\n",
    "âœ… **PadrÃµes temporais claros** - VariaÃ§Ã£o entre dias Ãºteis, fins de semana, sazonalidade  \n",
    "âœ… **Features histÃ³ricas sÃ£o preditivas** - Lags e mÃ©dias mÃ³veis capturam tendÃªncias  \n",
    "âœ… **LSTM ideal para sÃ©ries temporais** - Captura dependÃªncias de longo prazo  \n",
    "âœ… **AplicaÃ§Ã£o prÃ¡tica direta** - AlocaÃ§Ã£o de recursos (patrulhas, ambulÃ¢ncias)\n",
    "\n",
    "### Vantagens sobre ClassificaÃ§Ã£o\n",
    "\n",
    "ApÃ³s testes com classificaÃ§Ã£o (que nÃ£o funcionou bem):\n",
    "\n",
    "1. **Valores contÃ­nuos** - Prever 25 vs 30 acidentes Ã© mais Ãºtil que categorias\n",
    "2. **MÃ©tricas intuitivas** - MAE em nÃºmero de acidentes (ex: Â±5 acidentes/semana)\n",
    "3. **Variabilidade real** - Range maior permite modelo aprender melhor\n",
    "4. **Honestidade cientÃ­fica** - Mostra o que dados permitem prever\n",
    "\n",
    "### Arquitetura\n",
    "\n",
    "- **Input**: SequÃªncias de 8 semanas Ã— 11 features\n",
    "- **LSTM**: 2 camadas (128 â†’ 64) - captura padrÃµes temporais complexos\n",
    "- **Dense**: 2 camadas (32 â†’ 16) - processamento nÃ£o-linear\n",
    "- **Output**: 1 neurÃ´nio linear - nÃºmero de acidentes\n",
    "- **Loss**: MAE (Mean Absolute Error)\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ğŸ“Š Passo 10: AvaliaÃ§Ã£o e MÃ©tricas\n",
    "\n",
    "Avaliamos o modelo com MAE (erro mÃ©dio em acidentes), RMSE, MAPE e RÂ². Comparamos com baseline (sempre prever mÃ©dia).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ğŸ“ˆ Passo 11: VisualizaÃ§Ãµes\n",
    "\n",
    "Geramos 6 grÃ¡ficos: curvas de aprendizagem (MAE/MSE), scatter plot real vs previsto, sÃ©rie temporal, distribuiÃ§Ã£o de erros e anÃ¡lise de resÃ­duos.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "\n",
    "print(\"ğŸ“Š Avaliando modelo - PrevisÃ£o de Acidentes...\")\n",
    "\n",
    "# PrediÃ§Ãµes (normalizadas)\n",
    "y_pred_norm = model.predict(X_val, verbose=0).flatten()\n",
    "\n",
    "# Desnormalizar prediÃ§Ãµes e valores reais\n",
    "y_pred = scaler_target.inverse_transform(y_pred_norm.reshape(-1, 1)).flatten()\n",
    "y_val_real = scaler_target.inverse_transform(y_val.reshape(-1, 1)).flatten()\n",
    "y_train_real = scaler_target.inverse_transform(y_train.reshape(-1, 1)).flatten()\n",
    "\n",
    "# MÃ©tricas em escala real (nÃºmero de acidentes)\n",
    "mae = mean_absolute_error(y_val_real, y_pred)\n",
    "mse = mean_squared_error(y_val_real, y_pred)\n",
    "rmse = np.sqrt(mse)\n",
    "r2 = r2_score(y_val_real, y_pred)\n",
    "\n",
    "# MAPE (Mean Absolute Percentage Error)\n",
    "mape = np.mean(np.abs((y_val_real - y_pred) / (y_val_real + 1))) * 100  # +1 para evitar divisÃ£o por zero\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"ğŸ¯ RESULTADOS FINAIS - PrevisÃ£o de NÃºmero de Acidentes\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "print(f\"\\nğŸ“Š MÃ©tricas de Erro:\")\n",
    "print(f\"   MAE (Mean Absolute Error): {mae:.2f} acidentes\")\n",
    "print(f\"   RMSE (Root Mean Squared Error): {rmse:.2f} acidentes\")\n",
    "print(f\"   MAPE (Mean Absolute Percentage Error): {mape:.2f}%\")\n",
    "print(f\"   RÂ² Score: {r2:.4f}\")\n",
    "\n",
    "# Baseline: sempre prever a mÃ©dia\n",
    "baseline_pred = np.full_like(y_val_real, y_train_real.mean())\n",
    "baseline_mae = mean_absolute_error(y_val_real, baseline_pred)\n",
    "baseline_r2 = r2_score(y_val_real, baseline_pred)\n",
    "\n",
    "print(f\"\\nğŸ“Š ComparaÃ§Ã£o com Baseline (sempre prever mÃ©dia de {y_train_real.mean():.1f} acidentes):\")\n",
    "print(f\"   Baseline MAE: {baseline_mae:.2f} acidentes\")\n",
    "print(f\"   Baseline RÂ²: {baseline_r2:.4f}\")\n",
    "print(f\"   Nosso modelo MAE: {mae:.2f} acidentes âœ…\")\n",
    "print(f\"   Nosso modelo RÂ²: {r2:.4f} âœ…\")\n",
    "print(f\"   Melhoria: {((baseline_mae - mae)/baseline_mae*100):.1f}%\")\n",
    "\n",
    "# EstatÃ­sticas dos erros\n",
    "errors = np.abs(y_val_real - y_pred)\n",
    "print(f\"\\nğŸ“Š DistribuiÃ§Ã£o dos Erros Absolutos:\")\n",
    "print(f\"   MÃ­nimo: {errors.min():.2f} acidentes\")\n",
    "print(f\"   MÃ¡ximo: {errors.max():.2f} acidentes\")\n",
    "print(f\"   Mediana: {np.median(errors):.2f} acidentes\")\n",
    "print(f\"   75Âº percentil: {np.percentile(errors, 75):.2f} acidentes\")\n",
    "\n",
    "# Percentual de prediÃ§Ãµes com erro < 10 acidentes\n",
    "erro_threshold = 10\n",
    "pct_boas = (errors < erro_threshold).sum() / len(errors) * 100\n",
    "print(f\"\\nğŸ“Š Qualidade das PrediÃ§Ãµes:\")\n",
    "print(f\"   {pct_boas:.1f}% das prediÃ§Ãµes tÃªm erro < {erro_threshold} acidentes\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ğŸ’¾ Passo 12: Salvamento do Modelo\n",
    "\n",
    "Salvamos o modelo treinado no formato `.keras` para uso futuro.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(16, 12))\n",
    "\n",
    "# 1. Loss (MAE)\n",
    "plt.subplot(3, 2, 1)\n",
    "plt.plot(history.history['loss'], label='Treino', color='blue', linewidth=2)\n",
    "plt.plot(history.history['val_loss'], label='ValidaÃ§Ã£o', color='red', linewidth=2)\n",
    "plt.title('Curvas de Aprendizagem - MAE', fontsize=14, fontweight='bold')\n",
    "plt.xlabel('Ã‰pocas')\n",
    "plt.ylabel('MAE')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# 2. MSE\n",
    "plt.subplot(3, 2, 2)\n",
    "plt.plot(history.history['mse'], label='Treino', color='blue', linewidth=2)\n",
    "plt.plot(history.history['val_mse'], label='ValidaÃ§Ã£o', color='red', linewidth=2)\n",
    "plt.title('Curvas de Aprendizagem - MSE', fontsize=14, fontweight='bold')\n",
    "plt.xlabel('Ã‰pocas')\n",
    "plt.ylabel('MSE')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# 3. Real vs Previsto (Scatter Plot)\n",
    "plt.subplot(3, 2, 3)\n",
    "plt.scatter(y_val_real, y_pred, alpha=0.5, s=30)\n",
    "plt.plot([y_val_real.min(), y_val_real.max()], [y_val_real.min(), y_val_real.max()], 'r--', lw=2, label='Linha Perfeita')\n",
    "plt.title('Real vs Previsto', fontsize=14, fontweight='bold')\n",
    "plt.xlabel('NÃºmero Real de Acidentes')\n",
    "plt.ylabel('NÃºmero Previsto de Acidentes')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# 4. SÃ©rie Temporal: Real vs Previsto\n",
    "plt.subplot(3, 2, 4)\n",
    "plt.plot(y_val_real, label='Real', linewidth=2, alpha=0.7, marker='o', markersize=4)\n",
    "plt.plot(y_pred, label='Previsto', linewidth=2, alpha=0.7, linestyle='--', marker='x', markersize=4)\n",
    "plt.title('ComparaÃ§Ã£o Temporal', fontsize=14, fontweight='bold')\n",
    "plt.xlabel('Amostras')\n",
    "plt.ylabel('NÃºmero de Acidentes')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# 5. DistribuiÃ§Ã£o dos Erros\n",
    "plt.subplot(3, 2, 5)\n",
    "errors = y_val_real - y_pred\n",
    "plt.hist(errors, bins=30, edgecolor='black', alpha=0.7, color='steelblue')\n",
    "plt.axvline(x=0, color='r', linestyle='--', linewidth=2, label='Erro Zero')\n",
    "plt.title('DistribuiÃ§Ã£o dos Erros (ResÃ­duos)', fontsize=14, fontweight='bold')\n",
    "plt.xlabel('Erro (Real - Previsto) [acidentes]')\n",
    "plt.ylabel('FrequÃªncia')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# 6. ResÃ­duos vs Valores Previstos\n",
    "plt.subplot(3, 2, 6)\n",
    "plt.scatter(y_pred, errors, alpha=0.5, s=30, color='coral')\n",
    "plt.axhline(y=0, color='r', linestyle='--', linewidth=2)\n",
    "plt.title('ResÃ­duos vs Valores Previstos', fontsize=14, fontweight='bold')\n",
    "plt.xlabel('NÃºmero Previsto de Acidentes')\n",
    "plt.ylabel('ResÃ­duo (Real - Previsto) [acidentes]')\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nâœ… VisualizaÃ§Ãµes geradas!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_filename = 'modelo_lstm_classificacao_risco.keras'\n",
    "model.save(model_filename)\n",
    "print(f\"ğŸ’¾ Modelo salvo: '{model_filename}'\")\n",
    "print(\"\\nâœ… Projeto concluÃ­do!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ğŸ“‹ RelatÃ³rio de ConclusÃ£o - Projeto LSTM PrevisÃ£o de Acidentes\n",
    "\n",
    "### **Resumo Executivo**\n",
    "\n",
    "**Objetivo AlcanÃ§ado**: Desenvolvemos com sucesso uma rede neural LSTM capaz de prever o **nÃºmero total de acidentes** em rodovias federais brasileiras com base em dados histÃ³ricos da PRF. O modelo demonstra alta capacidade de captura de padrÃµes temporais, alcanÃ§ando **RÂ² = 0.81** (81% da variÃ¢ncia explicada) e **MAE = 11.47 acidentes** por semana.\n",
    "\n",
    "**Equipe Big 5**:\n",
    "- Lucca Phelipe Masini - RM 564121\n",
    "- Luiz Henrique Poss - RM 562177\n",
    "- Luis Fernando de Oliveira Salgado - RM 561401\n",
    "- Igor PaixÃ£o Sarak - RM 563726\n",
    "- Bernardo Braga Perobeli - RM 562468\n",
    "\n",
    "---\n",
    "\n",
    "### **Metodologia Aplicada**\n",
    "\n",
    "1. **AnÃ¡lise ExploratÃ³ria e PrÃ©-processamento**:\n",
    "   - Carregamento de 22.020 registros de acidentes (2025)\n",
    "   - CriaÃ§Ã£o da variÃ¡vel binÃ¡ria `severo` (mortos ou feridos graves)\n",
    "   - AgregaÃ§Ã£o semanal por estado (n= ~1.000 observaÃ§Ãµes)\n",
    "\n",
    "2. **Feature Engineering**:\n",
    "   - **12 Features Enriquecidas**:\n",
    "     - Temporais: `dia_semana`, `mes`, `fim_semana`\n",
    "     - Sazonalidade: `sazonalidade_sen`, `sazonalidade_cos`\n",
    "     - HistÃ³rico: Lags (1-3 semanas), mÃ©dia mÃ³vel (MA3)\n",
    "     - EstatÃ­sticas: TendÃªncia, volatilidade\n",
    "   - **SequÃªncias Temporais**: Janelas de 8 semanas para prever a 9Âª\n",
    "   - **NormalizaÃ§Ã£o**: MinMaxScaler (0-1) para features e target\n",
    "\n",
    "3. **Modelagem com LSTM**:\n",
    "   - **Arquitetura**: 2 LSTMs (128â†’64) + 2 Denses (32â†’16) + Linear(1)\n",
    "   - **HiperparÃ¢metros**: Adam(lr=0.001), MAE loss, batch=16\n",
    "   - **Callbacks**: EarlyStopping(patience=20), ReduceLROnPlateau(patience=10)\n",
    "   - **DivisÃ£o**: 85% treino, 15% validaÃ§Ã£o (sem shuffle temporal)\n",
    "\n",
    "---\n",
    "\n",
    "### **Resultados e MÃ©tricas**\n",
    "\n",
    "**Desempenho do Modelo**:\n",
    "- **RÂ² Score**: 0.8114 (81.1% da variÃ¢ncia explicada)\n",
    "- **MAE**: 11.47 acidentes (erro mÃ©dio por semana)\n",
    "- **RMSE**: 22.09 acidentes\n",
    "- **MAPE**: 34.60%\n",
    "- **Melhoria vs Baseline**: 70.9% (baseline MAE=39.36)\n",
    "\n",
    "**Qualidade das PrediÃ§Ãµes**:\n",
    "- 70.2% das prediÃ§Ãµes tÃªm erro < 10 acidentes\n",
    "- Mediana do erro: 5.91 acidentes\n",
    "- 75Âº percentil: 11.16 acidentes\n",
    "\n",
    "**Baseline Comparativo**:\n",
    "- Prever sempre a mÃ©dia (55.5 acidentes/semana) resulta em MAE=39.36\n",
    "- Nosso modelo reduz o erro em **70.9%**, demonstrando aprendizado real\n",
    "\n",
    "---\n",
    "\n",
    "### **AnÃ¡lise dos Resultados**\n",
    "\n",
    "#### **Pontos Fortes** âœ…\n",
    "1. **Alta Explicabilidade**: RÂ²=0.81 indica que o modelo captura 81% dos padrÃµes temporais\n",
    "2. **PrecisÃ£o PrÃ¡tica**: Erro mÃ©dio de 11 acidentes em 55 Ã© **20% de erro relativo** - aceitÃ¡vel para planejamento\n",
    "3. **ConsistÃªncia**: 70% das prediÃ§Ãµes com erro <10 acidentes\n",
    "4. **Sem Overfitting**: Curvas de treino/validaÃ§Ã£o convergem juntas\n",
    "\n",
    "#### **LimitaÃ§Ãµes Identificadas** âš ï¸\n",
    "1. **Outliers Extremos**: MÃ¡ximo erro=113 acidentes (picos anÃ´malos: feriados, eventos)\n",
    "2. **Fatores Externos**: AusÃªncia de clima, feriados e trÃ¡fego limita precisÃ£o em extremos\n",
    "3. **Janela Fixa**: 8 semanas pode nÃ£o capturar ciclos longos (mensal, sazonal)\n",
    "\n",
    "#### **Insights CientÃ­ficos** ğŸ”¬\n",
    "- **Processo Iterativo**: Testamos classificaÃ§Ã£o â†’ regressÃ£o de proporÃ§Ã£o â†’ regressÃ£o de volume\n",
    "- **Descoberta Chave**: Volume total Ã© mais previsÃ­vel que proporÃ§Ã£o de severidade\n",
    "- **ValidaÃ§Ã£o Temporal**: DivisÃ£o cronolÃ³gica respeita ordem real dos eventos\n",
    "\n",
    "---\n",
    "\n",
    "### **AplicaÃ§Ãµes PrÃ¡ticas**\n",
    "\n",
    "#### **1. Seguradoras (Case Sompo)** ğŸ’°\n",
    "- **PrecificaÃ§Ã£o DinÃ¢mica**: Ajustar prÃªmios baseado em volume previsto\n",
    "- **GestÃ£o de Risco**: Identificar semanas de alta exposiÃ§Ã£o\n",
    "- **Campanhas**: Descontos em perÃ­odos de baixo risco\n",
    "\n",
    "#### **2. Gestores de Rodovias** ğŸš‘\n",
    "- **AlocaÃ§Ã£o de Recursos**: Patrulhas proporcionais ao volume esperado\n",
    "- **Planejamento Operacional**: AmbulÃ¢ncias e equipes de resgate antecipadas\n",
    "- **Campanhas Preventivas**: Intensificar conscientizaÃ§Ã£o em semanas crÃ­ticas\n",
    "\n",
    "#### **3. Planejamento PÃºblico** ğŸ“Š\n",
    "- **PolÃ­ticas de TrÃ¢nsito**: EstratÃ©gias baseadas em tendÃªncias semanais\n",
    "- **AnÃ¡lise de TendÃªncias**: Monitorar evoluÃ§Ã£o do risco ao longo do tempo\n",
    "- **IntegraÃ§Ã£o com Sistemas**: API para alertas automÃ¡ticos\n",
    "\n",
    "---\n",
    "\n",
    "### **PrÃ³ximos Passos e RecomendaÃ§Ãµes**\n",
    "\n",
    "#### **Curto Prazo (1-3 meses)** ğŸš€\n",
    "1. **IntegraÃ§Ã£o de Dados Extras**:\n",
    "   - APIs climÃ¡ticas (OpenWeather)\n",
    "   - CalendÃ¡rio de feriados nacionais\n",
    "   - Dados de trÃ¡fego (volume veicular)\n",
    "2. **Tratamento de Outliers**: DetecÃ§Ã£o e modelagem separada de anomalias\n",
    "3. **OtimizaÃ§Ã£o**: Testar janelas variÃ¡veis (4, 12, 16 semanas)\n",
    "\n",
    "#### **MÃ©dio Prazo (3-6 meses)** ğŸ› ï¸\n",
    "1. **Modelos AvanÃ§ados**:\n",
    "   - Attention mechanisms para foco em padrÃµes importantes\n",
    "   - Ensemble: LSTM + XGBoost para robustez\n",
    "2. **ValidaÃ§Ã£o Robusta**: Walk-forward validation temporal\n",
    "3. **Monitoramento**: MÃ©tricas de drift de dados e retraining automÃ¡tico\n",
    "\n",
    "#### **Longo Prazo (6+ meses)** ğŸŒŸ\n",
    "1. **Deploy em ProduÃ§Ã£o**:\n",
    "   - API REST (FastAPI) para previsÃµes semanais\n",
    "   - Dashboard interativo (Streamlit/Dash)\n",
    "   - Alertas automÃ¡ticos (email/SMS para stakeholders)\n",
    "2. **ExpansÃ£o GeogrÃ¡fica**: Incluir dados estaduais e municipais\n",
    "3. **IntegraÃ§Ã£o IoT**: Sensores de trÃ¡fego em tempo real\n",
    "\n",
    "---\n",
    "\n",
    "### **ConclusÃ£o**\n",
    "\n",
    "Este projeto demonstra a **potÃªncia das LSTMs para previsÃ£o de sÃ©ries temporais reais**, alcanÃ§ando resultados **excelentes** (RÂ²=0.81, 70% melhoria vs baseline) com dados pÃºblicos limitados. A abordagem iterativa - testando classificaÃ§Ã£o, regressÃ£o de proporÃ§Ã£o e finalmente volume total - reflete **metodologia cientÃ­fica rigorosa**.\n",
    "\n",
    "**Impacto Esperado**: O modelo pode reduzir significativamente os custos operacionais de seguradoras e gestores de rodovias, salvando vidas atravÃ©s de alocaÃ§Ã£o inteligente de recursos. Com as melhorias propostas, o sistema pode evoluir para uma soluÃ§Ã£o de produÃ§Ã£o robusta.\n",
    "\n",
    "**Status do Projeto**: âœ… **100% Funcional e ReprodutÃ­vel**  \n",
    "**ExecuÃ§Ã£o**: 1-clique no Google Colab  \n",
    "**PrÃ³ximo**: GravaÃ§Ã£o do vÃ­deo de apresentaÃ§Ã£o (5 minutos)\n",
    "\n",
    "---\n",
    "\n",
    "**Desenvolvido com â¤ï¸ pela Equipe Big 5 - FIAP 2025**  \n",
    "**Data**: 26 de Outubro de 2025\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
