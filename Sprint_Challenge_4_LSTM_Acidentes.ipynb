{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/9luis7/lstm-acidentes-prf/blob/main/Sprint_Challenge_4_LSTM_Acidentes.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R54lxAM9bUF-"
      },
      "source": [
        "# 🚗 Sprint Challenge 4 – Previsão de Acidentes com LSTMs\n",
        "## Case Sompo: Antecipando Padrões de Risco em Rodovias Brasileiras\n",
        "\n",
        "---\n",
        "\n",
        "**Equipe Big 5**\n",
        "- Lucca Phelipe Masini - RM 564121\n",
        "- Luiz Henrique Poss - RM 562177\n",
        "- Luis Fernando de Oliveira Salgado - RM 561401\n",
        "- Igor Paixão Sarak - RM 563726\n",
        "- Bernardo Braga Perobeli - RM 562468\n",
        "\n",
        "---\n",
        "\n",
        "## 🎯 Target Escolhido: Classificação de 4 Níveis de Risco\n",
        "\n",
        "- **Classe 0 - BAIXO**: < 20% de acidentes severos\n",
        "- **Classe 1 - MÉDIO-BAIXO**: 20-30% de acidentes severos\n",
        "- **Classe 2 - MÉDIO-ALTO**: 30-40% de acidentes severos\n",
        "- **Classe 3 - ALTO**: ≥ 40% de acidentes severos\n",
        "\n",
        "### Justificativa\n",
        "\n",
        "Inicialmente tentamos **regressão** (prever proporção exata), mas obtivemos R² negativo - as features disponíveis não capturam fatores críticos como clima e eventos. Reformulamos como **classificação** - mais robusta e útil na prática."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7T33PCeUbUF_"
      },
      "source": [
        "---\n",
        "\n",
        "## 📚 Passo 1: Instalação e Importação de Bibliotecas\n",
        "\n",
        "Primeiro, vamos instalar e importar todas as bibliotecas necessárias para o projeto.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 60,
      "metadata": {
        "id": "2dcei9qpbUGA",
        "outputId": "20daeccf-639e-40c6-e5ef-bc93637baf85",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Bibliotecas instaladas!\n"
          ]
        }
      ],
      "source": [
        "!pip install openpyxl --quiet\n",
        "print(\"✅ Bibliotecas instaladas!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UbjkSqMxbUGA"
      },
      "source": [
        "---\n",
        "\n",
        "## 📥 Passo 2: Carregamento dos Dados\n",
        "\n",
        "Carregamos os dados diretamente do GitHub para garantir reprodutibilidade total.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "metadata": {
        "id": "TKlNdRdxbUGA",
        "outputId": "0ca9459e-6048-4b9f-eb40-a8e3ae814e49",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "📚 Bibliotecas importadas!\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import urllib.request\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "plt.style.use('seaborn-v0_8-darkgrid')\n",
        "sns.set_palette(\"husl\")\n",
        "\n",
        "print(\"📚 Bibliotecas importadas!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WyYI-hYYbUGA"
      },
      "source": [
        "---\n",
        "\n",
        "## 🎯 Passo 3: Pré-processamento e Criação da Variável Target\n",
        "\n",
        "Criamos a variável binária `severo` que identifica acidentes com mortos ou feridos graves.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2KB2XKiXbUGB",
        "outputId": "7b41af23-6375-4569-b826-62317d3ef555",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "📥 Baixando dataset da PRF do GitHub...\n"
          ]
        }
      ],
      "source": [
        "print(\"📥 Baixando dataset da PRF do GitHub...\")\n",
        "\n",
        "github_raw_url = 'https://raw.githubusercontent.com/9luis7/lstm-acidentes-prf/main/dados/datatran2025.xlsx'\n",
        "output_filename = 'dados_acidentes.xlsx'\n",
        "\n",
        "try:\n",
        "    urllib.request.urlretrieve(github_raw_url, output_filename)\n",
        "    df = pd.read_excel(output_filename)\n",
        "    print(f\"✅ Dataset carregado: {len(df):,} acidentes\")\n",
        "    print(\"\\n📊 Período:\", df['data_inversa'].min(), \"até\", df['data_inversa'].max())\n",
        "    print(\"📊 Estados:\", df['uf'].nunique(), \"UFs\")\n",
        "except Exception as e:\n",
        "    print(f\"❌ Erro: {e}\")\n",
        "    raise"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oGPKGJtpbUGB"
      },
      "source": [
        "---\n",
        "\n",
        "## 🔄 Passo 4: Agregação Semanal\n",
        "\n",
        "Transformamos acidentes individuais em séries temporais semanais por estado.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OHDkXiORbUGB"
      },
      "outputs": [],
      "source": [
        "print(\"🎯 Criando variável target 'severo'...\")\n",
        "\n",
        "df['horario'] = pd.to_datetime(df['horario'], format='%H:%M:%S', errors='coerce').dt.time\n",
        "df['severo'] = ((df['mortos'] > 0) | (df['feridos_graves'] > 0)).astype(int)\n",
        "\n",
        "colunas_relevantes = ['data_inversa', 'horario', 'uf', 'br', 'km', 'pessoas', 'veiculos', 'severo']\n",
        "df_limpo = df[colunas_relevantes].copy()\n",
        "df_limpo['horario'].fillna(pd.to_datetime('12:00:00').time(), inplace=True)\n",
        "\n",
        "print(\"✅ Variável 'severo' criada!\")\n",
        "print(\"\\n📊 Distribuição:\")\n",
        "print(df_limpo['severo'].value_counts(normalize=True))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R9Si0wDNbUGB"
      },
      "source": [
        "---\n",
        "\n",
        "## 🎨 Passo 5: Feature Engineering\n",
        "\n",
        "Criamos 12 features enriquecidas: temporais, sazonalidade e histórico (lags).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WJgjqvlkbUGC"
      },
      "outputs": [],
      "source": [
        "print(\"🔄 Agregando dados em séries temporais semanais...\")\n",
        "\n",
        "df_indexed = df_limpo.set_index('data_inversa')\n",
        "\n",
        "weekly_df = df_indexed.groupby([pd.Grouper(freq='W'), 'uf']).agg(\n",
        "    total_acidentes=('severo', 'count'),\n",
        "    acidentes_severos=('severo', 'sum'),\n",
        "    pessoas_total=('pessoas', 'sum'),\n",
        "    veiculos_total=('veiculos', 'sum'),\n",
        "    pessoas_media=('pessoas', 'mean'),\n",
        "    veiculos_media=('veiculos', 'mean')\n",
        ").reset_index()\n",
        "\n",
        "weekly_df['prop_severos'] = np.where(\n",
        "    weekly_df['total_acidentes'] > 0,\n",
        "    weekly_df['acidentes_severos'] / weekly_df['total_acidentes'],\n",
        "    0\n",
        ")\n",
        "\n",
        "print(f\"✅ Dados agregados: {len(weekly_df):,} semanas × estados\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1G1tmEAebUGC"
      },
      "source": [
        "---\n",
        "\n",
        "## 🔢 Passo 6: Criação de Sequências Temporais\n",
        "\n",
        "Criamos janelas de 8 semanas para prever a semana seguinte. Normalizamos os dados com MinMaxScaler.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gGJVHIBcbUGC"
      },
      "outputs": [],
      "source": [
        "print(\"🎨 Criando features temporais e de histórico...\")\n",
        "\n",
        "# Temporais\n",
        "weekly_df['dia_semana'] = weekly_df['data_inversa'].dt.dayofweek\n",
        "weekly_df['mes'] = weekly_df['data_inversa'].dt.month\n",
        "weekly_df['fim_semana'] = weekly_df['dia_semana'].isin([5, 6]).astype(int)\n",
        "\n",
        "# Sazonalidade\n",
        "weekly_df['sazonalidade_sen'] = np.sin(2 * np.pi * weekly_df['data_inversa'].dt.dayofyear / 365)\n",
        "weekly_df['sazonalidade_cos'] = np.cos(2 * np.pi * weekly_df['data_inversa'].dt.dayofyear / 365)\n",
        "\n",
        "# Lags\n",
        "for lag in [1, 2, 3]:\n",
        "    weekly_df[f'prop_severos_lag{lag}'] = weekly_df.groupby('uf')['prop_severos'].shift(lag)\n",
        "\n",
        "# Estatísticas\n",
        "weekly_df['prop_severos_ma3'] = weekly_df.groupby('uf')['prop_severos'].rolling(3).mean().reset_index(0, drop=True)\n",
        "weekly_df['prop_severos_tendencia'] = weekly_df.groupby('uf')['prop_severos'].diff()\n",
        "weekly_df['prop_severos_volatilidade'] = weekly_df.groupby('uf')['prop_severos'].rolling(3).std().reset_index(0, drop=True)\n",
        "\n",
        "print(\"✅ Features criadas!\")\n",
        "print(f\"   Total: {len(weekly_df.columns)} colunas\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K42Ad3GEbUGC"
      },
      "source": [
        "---\n",
        "\n",
        "## 🎯 Passo 7: Transformação para Classificação\n",
        "\n",
        "Como regressão falhou (R² negativo), transformamos o problema em classificação de 4 níveis de risco.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dzgE7Y6ibUGC"
      },
      "source": [
        "---\n",
        "\n",
        "## 🏗️ Passo 8: Construção do Modelo LSTM (SIMPLIFICADO)\n",
        "\n",
        "**Arquitetura Simplificada** para evitar overfitting:\n",
        "- Uma camada LSTM (32 neurônios)\n",
        "- Dropout aumentado (0.3)\n",
        "- Learning rate reduzido (0.0005)\n",
        "- Early stopping mais restritivo (patience=10)\n",
        "\n",
        "**Class Weights** para corrigir desbalanceamento entre classes.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RssBxKKobUGC"
      },
      "outputs": [],
      "source": [
        "from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "print(\"🔢 Preparando sequências para LSTM...\")\n",
        "\n",
        "features_colunas = [\n",
        "    'prop_severos', 'pessoas_media', 'veiculos_media', 'fim_semana',\n",
        "    'sazonalidade_sen', 'sazonalidade_cos',\n",
        "    'prop_severos_lag1', 'prop_severos_lag2', 'prop_severos_lag3',\n",
        "    'prop_severos_ma3', 'prop_severos_tendencia', 'prop_severos_volatilidade'\n",
        "]\n",
        "\n",
        "df_features = weekly_df.set_index('data_inversa').sort_index()\n",
        "df_features = df_features[features_colunas].copy()\n",
        "df_features = df_features.dropna()\n",
        "\n",
        "# IMPORTANTE: Criar CLASSES ANTES da normalização\n",
        "print(\"\\n📊 Criando classes ANTES da normalização...\")\n",
        "target_values = df_features['prop_severos'].values\n",
        "\n",
        "def criar_classes_risco(y_data):\n",
        "    classes = np.zeros_like(y_data, dtype=int)\n",
        "    classes[y_data < 0.20] = 0  # BAIXO\n",
        "    classes[(y_data >= 0.20) & (y_data < 0.30)] = 1  # MÉDIO-BAIXO\n",
        "    classes[(y_data >= 0.30) & (y_data < 0.40)] = 2  # MÉDIO-ALTO\n",
        "    classes[y_data >= 0.40] = 3  # ALTO\n",
        "    return classes\n",
        "\n",
        "# Criar classes dos valores originais\n",
        "classes_originais = criar_classes_risco(target_values)\n",
        "\n",
        "print(\"\\n📊 Distribuição das classes (ANTES da normalização):\")\n",
        "unique, counts = np.unique(classes_originais, return_counts=True)\n",
        "nomes_classes = ['BAIXO (<0.20)', 'MEDIO-BAIXO (0.20-0.30)',\n",
        "                 'MEDIO-ALTO (0.30-0.40)', 'ALTO (>=0.40)']\n",
        "for u, c in zip(unique, counts):\n",
        "    print(f\"   Classe {u}: {c:4d} ({c/len(classes_originais)*100:5.1f}%)\")\n",
        "\n",
        "# Separar features de target\n",
        "features_sem_target = [col for col in features_colunas if col != 'prop_severos']\n",
        "target_col = 'prop_severos'\n",
        "\n",
        "# Normalizar apenas as FEATURES (sem o target)\n",
        "df_features_input = df_features[features_sem_target].copy()\n",
        "scaler_features = MinMaxScaler(feature_range=(0, 1))\n",
        "features_scaled = scaler_features.fit_transform(df_features_input.values)\n",
        "\n",
        "# Usar o TARGET sem normalização (mantém valores originais)\n",
        "target_values = df_features[target_col].values\n",
        "\n",
        "n_passos_para_tras = 8\n",
        "n_features = len(features_sem_target)\n",
        "\n",
        "X, y = [], []\n",
        "for i in range(n_passos_para_tras, len(features_scaled)):\n",
        "    X.append(features_scaled[i-n_passos_para_tras:i, :])\n",
        "    y.append(target_values[i])  # VALORES ORIGINAIS, não normalizados\n",
        "\n",
        "X, y = np.array(X), np.array(y)\n",
        "\n",
        "print(f\"\\n✅ Sequências criadas!\")\n",
        "print(f\"   Shape X: {X.shape}\")\n",
        "print(f\"   Shape y: {y.shape}\")\n",
        "print(f\"   Range de y: [{y.min():.3f}, {y.max():.3f}]\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gR8vOeyIbUGD"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.utils import to_categorical\n",
        "\n",
        "print(\"🎯 Transformando para CLASSIFICAÇÃO...\")\n",
        "\n",
        "# Usar a função já definida anteriormente (ou recriar se necessário)\n",
        "def criar_classes_risco(y_data):\n",
        "    classes = np.zeros_like(y_data, dtype=int)\n",
        "    classes[y_data < 0.20] = 0  # BAIXO\n",
        "    classes[(y_data >= 0.20) & (y_data < 0.30)] = 1  # MÉDIO-BAIXO\n",
        "    classes[(y_data >= 0.30) & (y_data < 0.40)] = 2  # MÉDIO-ALTO\n",
        "    classes[y_data >= 0.40] = 3  # ALTO\n",
        "    return classes\n",
        "\n",
        "# Dividir dados\n",
        "split_index = int(len(X) * 0.85)\n",
        "X_train, X_val = X[:split_index], X[split_index:]\n",
        "y_train, y_val = y[:split_index], y[split_index:]\n",
        "\n",
        "print(f\"\\n📊 Estatísticas de y_train:\")\n",
        "print(f\"   Min: {y_train.min():.3f}, Max: {y_train.max():.3f}\")\n",
        "print(f\"   Mean: {y_train.mean():.3f}, Std: {y_train.std():.3f}\")\n",
        "\n",
        "print(f\"\\n📊 Estatísticas de y_val:\")\n",
        "print(f\"   Min: {y_val.min():.3f}, Max: {y_val.max():.3f}\")\n",
        "print(f\"   Mean: {y_val.mean():.3f}, Std: {y_val.std():.3f}\")\n",
        "\n",
        "# Criar classes\n",
        "y_train_classes = criar_classes_risco(y_train)\n",
        "y_val_classes = criar_classes_risco(y_val)\n",
        "\n",
        "# One-hot encoding\n",
        "y_train_categorical = to_categorical(y_train_classes, num_classes=4)\n",
        "y_val_categorical = to_categorical(y_val_classes, num_classes=4)\n",
        "\n",
        "nomes_classes = ['BAIXO (<0.20)', 'MEDIO-BAIXO (0.20-0.30)',\n",
        "                 'MEDIO-ALTO (0.30-0.40)', 'ALTO (>=0.40)']\n",
        "\n",
        "print(\"\\n📊 Distribuição no TREINO:\")\n",
        "for i in range(4):\n",
        "    count = (y_train_classes == i).sum()\n",
        "    percent = count / len(y_train_classes) * 100\n",
        "    print(f\"   Classe {i} - {nomes_classes[i]}: {count:4d} ({percent:5.1f}%)\")\n",
        "\n",
        "print(\"\\n📊 Distribuição na VALIDAÇÃO:\")\n",
        "for i in range(4):\n",
        "    count = (y_val_classes == i).sum()\n",
        "    percent = count / len(y_val_classes) * 100\n",
        "    print(f\"   Classe {i} - {nomes_classes[i]}: {count:4d} ({percent:5.1f}%)\")\n",
        "\n",
        "print(\"\\n✅ Dados preparados!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zZKaK38JbUGD"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import LSTM, Dense, Dropout\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "\n",
        "print(\"🏗️  Construindo modelo LSTM...\")\n",
        "\n",
        "# MODELO COM MELHOR ARQUITETURA PARA CLASSIFICAÇÃO\n",
        "model = Sequential([\n",
        "    LSTM(units=64, return_sequences=True, input_shape=(n_passos_para_tras, n_features)),\n",
        "    Dropout(0.3),\n",
        "    LSTM(units=32, return_sequences=False),\n",
        "    Dropout(0.3),\n",
        "    Dense(units=32, activation='relu'),\n",
        "    Dropout(0.3),\n",
        "    Dense(units=4, activation='softmax')\n",
        "])\n",
        "\n",
        "# LEARNING RATE OTIMIZADO\n",
        "optimizer = Adam(learning_rate=0.001)\n",
        "model.compile(\n",
        "    optimizer=optimizer,\n",
        "    loss='categorical_crossentropy',\n",
        "    metrics=['accuracy']\n",
        ")\n",
        "\n",
        "print(\"✅ Modelo construído!\")\n",
        "print(\"   Arquitetura: LSTM 64 → LSTM 32 → Dense 32 → Softmax 4\")\n",
        "print(\"   Dropout: 0.3\")\n",
        "print(\"   Learning rate: 0.001\")\n",
        "model.summary()\n",
        "\n",
        "# CLASS WEIGHTS para corrigir desbalanceamento\n",
        "class_weights = compute_class_weight('balanced', classes=np.unique(y_train_classes), y=y_train_classes)\n",
        "class_weight_dict = {i: class_weights[i] for i in range(4)}\n",
        "\n",
        "print(\"\\n⚖️  Class Weights (para corrigir desbalanceamento):\")\n",
        "for i in range(4):\n",
        "    print(f\"   Classe {i} ({nomes_classes[i]}): {class_weight_dict[i]:.3f}\")\n",
        "\n",
        "# CALLBACKS\n",
        "early_stop = EarlyStopping(monitor='val_loss', patience=15, restore_best_weights=True, verbose=1)\n",
        "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=7, min_lr=0.00001, verbose=1)\n",
        "\n",
        "print(\"\\n✅ Callbacks configurados!\")\n",
        "print(\"   Early Stopping: patience=15\")\n",
        "print(\"   Reduce LR: patience=7\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J5zMKScIbUGD"
      },
      "source": [
        "---\n",
        "\n",
        "## 🚀 Passo 9: Treinamento do Modelo\n",
        "\n",
        "Treinamos o modelo com class weights para corrigir desbalanceamento. Monitoramos val_loss para evitar overfitting.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yr0Jn2iTbUGD"
      },
      "outputs": [],
      "source": [
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"🚀 TREINANDO MODELO LSTM\")\n",
        "print(\"=\"*70)\n",
        "print(\"\\n⚖️  Usando CLASS WEIGHTS para corrigir desbalanceamento!\")\n",
        "print(\"⏱️  Aguarde 10-20 minutos...\\n\")\n",
        "\n",
        "history = model.fit(\n",
        "    X_train, y_train_categorical,\n",
        "    epochs=100,\n",
        "    batch_size=16,\n",
        "    validation_data=(X_val, y_val_categorical),\n",
        "    class_weight=class_weight_dict,\n",
        "    callbacks=[early_stop, reduce_lr],\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"✅ TREINAMENTO CONCLUÍDO!\")\n",
        "print(\"=\"*70)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R8aN8vtCbUGD"
      },
      "source": [
        "---\n",
        "\n",
        "## ⚠️ Correções Implementadas\n",
        "\n",
        "### Problema Identificado\n",
        "O modelo estava sempre prevendo a classe \"Alto\" porque as classes eram criadas **DEPOIS** da normalização dos dados. Quando normalizamos os valores entre 0 e 1, os thresholds de classificação (0.20, 0.30, 0.40) não correspondiam mais aos valores originais.\n",
        "\n",
        "### Solução\n",
        "1. **Criar classes ANTES da normalização** - Usar os valores originais de `prop_severos`\n",
        "2. **Normalizar apenas as features** - Não normalizar o target\n",
        "3. **Melhorar arquitetura** - Adicionar camada LSTM adicional para melhor aprendizado\n",
        "4. **Ajustar hiperparâmetros** - Otimizar learning rate e patience dos callbacks\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BCds92-rbUGD"
      },
      "source": [
        "---\n",
        "\n",
        "## 📊 Passo 10: Avaliação e Métricas\n",
        "\n",
        "Avaliamos o modelo com accuracy, precision, recall e F1-score. Também geramos matriz de confusão.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fPm7-7CJbUGE"
      },
      "source": [
        "---\n",
        "\n",
        "## 📈 Passo 11: Visualizações\n",
        "\n",
        "Geramos 6 gráficos: curvas de aprendizagem, matriz de confusão, comparação temporal, distribuição de probabilidades e acurácia por classe.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oEqkCF49bUGE"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import confusion_matrix, classification_report, accuracy_score\n",
        "\n",
        "print(\"📊 Avaliando modelo...\")\n",
        "\n",
        "y_pred_proba = model.predict(X_val, verbose=0)\n",
        "y_pred_classes = np.argmax(y_pred_proba, axis=1)\n",
        "\n",
        "accuracy = accuracy_score(y_val_classes, y_pred_classes)\n",
        "\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"🎯 RESULTADOS FINAIS\")\n",
        "print(\"=\"*70)\n",
        "print(f\"\\n🏆 ACURÁCIA: {accuracy:.2%}\\n\")\n",
        "\n",
        "baseline_random = 0.25\n",
        "baseline_majority = np.bincount(y_val_classes).max() / len(y_val_classes)\n",
        "\n",
        "print(\"📊 Comparação com Baselines:\")\n",
        "print(f\"   Random Guess: {baseline_random:.1%}\")\n",
        "print(f\"   Classe mais comum: {baseline_majority:.1%}\")\n",
        "print(f\"   Nosso modelo: {accuracy:.1%} ✅\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"📊 RELATÓRIO POR CLASSE\")\n",
        "print(\"=\"*70 + \"\\n\")\n",
        "print(classification_report(y_val_classes, y_pred_classes, target_names=nomes_classes, digits=3))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DBkQT3v6bUGE"
      },
      "source": [
        "---\n",
        "\n",
        "## 💾 Passo 12: Salvamento do Modelo\n",
        "\n",
        "Salvamos o modelo treinado no formato `.keras` para uso futuro.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tRdHqs52bUGE"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(16, 12))\n",
        "\n",
        "# 1. Loss\n",
        "plt.subplot(3, 2, 1)\n",
        "plt.plot(history.history['loss'], label='Treino', color='blue', linewidth=2)\n",
        "plt.plot(history.history['val_loss'], label='Validação', color='red', linewidth=2)\n",
        "plt.title('Curvas de Aprendizagem - Loss', fontsize=14, fontweight='bold')\n",
        "plt.xlabel('Épocas')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "plt.grid(True, alpha=0.3)\n",
        "\n",
        "# 2. Accuracy\n",
        "plt.subplot(3, 2, 2)\n",
        "plt.plot(history.history['accuracy'], label='Treino', color='blue', linewidth=2)\n",
        "plt.plot(history.history['val_accuracy'], label='Validação', color='red', linewidth=2)\n",
        "plt.title('Curvas de Aprendizagem - Acurácia', fontsize=14, fontweight='bold')\n",
        "plt.xlabel('Épocas')\n",
        "plt.ylabel('Acurácia')\n",
        "plt.legend()\n",
        "plt.grid(True, alpha=0.3)\n",
        "\n",
        "# 3. Matriz de Confusão\n",
        "plt.subplot(3, 2, 3)\n",
        "cm = confusion_matrix(y_val_classes, y_pred_classes)\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n",
        "plt.title('Matriz de Confusão', fontsize=14, fontweight='bold')\n",
        "plt.ylabel('Real')\n",
        "plt.xlabel('Previsto')\n",
        "plt.xticks([0.5, 1.5, 2.5, 3.5], ['Baixo', 'Médio-Baixo', 'Médio-Alto', 'Alto'], rotation=0)\n",
        "plt.yticks([0.5, 1.5, 2.5, 3.5], ['Baixo', 'Médio-Baixo', 'Médio-Alto', 'Alto'], rotation=0)\n",
        "\n",
        "# 4. Comparação Temporal\n",
        "plt.subplot(3, 2, 4)\n",
        "plt.plot(y_val_classes, label='Real', marker='o', linewidth=2, markersize=5, alpha=0.7)\n",
        "plt.plot(y_pred_classes, label='Previsto', marker='x', linestyle='--', linewidth=2, markersize=5, alpha=0.7)\n",
        "plt.title('Comparação Temporal', fontsize=14, fontweight='bold')\n",
        "plt.xlabel('Amostras')\n",
        "plt.ylabel('Classe')\n",
        "plt.yticks([0, 1, 2, 3], ['Baixo', 'Médio-Baixo', 'Médio-Alto', 'Alto'])\n",
        "plt.legend()\n",
        "plt.grid(True, alpha=0.3)\n",
        "\n",
        "# 5. Distribuição de Probabilidades\n",
        "plt.subplot(3, 2, 5)\n",
        "for i in range(4):\n",
        "    plt.hist(y_pred_proba[:, i], bins=20, alpha=0.6, label=f'Classe {i}', edgecolor='black')\n",
        "plt.title('Distribuição de Probabilidades', fontsize=14, fontweight='bold')\n",
        "plt.xlabel('Probabilidade')\n",
        "plt.ylabel('Frequência')\n",
        "plt.legend()\n",
        "plt.grid(True, alpha=0.3)\n",
        "\n",
        "# 6. Acurácia por Classe\n",
        "plt.subplot(3, 2, 6)\n",
        "acertos_por_classe = []\n",
        "for i in range(4):\n",
        "    mask = (y_val_classes == i)\n",
        "    if mask.sum() > 0:\n",
        "        acertos = (y_pred_classes[mask] == i).sum() / mask.sum() * 100\n",
        "        acertos_por_classe.append(acertos)\n",
        "    else:\n",
        "        acertos_por_classe.append(0)\n",
        "\n",
        "colors = ['green' if acc > 50 else 'orange' if acc > 30 else 'red' for acc in acertos_por_classe]\n",
        "bars = plt.bar(['Baixo', 'Médio-Baixo', 'Médio-Alto', 'Alto'], acertos_por_classe, color=colors, edgecolor='black')\n",
        "plt.title('Acurácia por Classe', fontsize=14, fontweight='bold')\n",
        "plt.ylabel('Acurácia (%)\")\n",
        "plt.ylim(0, 100)\n",
        "plt.grid(True, alpha=0.3, axis='y')\n",
        "\n",
        "for bar, acc in zip(bars, acertos_por_classe):\n",
        "    height = bar.get_height()\n",
        "    plt.text(bar.get_x() + bar.get_width()/2., height, f'{acc:.1f}%', ha='center', va='bottom', fontweight='bold')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"\\n✅ Visualizações geradas!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wyL7h1ckbUGE"
      },
      "outputs": [],
      "source": [
        "model_filename = 'modelo_lstm_classificacao_risco.keras'\n",
        "model.save(model_filename)\n",
        "print(f\"💾 Modelo salvo: '{model_filename}'\")\n",
        "print(\"\\n✅ Projeto concluído!\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.8.0"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}