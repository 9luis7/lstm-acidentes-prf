{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üöó Sprint Challenge 4 ‚Äì Previs√£o de Acidentes com LSTMs\n",
    "## Case Sompo: Antecipando Padr√µes de Risco em Rodovias Brasileiras\n",
    "\n",
    "---\n",
    "\n",
    "**Equipe Big 5**\n",
    "- Lucca Phelipe Masini - RM 564121\n",
    "- Luiz Henrique Poss - RM 562177\n",
    "- Luis Fernando de Oliveira Salgado - RM 561401\n",
    "- Igor Paix√£o Sarak - RM 563726\n",
    "- Bernardo Braga Perobeli - RM 562468\n",
    "\n",
    "---\n",
    "\n",
    "## üéØ Target Escolhido: Classifica√ß√£o Bin√°ria de Risco\n",
    "\n",
    "- **Classe 0 - BAIXO/M√âDIO RISCO**: < 30% de acidentes severos\n",
    "- **Classe 1 - ALTO RISCO**: ‚â• 30% de acidentes severos\n",
    "\n",
    "### Justificativa\n",
    "\n",
    "Utilizamos **classifica√ß√£o bin√°ria** para identificar semanas de alto risco em rodovias. O threshold de 30% separa per√≠odos normais de per√≠odos cr√≠ticos que requerem aten√ß√£o especial. Esta abordagem:\n",
    "\n",
    "- ‚úÖ √â mais robusta com features temporais limitadas\n",
    "- ‚úÖ Facilita tomada de decis√£o (alerta sim/n√£o)\n",
    "- ‚úÖ Permite ao modelo aprender padr√µes distintos\n",
    "- ‚úÖ Alinha-se com necessidades pr√°ticas de gest√£o de risco"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üìö Passo 1: Instala√ß√£o e Importa√ß√£o de Bibliotecas\n",
    "\n",
    "Primeiro, vamos instalar e importar todas as bibliotecas necess√°rias para o projeto.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install openpyxl --quiet\n",
    "print(\"‚úÖ Bibliotecas instaladas!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üì• Passo 2: Carregamento dos Dados\n",
    "\n",
    "Carregamos os dados diretamente do GitHub para garantir reprodutibilidade total.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import urllib.request\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "print(\"üìö Bibliotecas importadas!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üéØ Passo 3: Pr√©-processamento e Cria√ß√£o da Vari√°vel Target\n",
    "\n",
    "Criamos a vari√°vel bin√°ria `severo` que identifica acidentes com mortos ou feridos graves.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"üì• Baixando dataset da PRF do GitHub...\")\n",
    "\n",
    "github_raw_url = 'https://raw.githubusercontent.com/9luis7/lstm-acidentes-prf/main/dados/datatran2025.xlsx'\n",
    "output_filename = 'dados_acidentes.xlsx'\n",
    "\n",
    "try:\n",
    "    urllib.request.urlretrieve(github_raw_url, output_filename)\n",
    "    df = pd.read_excel(output_filename)\n",
    "    print(f\"‚úÖ Dataset carregado: {len(df):,} acidentes\")\n",
    "    print(\"\\nüìä Per√≠odo:\", df['data_inversa'].min(), \"at√©\", df['data_inversa'].max())\n",
    "    print(\"üìä Estados:\", df['uf'].nunique(), \"UFs\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Erro: {e}\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üîÑ Passo 4: Agrega√ß√£o Semanal\n",
    "\n",
    "Transformamos acidentes individuais em s√©ries temporais semanais por estado.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"üéØ Criando vari√°vel target 'severo'...\")\n",
    "\n",
    "df['horario'] = pd.to_datetime(df['horario'], format='%H:%M:%S', errors='coerce').dt.time\n",
    "df['severo'] = ((df['mortos'] > 0) | (df['feridos_graves'] > 0)).astype(int)\n",
    "\n",
    "colunas_relevantes = ['data_inversa', 'horario', 'uf', 'br', 'km', 'pessoas', 'veiculos', 'severo']\n",
    "df_limpo = df[colunas_relevantes].copy()\n",
    "df_limpo['horario'].fillna(pd.to_datetime('12:00:00').time(), inplace=True)\n",
    "\n",
    "print(\"‚úÖ Vari√°vel 'severo' criada!\")\n",
    "print(\"\\nüìä Distribui√ß√£o:\")\n",
    "print(df_limpo['severo'].value_counts(normalize=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üé® Passo 5: Feature Engineering\n",
    "\n",
    "Criamos 12 features enriquecidas: temporais, sazonalidade e hist√≥rico (lags).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"üîÑ Agregando dados em s√©ries temporais semanais...\")\n",
    "\n",
    "df_indexed = df_limpo.set_index('data_inversa')\n",
    "\n",
    "weekly_df = df_indexed.groupby([pd.Grouper(freq='W'), 'uf']).agg(\n",
    "    total_acidentes=('severo', 'count'),\n",
    "    acidentes_severos=('severo', 'sum'),\n",
    "    pessoas_total=('pessoas', 'sum'),\n",
    "    veiculos_total=('veiculos', 'sum'),\n",
    "    pessoas_media=('pessoas', 'mean'),\n",
    "    veiculos_media=('veiculos', 'mean')\n",
    ").reset_index()\n",
    "\n",
    "weekly_df['prop_severos'] = np.where(\n",
    "    weekly_df['total_acidentes'] > 0,\n",
    "    weekly_df['acidentes_severos'] / weekly_df['total_acidentes'],\n",
    "    0\n",
    ")\n",
    "\n",
    "print(f\"‚úÖ Dados agregados: {len(weekly_df):,} semanas √ó estados\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üî¢ Passo 6: Cria√ß√£o de Sequ√™ncias Temporais\n",
    "\n",
    "Criamos janelas de 8 semanas para prever a semana seguinte. Normalizamos os dados com MinMaxScaler.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"üé® Criando features temporais e de hist√≥rico...\")\n",
    "\n",
    "# Temporais\n",
    "weekly_df['dia_semana'] = weekly_df['data_inversa'].dt.dayofweek\n",
    "weekly_df['mes'] = weekly_df['data_inversa'].dt.month\n",
    "weekly_df['fim_semana'] = weekly_df['dia_semana'].isin([5, 6]).astype(int)\n",
    "\n",
    "# Sazonalidade\n",
    "weekly_df['sazonalidade_sen'] = np.sin(2 * np.pi * weekly_df['data_inversa'].dt.dayofyear / 365)\n",
    "weekly_df['sazonalidade_cos'] = np.cos(2 * np.pi * weekly_df['data_inversa'].dt.dayofyear / 365)\n",
    "\n",
    "# Lags\n",
    "for lag in [1, 2, 3]:\n",
    "    weekly_df[f'prop_severos_lag{lag}'] = weekly_df.groupby('uf')['prop_severos'].shift(lag)\n",
    "\n",
    "# Estat√≠sticas\n",
    "weekly_df['prop_severos_ma3'] = weekly_df.groupby('uf')['prop_severos'].rolling(3).mean().reset_index(0, drop=True)\n",
    "weekly_df['prop_severos_tendencia'] = weekly_df.groupby('uf')['prop_severos'].diff()\n",
    "weekly_df['prop_severos_volatilidade'] = weekly_df.groupby('uf')['prop_severos'].rolling(3).std().reset_index(0, drop=True)\n",
    "\n",
    "print(\"‚úÖ Features criadas!\")\n",
    "print(f\"   Total: {len(weekly_df.columns)} colunas\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üéØ Passo 7: Transforma√ß√£o para Classifica√ß√£o\n",
    "\n",
    "Como regress√£o falhou (R¬≤ negativo), transformamos o problema em classifica√ß√£o de 4 n√≠veis de risco.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üèóÔ∏è Passo 8: Constru√ß√£o do Modelo LSTM (SIMPLIFICADO)\n",
    "\n",
    "**Arquitetura Simplificada** para evitar overfitting:\n",
    "- Uma camada LSTM (32 neur√¥nios)\n",
    "- Dropout aumentado (0.3)\n",
    "- Learning rate reduzido (0.0005)\n",
    "- Early stopping mais restritivo (patience=10)\n",
    "\n",
    "**Class Weights** para corrigir desbalanceamento entre classes.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "print(\"üî¢ Preparando sequ√™ncias para LSTM...\")\n",
    "\n",
    "features_colunas = [\n",
    "    'prop_severos', 'pessoas_media', 'veiculos_media', 'fim_semana',\n",
    "    'sazonalidade_sen', 'sazonalidade_cos',\n",
    "    'prop_severos_lag1', 'prop_severos_lag2', 'prop_severos_lag3',\n",
    "    'prop_severos_ma3', 'prop_severos_tendencia', 'prop_severos_volatilidade'\n",
    "]\n",
    "\n",
    "df_features = weekly_df.set_index('data_inversa').sort_index()\n",
    "df_features = df_features[features_colunas].copy()\n",
    "df_features = df_features.dropna()\n",
    "\n",
    "# IMPORTANTE: Criar CLASSES ANTES da normaliza√ß√£o\n",
    "print(\"\\nüìä Criando classes BINARIAS ANTES da normaliza√ß√£o...\")\n",
    "target_values = df_features['prop_severos'].values\n",
    "\n",
    "def criar_classes_risco(y_data):\n",
    "    \"\"\"Classifica√ß√£o bin√°ria: 0 = Baixo/M√©dio (< 30%), 1 = Alto Risco (>= 30%)\"\"\"\n",
    "    classes = np.zeros_like(y_data, dtype=int)\n",
    "    classes[y_data >= 0.30] = 1  # ALTO RISCO\n",
    "    return classes\n",
    "\n",
    "# Criar classes dos valores originais\n",
    "classes_originais = criar_classes_risco(target_values)\n",
    "\n",
    "print(\"\\nüìä Distribui√ß√£o das classes (ANTES da normaliza√ß√£o):\")\n",
    "unique, counts = np.unique(classes_originais, return_counts=True)\n",
    "nomes_classes = ['BAIXO/MEDIO (<0.30)', 'ALTO RISCO (>=0.30)']\n",
    "for u, c in zip(unique, counts):\n",
    "    print(f\"   Classe {u} - {nomes_classes[u]}: {c:4d} ({c/len(classes_originais)*100:5.1f}%)\")\n",
    "\n",
    "# Separar features de target\n",
    "features_sem_target = [col for col in features_colunas if col != 'prop_severos']\n",
    "target_col = 'prop_severos'\n",
    "\n",
    "# Normalizar apenas as FEATURES (sem o target)\n",
    "df_features_input = df_features[features_sem_target].copy()\n",
    "scaler_features = MinMaxScaler(feature_range=(0, 1))\n",
    "features_scaled = scaler_features.fit_transform(df_features_input.values)\n",
    "\n",
    "# Usar o TARGET sem normaliza√ß√£o (mant√©m valores originais)\n",
    "target_values = df_features[target_col].values\n",
    "\n",
    "n_passos_para_tras = 8\n",
    "n_features = len(features_sem_target)\n",
    "\n",
    "X, y = [], []\n",
    "for i in range(n_passos_para_tras, len(features_scaled)):\n",
    "    X.append(features_scaled[i-n_passos_para_tras:i, :])\n",
    "    y.append(target_values[i])  # VALORES ORIGINAIS, n√£o normalizados\n",
    "\n",
    "X, y = np.array(X), np.array(y)\n",
    "\n",
    "print(f\"\\n‚úÖ Sequ√™ncias criadas!\")\n",
    "print(f\"   Shape X: {X.shape}\")\n",
    "print(f\"   Shape y: {y.shape}\")\n",
    "print(f\"   Range de y: [{y.min():.3f}, {y.max():.3f}]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "print(\"üéØ Transformando para CLASSIFICA√á√ÉO BIN√ÅRIA...\")\n",
    "\n",
    "# Usar a fun√ß√£o j√° definida anteriormente\n",
    "def criar_classes_risco(y_data):\n",
    "    \"\"\"Classifica√ß√£o bin√°ria: 0 = Baixo/M√©dio (< 30%), 1 = Alto Risco (>= 30%)\"\"\"\n",
    "    classes = np.zeros_like(y_data, dtype=int)\n",
    "    classes[y_data >= 0.30] = 1  # ALTO RISCO\n",
    "    return classes\n",
    "\n",
    "# Dividir dados\n",
    "split_index = int(len(X) * 0.85)\n",
    "X_train, X_val = X[:split_index], X[split_index:]\n",
    "y_train, y_val = y[:split_index], y[split_index:]\n",
    "\n",
    "print(f\"\\nüìä Estat√≠sticas de y_train:\")\n",
    "print(f\"   Min: {y_train.min():.3f}, Max: {y_train.max():.3f}\")\n",
    "print(f\"   Mean: {y_train.mean():.3f}, Std: {y_train.std():.3f}\")\n",
    "\n",
    "print(f\"\\nüìä Estat√≠sticas de y_val:\")\n",
    "print(f\"   Min: {y_val.min():.3f}, Max: {y_val.max():.3f}\")\n",
    "print(f\"   Mean: {y_val.mean():.3f}, Std: {y_val.std():.3f}\")\n",
    "\n",
    "# Criar classes\n",
    "y_train_classes = criar_classes_risco(y_train)\n",
    "y_val_classes = criar_classes_risco(y_val)\n",
    "\n",
    "# One-hot encoding (2 classes)\n",
    "y_train_categorical = to_categorical(y_train_classes, num_classes=2)\n",
    "y_val_categorical = to_categorical(y_val_classes, num_classes=2)\n",
    "\n",
    "nomes_classes = ['BAIXO/MEDIO (<0.30)', 'ALTO RISCO (>=0.30)']\n",
    "\n",
    "print(\"\\nüìä Distribui√ß√£o no TREINO:\")\n",
    "for i in range(2):\n",
    "    count = (y_train_classes == i).sum()\n",
    "    percent = count / len(y_train_classes) * 100\n",
    "    print(f\"   Classe {i} - {nomes_classes[i]}: {count:4d} ({percent:5.1f}%)\")\n",
    "\n",
    "print(\"\\nüìä Distribui√ß√£o na VALIDA√á√ÉO:\")\n",
    "for i in range(2):\n",
    "    count = (y_val_classes == i).sum()\n",
    "    percent = count / len(y_val_classes) * 100\n",
    "    print(f\"   Classe {i} - {nomes_classes[i]}: {count:4d} ({percent:5.1f}%)\")\n",
    "\n",
    "print(\"\\n‚úÖ Dados preparados!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Dropout\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "print(\"üèóÔ∏è  Construindo modelo LSTM SIMPLIFICADO...\")\n",
    "\n",
    "# MODELO SIMPLIFICADO PARA CLASSIFICA√á√ÉO BIN√ÅRIA\n",
    "model = Sequential([\n",
    "    LSTM(units=64, return_sequences=False, input_shape=(n_passos_para_tras, n_features)),\n",
    "    Dropout(0.3),\n",
    "    Dense(units=16, activation='relu'),\n",
    "    Dropout(0.3),\n",
    "    Dense(units=2, activation='softmax')  # 2 classes\n",
    "])\n",
    "\n",
    "# LEARNING RATE\n",
    "optimizer = Adam(learning_rate=0.001)\n",
    "model.compile(\n",
    "    optimizer=optimizer,\n",
    "    loss='categorical_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "print(\"‚úÖ Modelo constru√≠do!\")\n",
    "print(\"   Arquitetura: LSTM 64 ‚Üí Dense 16 ‚Üí Softmax 2\")\n",
    "print(\"   Dropout: 0.3\")\n",
    "print(\"   Learning rate: 0.001\")\n",
    "print(\"   SEM class weights - aprendizado natural\")\n",
    "model.summary()\n",
    "\n",
    "# CALLBACKS\n",
    "early_stop = EarlyStopping(monitor='val_loss', patience=15, restore_best_weights=True, verbose=1)\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=7, min_lr=0.00001, verbose=1)\n",
    "\n",
    "print(\"\\n‚úÖ Callbacks configurados!\")\n",
    "print(\"   Early Stopping: patience=15\")\n",
    "print(\"   Reduce LR: patience=7\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üöÄ Passo 9: Treinamento do Modelo\n",
    "\n",
    "Treinamos o modelo com class weights para corrigir desbalanceamento. Monitoramos val_loss para evitar overfitting.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"üöÄ TREINANDO MODELO LSTM\")\n",
    "print(\"=\"*70)\n",
    "print(\"\\nüìö Aprendizado natural SEM class weights\")\n",
    "print(\"‚è±Ô∏è  Aguarde 10-20 minutos...\\n\")\n",
    "\n",
    "history = model.fit(\n",
    "    X_train, y_train_categorical,\n",
    "    epochs=100,\n",
    "    batch_size=16,\n",
    "    validation_data=(X_val, y_val_categorical),\n",
    "    callbacks=[early_stop, reduce_lr],\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"‚úÖ TREINAMENTO CONCLU√çDO!\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üîß Abordagem de Modelagem\n",
    "\n",
    "### Decis√£o: Classifica√ß√£o Bin√°ria\n",
    "\n",
    "Optamos por **classifica√ß√£o bin√°ria** em vez de 4 classes para:\n",
    "\n",
    "1. **Maior robustez** - Modelo mais simples aprende melhor com features temporais limitadas\n",
    "2. **Threshold claro** - 30% de acidentes severos separa risco normal de cr√≠tico\n",
    "3. **Aplica√ß√£o pr√°tica** - Alertas bin√°rios (sim/n√£o) facilitam tomada de decis√£o\n",
    "4. **Melhor converg√™ncia** - Modelo consegue distinguir 2 padr√µes distintos\n",
    "\n",
    "### T√©cnicas Aplicadas\n",
    "\n",
    "1. **Classes criadas ANTES da normaliza√ß√£o** - Usar valores originais de `prop_severos`\n",
    "2. **Normalizar apenas features** - Target mant√©m valores originais\n",
    "3. **Arquitetura simplificada** - LSTM √∫nica (64 neur√¥nios) + Dense (16)\n",
    "4. **Aprendizado natural** - SEM class weights para evitar vi√©s artificial\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üìä Passo 10: Avalia√ß√£o e M√©tricas\n",
    "\n",
    "Avaliamos o modelo com accuracy, precision, recall e F1-score. Tamb√©m geramos matriz de confus√£o.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üìà Passo 11: Visualiza√ß√µes\n",
    "\n",
    "Geramos 6 gr√°ficos: curvas de aprendizagem, matriz de confus√£o, compara√ß√£o temporal, distribui√ß√£o de probabilidades e acur√°cia por classe.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, classification_report, accuracy_score\n",
    "\n",
    "print(\"üìä Avaliando modelo...\")\n",
    "\n",
    "y_pred_proba = model.predict(X_val, verbose=0)\n",
    "y_pred_classes = np.argmax(y_pred_proba, axis=1)\n",
    "\n",
    "accuracy = accuracy_score(y_val_classes, y_pred_classes)\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"üéØ RESULTADOS FINAIS\")\n",
    "print(\"=\"*70)\n",
    "print(f\"\\nüèÜ ACUR√ÅCIA: {accuracy:.2%}\\n\")\n",
    "\n",
    "baseline_random = 0.25\n",
    "baseline_majority = np.bincount(y_val_classes).max() / len(y_val_classes)\n",
    "\n",
    "print(\"üìä Compara√ß√£o com Baselines:\")\n",
    "print(f\"   Random Guess: {baseline_random:.1%}\")\n",
    "print(f\"   Classe mais comum: {baseline_majority:.1%}\")\n",
    "print(f\"   Nosso modelo: {accuracy:.1%} ‚úÖ\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"üìä RELAT√ìRIO POR CLASSE\")\n",
    "print(\"=\"*70 + \"\\n\")\n",
    "print(classification_report(y_val_classes, y_pred_classes, target_names=nomes_classes, digits=3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üíæ Passo 12: Salvamento do Modelo\n",
    "\n",
    "Salvamos o modelo treinado no formato `.keras` para uso futuro.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(16, 12))\n",
    "\n",
    "# 1. Loss\n",
    "plt.subplot(3, 2, 1)\n",
    "plt.plot(history.history['loss'], label='Treino', color='blue', linewidth=2)\n",
    "plt.plot(history.history['val_loss'], label='Valida√ß√£o', color='red', linewidth=2)\n",
    "plt.title('Curvas de Aprendizagem - Loss', fontsize=14, fontweight='bold')\n",
    "plt.xlabel('√âpocas')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# 2. Accuracy\n",
    "plt.subplot(3, 2, 2)\n",
    "plt.plot(history.history['accuracy'], label='Treino', color='blue', linewidth=2)\n",
    "plt.plot(history.history['val_accuracy'], label='Valida√ß√£o', color='red', linewidth=2)\n",
    "plt.title('Curvas de Aprendizagem - Acur√°cia', fontsize=14, fontweight='bold')\n",
    "plt.xlabel('√âpocas')\n",
    "plt.ylabel('Acur√°cia')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# 3. Matriz de Confus√£o\n",
    "plt.subplot(3, 2, 3)\n",
    "cm = confusion_matrix(y_val_classes, y_pred_classes)\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', cbar_kws={'label': 'Contagem'})\n",
    "plt.title('Matriz de Confus√£o', fontsize=14, fontweight='bold')\n",
    "plt.ylabel('Real')\n",
    "plt.xlabel('Previsto')\n",
    "plt.xticks([0.5, 1.5], ['Baixo/M√©dio', 'Alto Risco'], rotation=0)\n",
    "plt.yticks([0.5, 1.5], ['Baixo/M√©dio', 'Alto Risco'], rotation=0)\n",
    "\n",
    "# 4. Compara√ß√£o Temporal\n",
    "plt.subplot(3, 2, 4)\n",
    "plt.plot(y_val_classes, label='Real', marker='o', linewidth=2, markersize=5, alpha=0.7)\n",
    "plt.plot(y_pred_classes, label='Previsto', marker='x', linestyle='--', linewidth=2, markersize=5, alpha=0.7)\n",
    "plt.title('Compara√ß√£o Temporal', fontsize=14, fontweight='bold')\n",
    "plt.xlabel('Amostras')\n",
    "plt.ylabel('Classe')\n",
    "plt.yticks([0, 1], ['Baixo/M√©dio', 'Alto Risco'])\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# 5. Distribui√ß√£o de Probabilidades\n",
    "plt.subplot(3, 2, 5)\n",
    "labels_proba = ['Baixo/M√©dio', 'Alto Risco']\n",
    "for i in range(2):\n",
    "    plt.hist(y_pred_proba[:, i], bins=20, alpha=0.6, label=labels_proba[i], edgecolor='black')\n",
    "plt.title('Distribui√ß√£o de Probabilidades', fontsize=14, fontweight='bold')\n",
    "plt.xlabel('Probabilidade')\n",
    "plt.ylabel('Frequ√™ncia')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# 6. Acur√°cia por Classe\n",
    "plt.subplot(3, 2, 6)\n",
    "acertos_por_classe = []\n",
    "labels_acc = ['Baixo/M√©dio', 'Alto Risco']\n",
    "for i in range(2):\n",
    "    mask = (y_val_classes == i)\n",
    "    if mask.sum() > 0:\n",
    "        acertos = (y_pred_classes[mask] == i).sum() / mask.sum() * 100\n",
    "        acertos_por_classe.append(acertos)\n",
    "    else:\n",
    "        acertos_por_classe.append(0)\n",
    "\n",
    "colors = ['green' if acc > 50 else 'orange' if acc > 30 else 'red' for acc in acertos_por_classe]\n",
    "bars = plt.bar(labels_acc, acertos_por_classe, color=colors, edgecolor='black')\n",
    "plt.title('Acur√°cia por Classe', fontsize=14, fontweight='bold')\n",
    "plt.ylabel('Acur√°cia (%)')\n",
    "plt.ylim(0, 100)\n",
    "plt.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "for bar, acc in zip(bars, acertos_por_classe):\n",
    "    height = bar.get_height()\n",
    "    plt.text(bar.get_x() + bar.get_width()/2., height, f'{acc:.1f}%', ha='center', va='bottom', fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n‚úÖ Visualiza√ß√µes geradas!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_filename = 'modelo_lstm_classificacao_risco.keras'\n",
    "model.save(model_filename)\n",
    "print(f\"üíæ Modelo salvo: '{model_filename}'\")\n",
    "print(\"\\n‚úÖ Projeto conclu√≠do!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
