{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üöó Sprint Challenge 4 ‚Äì Previs√£o de Acidentes com LSTMs\n",
    "## Case Sompo: Antecipando Padr√µes de Risco em Rodovias Brasileiras\n",
    "\n",
    "---\n",
    "\n",
    "**Equipe Big 5**\n",
    "- Lucca Phelipe Masini - RM 564121\n",
    "- Luiz Henrique Poss - RM 562177\n",
    "- Luis Fernando de Oliveira Salgado - RM 561401\n",
    "- Igor Paix√£o Sarak - RM 563726\n",
    "- Bernardo Braga Perobeli - RM 562468\n",
    "\n",
    "---\n",
    "\n",
    "## üéØ Target Escolhido: Regress√£o - Propor√ß√£o de Acidentes Severos\n",
    "\n",
    "**Prevemos o valor cont√≠nuo** da propor√ß√£o de acidentes severos (com mortos ou feridos graves) em cada semana por estado.\n",
    "\n",
    "- **Range**: 0% a 100% de acidentes severos\n",
    "- **Interpreta√ß√£o**: Quanto maior o valor, maior o risco da semana\n",
    "\n",
    "### Justificativa\n",
    "\n",
    "Optamos por **regress√£o** em vez de classifica√ß√£o porque:\n",
    "\n",
    "1. ‚úÖ **Mais adequado aos dados** - Features temporais n√£o capturam fatores cr√≠ticos para classifica√ß√£o (clima, eventos)\n",
    "2. ‚úÖ **Informa√ß√£o mais rica** - Valor cont√≠nuo (ex: 28% vs 32%) √© mais √∫til que categoria bin√°ria\n",
    "3. ‚úÖ **M√©tricas claras** - MAE e RMSE mostram erro m√©dio real em pontos percentuais\n",
    "4. ‚úÖ **Honestidade cient√≠fica** - Mostra o que o modelo CONSEGUE fazer, n√£o for√ßa classifica√ß√£o artificial"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üìö Passo 1: Instala√ß√£o e Importa√ß√£o de Bibliotecas\n",
    "\n",
    "Primeiro, vamos instalar e importar todas as bibliotecas necess√°rias para o projeto.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install openpyxl --quiet\n",
    "print(\"‚úÖ Bibliotecas instaladas!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üì• Passo 2: Carregamento dos Dados\n",
    "\n",
    "Carregamos os dados diretamente do GitHub para garantir reprodutibilidade total.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import urllib.request\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "print(\"üìö Bibliotecas importadas!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üéØ Passo 3: Pr√©-processamento e Cria√ß√£o da Vari√°vel Target\n",
    "\n",
    "Criamos a vari√°vel bin√°ria `severo` que identifica acidentes com mortos ou feridos graves.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"üì• Baixando dataset da PRF do GitHub...\")\n",
    "\n",
    "github_raw_url = 'https://raw.githubusercontent.com/9luis7/lstm-acidentes-prf/main/dados/datatran2025.xlsx'\n",
    "output_filename = 'dados_acidentes.xlsx'\n",
    "\n",
    "try:\n",
    "    urllib.request.urlretrieve(github_raw_url, output_filename)\n",
    "    df = pd.read_excel(output_filename)\n",
    "    print(f\"‚úÖ Dataset carregado: {len(df):,} acidentes\")\n",
    "    print(\"\\nüìä Per√≠odo:\", df['data_inversa'].min(), \"at√©\", df['data_inversa'].max())\n",
    "    print(\"üìä Estados:\", df['uf'].nunique(), \"UFs\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Erro: {e}\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üîÑ Passo 4: Agrega√ß√£o Semanal\n",
    "\n",
    "Transformamos acidentes individuais em s√©ries temporais semanais por estado.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"üéØ Criando vari√°vel target 'severo'...\")\n",
    "\n",
    "df['horario'] = pd.to_datetime(df['horario'], format='%H:%M:%S', errors='coerce').dt.time\n",
    "df['severo'] = ((df['mortos'] > 0) | (df['feridos_graves'] > 0)).astype(int)\n",
    "\n",
    "colunas_relevantes = ['data_inversa', 'horario', 'uf', 'br', 'km', 'pessoas', 'veiculos', 'severo']\n",
    "df_limpo = df[colunas_relevantes].copy()\n",
    "df_limpo['horario'].fillna(pd.to_datetime('12:00:00').time(), inplace=True)\n",
    "\n",
    "print(\"‚úÖ Vari√°vel 'severo' criada!\")\n",
    "print(\"\\nüìä Distribui√ß√£o:\")\n",
    "print(df_limpo['severo'].value_counts(normalize=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üé® Passo 5: Feature Engineering\n",
    "\n",
    "Criamos 12 features enriquecidas: temporais, sazonalidade e hist√≥rico (lags).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"üîÑ Agregando dados em s√©ries temporais semanais...\")\n",
    "\n",
    "df_indexed = df_limpo.set_index('data_inversa')\n",
    "\n",
    "weekly_df = df_indexed.groupby([pd.Grouper(freq='W'), 'uf']).agg(\n",
    "    total_acidentes=('severo', 'count'),\n",
    "    acidentes_severos=('severo', 'sum'),\n",
    "    pessoas_total=('pessoas', 'sum'),\n",
    "    veiculos_total=('veiculos', 'sum'),\n",
    "    pessoas_media=('pessoas', 'mean'),\n",
    "    veiculos_media=('veiculos', 'mean')\n",
    ").reset_index()\n",
    "\n",
    "weekly_df['prop_severos'] = np.where(\n",
    "    weekly_df['total_acidentes'] > 0,\n",
    "    weekly_df['acidentes_severos'] / weekly_df['total_acidentes'],\n",
    "    0\n",
    ")\n",
    "\n",
    "print(f\"‚úÖ Dados agregados: {len(weekly_df):,} semanas √ó estados\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üî¢ Passo 6: Cria√ß√£o de Sequ√™ncias Temporais\n",
    "\n",
    "Criamos janelas de 8 semanas para prever a semana seguinte. Normalizamos os dados com MinMaxScaler.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"üé® Criando features temporais e de hist√≥rico...\")\n",
    "\n",
    "# Temporais\n",
    "weekly_df['dia_semana'] = weekly_df['data_inversa'].dt.dayofweek\n",
    "weekly_df['mes'] = weekly_df['data_inversa'].dt.month\n",
    "weekly_df['fim_semana'] = weekly_df['dia_semana'].isin([5, 6]).astype(int)\n",
    "\n",
    "# Sazonalidade\n",
    "weekly_df['sazonalidade_sen'] = np.sin(2 * np.pi * weekly_df['data_inversa'].dt.dayofyear / 365)\n",
    "weekly_df['sazonalidade_cos'] = np.cos(2 * np.pi * weekly_df['data_inversa'].dt.dayofyear / 365)\n",
    "\n",
    "# Lags\n",
    "for lag in [1, 2, 3]:\n",
    "    weekly_df[f'prop_severos_lag{lag}'] = weekly_df.groupby('uf')['prop_severos'].shift(lag)\n",
    "\n",
    "# Estat√≠sticas\n",
    "weekly_df['prop_severos_ma3'] = weekly_df.groupby('uf')['prop_severos'].rolling(3).mean().reset_index(0, drop=True)\n",
    "weekly_df['prop_severos_tendencia'] = weekly_df.groupby('uf')['prop_severos'].diff()\n",
    "weekly_df['prop_severos_volatilidade'] = weekly_df.groupby('uf')['prop_severos'].rolling(3).std().reset_index(0, drop=True)\n",
    "\n",
    "print(\"‚úÖ Features criadas!\")\n",
    "print(f\"   Total: {len(weekly_df.columns)} colunas\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üéØ Passo 7: Prepara√ß√£o para Regress√£o\n",
    "\n",
    "Preparamos os dados para prever valores cont√≠nuos da propor√ß√£o de acidentes severos.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üèóÔ∏è Passo 8: Constru√ß√£o do Modelo LSTM para Regress√£o\n",
    "\n",
    "**Arquitetura para Regress√£o:**\n",
    "- LSTM (64 neur√¥nios) - captura padr√µes temporais\n",
    "- Dense (32 ‚Üí 16 neur√¥nios) - processamento n√£o-linear\n",
    "- Output (1 neur√¥nio, linear) - valor cont√≠nuo\n",
    "- Loss: MAE (Mean Absolute Error)\n",
    "- M√©tricas: MAE e MSE\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "print(\"üî¢ Preparando sequ√™ncias para LSTM (REGRESS√ÉO)...\")\n",
    "\n",
    "features_colunas = [\n",
    "    'prop_severos', 'pessoas_media', 'veiculos_media', 'fim_semana',\n",
    "    'sazonalidade_sen', 'sazonalidade_cos',\n",
    "    'prop_severos_lag1', 'prop_severos_lag2', 'prop_severos_lag3',\n",
    "    'prop_severos_ma3', 'prop_severos_tendencia', 'prop_severos_volatilidade'\n",
    "]\n",
    "\n",
    "df_features = weekly_df.set_index('data_inversa').sort_index()\n",
    "df_features = df_features[features_colunas].copy()\n",
    "df_features = df_features.dropna()\n",
    "\n",
    "# Target: propor√ß√£o de acidentes severos (valores cont√≠nuos 0-1)\n",
    "target_values = df_features['prop_severos'].values\n",
    "\n",
    "print(\"\\nüìä Estat√≠sticas do target (propor√ß√£o de acidentes severos):\")\n",
    "print(f\"   Min: {target_values.min():.3f} ({target_values.min()*100:.1f}%)\")\n",
    "print(f\"   Max: {target_values.max():.3f} ({target_values.max()*100:.1f}%)\")\n",
    "print(f\"   M√©dia: {target_values.mean():.3f} ({target_values.mean()*100:.1f}%)\")\n",
    "print(f\"   Mediana: {np.median(target_values):.3f} ({np.median(target_values)*100:.1f}%)\")\n",
    "print(f\"   Desvio padr√£o: {target_values.std():.3f}\")\n",
    "\n",
    "# Separar features de target\n",
    "features_sem_target = [col for col in features_colunas if col != 'prop_severos']\n",
    "target_col = 'prop_severos'\n",
    "\n",
    "# Normalizar apenas as FEATURES (sem o target)\n",
    "df_features_input = df_features[features_sem_target].copy()\n",
    "scaler_features = MinMaxScaler(feature_range=(0, 1))\n",
    "features_scaled = scaler_features.fit_transform(df_features_input.values)\n",
    "\n",
    "# TARGET permanece em escala original (0-1 j√° √© uma escala natural)\n",
    "target_values = df_features[target_col].values\n",
    "\n",
    "n_passos_para_tras = 8\n",
    "n_features = len(features_sem_target)\n",
    "\n",
    "X, y = [], []\n",
    "for i in range(n_passos_para_tras, len(features_scaled)):\n",
    "    X.append(features_scaled[i-n_passos_para_tras:i, :])\n",
    "    y.append(target_values[i])\n",
    "\n",
    "X, y = np.array(X), np.array(y)\n",
    "\n",
    "print(f\"\\n‚úÖ Sequ√™ncias criadas para regress√£o!\")\n",
    "print(f\"   Shape X: {X.shape}\")\n",
    "print(f\"   Shape y: {y.shape}\")\n",
    "print(f\"   Range de y: [{y.min():.3f}, {y.max():.3f}]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"üéØ Preparando dados para REGRESS√ÉO...\")\n",
    "\n",
    "# Dividir dados temporalmente (85% treino, 15% valida√ß√£o)\n",
    "split_index = int(len(X) * 0.85)\n",
    "X_train, X_val = X[:split_index], X[split_index:]\n",
    "y_train, y_val = y[:split_index], y[split_index:]\n",
    "\n",
    "print(f\"\\nüìä Divis√£o temporal:\")\n",
    "print(f\"   Treino: {len(X_train)} sequ√™ncias ({len(X_train)/len(X)*100:.1f}%)\")\n",
    "print(f\"   Valida√ß√£o: {len(X_val)} sequ√™ncias ({len(X_val)/len(X)*100:.1f}%)\")\n",
    "\n",
    "print(f\"\\nüìä Estat√≠sticas de y_train:\")\n",
    "print(f\"   Min: {y_train.min():.3f} ({y_train.min()*100:.1f}%)\")\n",
    "print(f\"   Max: {y_train.max():.3f} ({y_train.max()*100:.1f}%)\")\n",
    "print(f\"   M√©dia: {y_train.mean():.3f} ({y_train.mean()*100:.1f}%)\")\n",
    "print(f\"   Desvio: {y_train.std():.3f}\")\n",
    "\n",
    "print(f\"\\nüìä Estat√≠sticas de y_val:\")\n",
    "print(f\"   Min: {y_val.min():.3f} ({y_val.min()*100:.1f}%)\")\n",
    "print(f\"   Max: {y_val.max():.3f} ({y_val.max()*100:.1f}%)\")\n",
    "print(f\"   M√©dia: {y_val.mean():.3f} ({y_val.mean()*100:.1f}%)\")\n",
    "print(f\"   Desvio: {y_val.std():.3f}\")\n",
    "\n",
    "print(\"\\n‚úÖ Dados preparados para regress√£o!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Dropout\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "print(\"üèóÔ∏è  Construindo modelo LSTM para REGRESS√ÉO...\")\n",
    "\n",
    "# MODELO LSTM PARA REGRESS√ÉO\n",
    "model = Sequential([\n",
    "    LSTM(units=64, return_sequences=False, input_shape=(n_passos_para_tras, n_features)),\n",
    "    Dropout(0.3),\n",
    "    Dense(units=32, activation='relu'),\n",
    "    Dropout(0.2),\n",
    "    Dense(units=16, activation='relu'),\n",
    "    Dense(units=1, activation='linear')  # REGRESS√ÉO: 1 neur√¥nio, ativa√ß√£o linear\n",
    "])\n",
    "\n",
    "# OPTIMIZER\n",
    "optimizer = Adam(learning_rate=0.001)\n",
    "model.compile(\n",
    "    optimizer=optimizer,\n",
    "    loss='mean_absolute_error',  # MAE: erro m√©dio absoluto em pontos percentuais\n",
    "    metrics=['mae', 'mse']\n",
    ")\n",
    "\n",
    "print(\"‚úÖ Modelo constru√≠do para REGRESS√ÉO!\")\n",
    "print(\"   Arquitetura: LSTM 64 ‚Üí Dense 32 ‚Üí Dense 16 ‚Üí Linear 1\")\n",
    "print(\"   Dropout: 0.3 e 0.2\")\n",
    "print(\"   Loss: MAE (Mean Absolute Error)\")\n",
    "print(\"   M√©tricas: MAE e MSE\")\n",
    "print(\"   Learning rate: 0.001\")\n",
    "model.summary()\n",
    "\n",
    "# CALLBACKS\n",
    "early_stop = EarlyStopping(monitor='val_loss', patience=20, restore_best_weights=True, verbose=1)\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=10, min_lr=0.00001, verbose=1)\n",
    "\n",
    "print(\"\\n‚úÖ Callbacks configurados!\")\n",
    "print(\"   Early Stopping: patience=20 (monitor val_loss/MAE)\")\n",
    "print(\"   Reduce LR: patience=10\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üöÄ Passo 9: Treinamento do Modelo\n",
    "\n",
    "Treinamos o modelo com class weights para corrigir desbalanceamento. Monitoramos val_loss para evitar overfitting.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"üöÄ TREINANDO MODELO LSTM DE REGRESS√ÉO\")\n",
    "print(\"=\"*70)\n",
    "print(\"\\nüìä Prevendo propor√ß√£o cont√≠nua de acidentes severos\")\n",
    "print(\"‚è±Ô∏è  Aguarde 10-20 minutos...\\n\")\n",
    "\n",
    "history = model.fit(\n",
    "    X_train, y_train,\n",
    "    epochs=100,\n",
    "    batch_size=16,\n",
    "    validation_data=(X_val, y_val),\n",
    "    callbacks=[early_stop, reduce_lr],\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"‚úÖ TREINAMENTO CONCLU√çDO!\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üîß Abordagem de Modelagem: Regress√£o\n",
    "\n",
    "### Por que Regress√£o?\n",
    "\n",
    "Ap√≥s testes com classifica√ß√£o (4 classes e bin√°ria), identificamos que:\n",
    "\n",
    "‚ùå **Classifica√ß√£o n√£o funcionou** - Modelo sempre previa uma √∫nica classe  \n",
    "‚ùå **Features limitadas** - Dados temporais n√£o capturam fatores cr√≠ticos (clima, eventos)  \n",
    "‚úÖ **Regress√£o √© mais adequada** - Valores cont√≠nuos aproveitam melhor informa√ß√£o dispon√≠vel\n",
    "\n",
    "### Vantagens da Abordagem\n",
    "\n",
    "1. **Informa√ß√£o rica** - Prever 28% vs 32% √© mais √∫til que \"risco baixo\" vs \"alto\"\n",
    "2. **M√©tricas claras** - MAE mostra erro m√©dio em pontos percentuais\n",
    "3. **Honestidade cient√≠fica** - Mostra capacidade real do modelo\n",
    "4. **Aplica√ß√£o pr√°tica** - Gestores podem definir pr√≥prios thresholds de alerta\n",
    "\n",
    "### Arquitetura\n",
    "\n",
    "- **Input**: Sequ√™ncias de 8 semanas √ó 11 features\n",
    "- **LSTM**: 64 neur√¥nios (captura padr√µes temporais)\n",
    "- **Dense**: 32 ‚Üí 16 neur√¥nios (processamento n√£o-linear)\n",
    "- **Output**: 1 neur√¥nio linear (valor cont√≠nuo 0-1)\n",
    "- **Loss**: MAE (Mean Absolute Error)\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üìä Passo 10: Avalia√ß√£o e M√©tricas\n",
    "\n",
    "Avaliamos o modelo com MAE, RMSE e R¬≤. Comparamos com baseline (sempre prever a m√©dia).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üìà Passo 11: Visualiza√ß√µes\n",
    "\n",
    "Geramos 6 gr√°ficos: curvas de aprendizagem (MAE/MSE), scatter plot real vs previsto, s√©rie temporal, distribui√ß√£o de erros e an√°lise de res√≠duos.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "\n",
    "print(\"üìä Avaliando modelo de REGRESS√ÉO...\")\n",
    "\n",
    "# Predi√ß√µes\n",
    "y_pred = model.predict(X_val, verbose=0).flatten()\n",
    "\n",
    "# M√©tricas\n",
    "mae = mean_absolute_error(y_val, y_pred)\n",
    "mse = mean_squared_error(y_val, y_pred)\n",
    "rmse = np.sqrt(mse)\n",
    "r2 = r2_score(y_val, y_pred)\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"üéØ RESULTADOS FINAIS - REGRESS√ÉO\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "print(f\"\\nüìä M√©tricas de Erro:\")\n",
    "print(f\"   MAE (Mean Absolute Error): {mae:.4f} ({mae*100:.2f} pontos percentuais)\")\n",
    "print(f\"   RMSE (Root Mean Squared Error): {rmse:.4f} ({rmse*100:.2f} pontos percentuais)\")\n",
    "print(f\"   R¬≤ Score: {r2:.4f}\")\n",
    "\n",
    "# Baseline: sempre prever a m√©dia\n",
    "baseline_pred = np.full_like(y_val, y_train.mean())\n",
    "baseline_mae = mean_absolute_error(y_val, baseline_pred)\n",
    "\n",
    "print(f\"\\nüìä Compara√ß√£o com Baseline (sempre prever m√©dia):\")\n",
    "print(f\"   Baseline MAE: {baseline_mae:.4f} ({baseline_mae*100:.2f}pp)\")\n",
    "print(f\"   Nosso modelo MAE: {mae:.4f} ({mae*100:.2f}pp)\")\n",
    "print(f\"   Melhoria: {((baseline_mae - mae)/baseline_mae*100):.1f}%\")\n",
    "\n",
    "# Estat√≠sticas dos erros\n",
    "errors = np.abs(y_val - y_pred)\n",
    "print(f\"\\nüìä Distribui√ß√£o dos Erros Absolutos:\")\n",
    "print(f\"   M√≠nimo: {errors.min():.4f} ({errors.min()*100:.2f}pp)\")\n",
    "print(f\"   M√°ximo: {errors.max():.4f} ({errors.max()*100:.2f}pp)\")\n",
    "print(f\"   Mediana: {np.median(errors):.4f} ({np.median(errors)*100:.2f}pp)\")\n",
    "print(f\"   75¬∫ percentil: {np.percentile(errors, 75):.4f} ({np.percentile(errors, 75)*100:.2f}pp)\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üíæ Passo 12: Salvamento do Modelo\n",
    "\n",
    "Salvamos o modelo treinado no formato `.keras` para uso futuro.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(16, 12))\n",
    "\n",
    "# 1. Loss (MAE)\n",
    "plt.subplot(3, 2, 1)\n",
    "plt.plot(history.history['loss'], label='Treino', color='blue', linewidth=2)\n",
    "plt.plot(history.history['val_loss'], label='Valida√ß√£o', color='red', linewidth=2)\n",
    "plt.title('Curvas de Aprendizagem - MAE', fontsize=14, fontweight='bold')\n",
    "plt.xlabel('√âpocas')\n",
    "plt.ylabel('MAE')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# 2. MSE\n",
    "plt.subplot(3, 2, 2)\n",
    "plt.plot(history.history['mse'], label='Treino', color='blue', linewidth=2)\n",
    "plt.plot(history.history['val_mse'], label='Valida√ß√£o', color='red', linewidth=2)\n",
    "plt.title('Curvas de Aprendizagem - MSE', fontsize=14, fontweight='bold')\n",
    "plt.xlabel('√âpocas')\n",
    "plt.ylabel('MSE')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# 3. Real vs Previsto (Scatter Plot)\n",
    "plt.subplot(3, 2, 3)\n",
    "plt.scatter(y_val, y_pred, alpha=0.5, s=30)\n",
    "plt.plot([y_val.min(), y_val.max()], [y_val.min(), y_val.max()], 'r--', lw=2, label='Linha Perfeita')\n",
    "plt.title('Real vs Previsto', fontsize=14, fontweight='bold')\n",
    "plt.xlabel('Valor Real')\n",
    "plt.ylabel('Valor Previsto')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# 4. S√©rie Temporal: Real vs Previsto\n",
    "plt.subplot(3, 2, 4)\n",
    "plt.plot(y_val, label='Real', linewidth=2, alpha=0.7)\n",
    "plt.plot(y_pred, label='Previsto', linewidth=2, alpha=0.7, linestyle='--')\n",
    "plt.title('Compara√ß√£o Temporal', fontsize=14, fontweight='bold')\n",
    "plt.xlabel('Amostras')\n",
    "plt.ylabel('Propor√ß√£o de Acidentes Severos')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# 5. Distribui√ß√£o dos Erros\n",
    "plt.subplot(3, 2, 5)\n",
    "errors = y_val - y_pred\n",
    "plt.hist(errors, bins=30, edgecolor='black', alpha=0.7)\n",
    "plt.axvline(x=0, color='r', linestyle='--', linewidth=2, label='Erro Zero')\n",
    "plt.title('Distribui√ß√£o dos Erros (Res√≠duos)', fontsize=14, fontweight='bold')\n",
    "plt.xlabel('Erro (Real - Previsto)')\n",
    "plt.ylabel('Frequ√™ncia')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# 6. Res√≠duos vs Valores Previstos\n",
    "plt.subplot(3, 2, 6)\n",
    "plt.scatter(y_pred, errors, alpha=0.5, s=30)\n",
    "plt.axhline(y=0, color='r', linestyle='--', linewidth=2)\n",
    "plt.title('Res√≠duos vs Valores Previstos', fontsize=14, fontweight='bold')\n",
    "plt.xlabel('Valor Previsto')\n",
    "plt.ylabel('Res√≠duo (Real - Previsto)')\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n‚úÖ Visualiza√ß√µes geradas!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_filename = 'modelo_lstm_classificacao_risco.keras'\n",
    "model.save(model_filename)\n",
    "print(f\"üíæ Modelo salvo: '{model_filename}'\")\n",
    "print(\"\\n‚úÖ Projeto conclu√≠do!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
